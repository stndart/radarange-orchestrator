{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a5f22f",
   "metadata": {},
   "source": [
    "Простенький ноутбук, демонстрирующий создание и использование ретривера pdf-ок из моего zotero на elasticsearch. Поиск с помощью BM25-алгоритма.\n",
    "Для использования уже поднятого ретривера. промотать до секции `Inference`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf11c6b",
   "metadata": {},
   "source": [
    "# Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7eb79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker run -d --name elasticsearch -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" -p 9200:9200 -p 9300:9300 docker.elastic.co/elasticsearch/elasticsearch:8.13.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1742c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/home/lexi/radarange-orchestrator/data/Library.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f12a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210,\n",
       " ['http://arxiv.org/pdf/1910.02893',\n",
       "  'http://arxiv.org/pdf/2005.06600',\n",
       "  'http://arxiv.org/pdf/2210.05619',\n",
       "  'http://arxiv.org/pdf/2003.11080',\n",
       "  'http://arxiv.org/pdf/1609.04747'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [url.replace('abs', 'pdf') for url in list(data['Url'].unique()) if pd.notna(url) and 'arxiv.org' in url]\n",
    "len(urls), urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10dd4ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading batches:   0%|          | 0/42 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (671 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:   2%|▏         | 1/42 [01:49<1:14:50, 109.53s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (6760 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:   5%|▍         | 2/42 [03:19<1:05:10, 97.75s/it] Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:   7%|▋         | 3/42 [06:20<1:28:19, 135.90s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  10%|▉         | 4/42 [07:23<1:07:49, 107.08s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (603 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  12%|█▏        | 5/42 [09:06<1:05:08, 105.64s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1166 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  14%|█▍        | 6/42 [10:43<1:01:44, 102.91s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1092 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  17%|█▋        | 7/42 [13:07<1:07:43, 116.11s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  19%|█▉        | 8/42 [14:38<1:01:14, 108.07s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1973 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  21%|██▏       | 9/42 [17:15<1:07:56, 123.52s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (833 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  24%|██▍       | 10/42 [19:04<1:03:30, 119.07s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  26%|██▌       | 11/42 [23:21<1:23:17, 161.21s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  29%|██▊       | 12/42 [25:06<1:12:02, 144.07s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (664 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  31%|███       | 13/42 [26:29<1:00:40, 125.52s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (664 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  33%|███▎      | 14/42 [28:49<1:00:42, 130.09s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  36%|███▌      | 15/42 [31:18<1:01:06, 135.81s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (3884 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  38%|███▊      | 16/42 [32:36<51:13, 118.21s/it]  Token indices sequence length is longer than the specified maximum sequence length for this model (633 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  40%|████      | 17/42 [33:48<43:30, 104.43s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (789 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  43%|████▎     | 18/42 [34:56<37:23, 93.47s/it] Token indices sequence length is longer than the specified maximum sequence length for this model (1326 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  45%|████▌     | 19/42 [38:18<48:16, 125.95s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1220 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  48%|████▊     | 20/42 [39:30<40:18, 109.94s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1066 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  50%|█████     | 21/42 [40:49<35:09, 100.46s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1137 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  52%|█████▏    | 22/42 [43:21<38:40, 116.00s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2002 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  55%|█████▍    | 23/42 [45:13<36:23, 114.94s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2128 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  57%|█████▋    | 24/42 [47:17<35:16, 117.56s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  60%|█████▉    | 25/42 [48:45<30:49, 108.79s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  62%|██████▏   | 26/42 [49:59<26:10, 98.14s/it] Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  64%|██████▍   | 27/42 [52:28<28:21, 113.45s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  67%|██████▋   | 28/42 [55:06<29:36, 126.89s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  69%|██████▉   | 29/42 [56:22<24:11, 111.66s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1875 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  71%|███████▏  | 30/42 [58:27<23:08, 115.72s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  74%|███████▍  | 31/42 [1:00:50<22:43, 123.94s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (3980 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  76%|███████▌  | 32/42 [1:03:34<22:39, 135.95s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (918 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  79%|███████▊  | 33/42 [1:12:56<39:32, 263.58s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  81%|████████  | 34/42 [1:15:40<31:09, 233.67s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  83%|████████▎ | 35/42 [1:16:59<21:50, 187.27s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (4767 > 512). Running this sequence through the model will result in indexing errors\n",
      "/home/lexi/radarange-orchestrator/.venv/lib/python3.11/site-packages/docling_core/transforms/chunker/hybrid_chunker.py:235: UserWarning: Headers and captions for this chunk are longer than the total available size for the chunk, so they will be ignored: doc_chunk.text=\"Fig. 1. Topological Neural Network : Data associated with a complex system are features defined on a data domain , which is preprocessed into a computational domain that encodes interactions between the system's components with neighborhoods . The TNN's layers use message passing to successively update features and yield an output, e.g. a categorical label in classification or a quantitative value in regression. The output represents new knowledge extracted from the input data.\", doc_chunk.meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[TextItem(self_ref='#/texts/22', parent=RefItem(cref='#/pictures/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.CAPTION: 'caption'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=79.2, t=528.668, r=532.802, b=494.617, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 482))], orig=\"Fig. 1. Topological Neural Network : Data associated with a complex system are features defined on a data domain , which is preprocessed into a computational domain that encodes interactions between the system's components with neighborhoods . The TNN's layers use message passing to successively update features and yield an output, e.g. a categorical label in classification or a quantitative value in regression. The output represents new knowledge extracted from the input data.\", text=\"Fig. 1. Topological Neural Network : Data associated with a complex system are features defined on a data domain , which is preprocessed into a computational domain that encodes interactions between the system's components with neighborhoods . The TNN's layers use message passing to successively update features and yield an output, e.g. a categorical label in classification or a quantitative value in regression. The output represents new knowledge extracted from the input data.\", formatting=None, hyperlink=None)], headings=['/gid00053 /gid00078/gid00079/gid00078/gid00075/gid00078/gid00070/gid00072/gid00066/gid00064/gid00075/gid00001/gid00047/gid00068/gid00084/gid00081/gid00064/gid00075/gid00001/gid00047/gid00068/gid00083/gid00086/gid00078/gid00081/gid00074/gid00001/gid00086/gid00072/gid00083/gid00071/gid00001/gid00020/gid00001/gid00045/gid00064/gid00088/gid00068/gid00081/gid00082'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=15149095900601441673, filename='2304.10031v3.pdf', uri=None))\n",
      "  warnings.warn(\n",
      "Loading batches:  86%|████████▌ | 36/42 [1:19:14<17:09, 171.61s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1475 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  88%|████████▊ | 37/42 [1:24:48<18:21, 220.32s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (852 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  90%|█████████ | 38/42 [1:28:01<14:08, 212.06s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1014 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  93%|█████████▎| 39/42 [1:30:26<09:35, 191.93s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  95%|█████████▌| 40/42 [1:37:11<08:32, 256.12s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (707 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches:  98%|█████████▊| 41/42 [1:39:00<03:31, 211.72s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Loading batches: 100%|██████████| 42/42 [1:40:45<00:00, 143.94s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_docling import DoclingLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 5\n",
    "documents = []\n",
    "\n",
    "for i in tqdm(range(0, len(urls), batch_size), desc=\"Loading batches\"):\n",
    "    batch_urls = urls[i:i + batch_size]\n",
    "    loader = DoclingLoader(batch_urls)\n",
    "    documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "733ee65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_metadata(doc):\n",
    "    if \"dl_meta\" in doc.metadata:\n",
    "        # Convert all hash values to strings\n",
    "        if \"origin\" in doc.metadata[\"dl_meta\"]:\n",
    "            origin = doc.metadata[\"dl_meta\"][\"origin\"]\n",
    "            if \"binary_hash\" in origin:\n",
    "                origin[\"binary_hash\"] = str(origin[\"binary_hash\"])\n",
    "    return doc\n",
    "\n",
    "documents = [clean_metadata(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e0f8790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- d.page_content='Parallel Iterative Edit Models for Local Sequence Transduction\\nAbhijeet Awasthi ∗ , Sunita Sarawagi , Rasna Goyal , Sabyasachi Ghosh , Vihari Piratla Department of Computer Science and Engineering, IIT Bombay'\n",
      "- d.page_content='Abstract\\nWe present a Parallel Iterative Edit (PIE) model for the problem of local sequence transduction arising in tasks like Grammatical error correction (GEC). Recent approaches are based on the popular encoder-decoder (ED) model for sequence to sequence learning. The ED model auto-regressively captures full dependency among output tokens but is slow due to sequential decoding. The PIE model does parallel decoding, giving up the advantage of modelling full dependency in the output, yet it achieves accuracy competitive with the ED model for four reasons: 1. predicting edits instead of tokens, 2. labeling sequences instead of generating sequences, 3. iteratively refining predictions to capture dependencies, and 4. factorizing logits over edits and their token argument to harness pretrained language models like BERT. Experiments on tasks spanning GEC, OCR correction and spell correction demonstrate that the PIE model is an accurate and significantly faster alternative for local sequence transduction. The code and pre-trained models for GEC are available at https://github. com/awasthiabhijeet/PIE .'\n",
      "- d.page_content='Abstract\\nquence transduction task is cast as sequence to sequence (seq2seq) learning and modeled popularly using an attentional encoder-decoder (ED) model. The ED model auto-regressively produces each token y t in the output sequence conditioned on all previous tokens y 1 , . . . , y t -1 . Owing to the remarkable success of this model in challenging tasks like translation, almost all state-of-the-art neural models for GEC use it (Zhao et al., 2019; Lichtarge et al., 2019; Ge et al., 2018b; Chollampatt and Ng, 2018b; Junczys-Dowmunt et al., 2018).'\n"
     ]
    }
   ],
   "source": [
    "for d in documents[:3]:\n",
    "    print(f\"- {d.page_content=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e1bcc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "\n",
    "def create_index_with_mapping(es_url, index_name):\n",
    "    from elasticsearch import Elasticsearch\n",
    "    es = Elasticsearch(es_url)\n",
    "\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"content\": {\"type\": \"text\"},\n",
    "                \"metadata\": {\n",
    "                    \"properties\": {\n",
    "                        \"dl_meta\": {\n",
    "                            \"properties\": {\n",
    "                                \"origin\": {\n",
    "                                    \"properties\": {\n",
    "                                        \"binary_hash\": {\"type\": \"keyword\"}  # Critical fix\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if es.indices.exists(index=index_name):\n",
    "        es.indices.delete(index=index_name)\n",
    "    es.indices.create(index=index_name, body=mapping)\n",
    "\n",
    "create_index_with_mapping(\"http://localhost:9200\", \"test_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39a2b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = ElasticsearchStore(\n",
    "    es_url=\"http://localhost:9200\",\n",
    "    index_name=\"test_index\",\n",
    "    strategy=ElasticsearchStore.BM25RetrievalStrategy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97f8adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = db.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f06e6052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.901587\n",
      "http://arxiv.org/pdf/2310.20360\n",
      "page_content='1.2.6 Gaussian error linear unit (GELU) activation\n",
      "Another popular activation function is the GELU activation function first introduced in Hendrycks & Gimpel [201]. This activation function is the subject of the next definition.\n",
      "Definition 1.2.15 (GELU activation function) . We say that a is the GELU unit activation function (we say that a is the GELU activation function) if and only if it holds that a : R → R is the function from R to R which satisfies for all x ∈ R that\n",
      "<!-- formula-not-decoded -->' metadata={'source': 'http://arxiv.org/pdf/2310.20360', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/538', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 39, 'bbox': {'l': 56.26, 't': 424.6710146484375, 'r': 515.91, 'b': 395.1370146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 177]}]}, {'self_ref': '#/texts/539', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 39, 'bbox': {'l': 64.016, 't': 383.1220146484375, 'r': 507.65, 'b': 340.6490146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 247]}]}, {'self_ref': '#/texts/540', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 39, 'bbox': {'l': 204.022, 't': 328.9190146484375, 'r': 509.057, 'b': 298.1560146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 54]}]}], 'headings': ['1.2.6 Gaussian error linear unit (GELU) activation'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': '11743000372918994593', 'filename': '2310.20360v2.pdf'}}}\n",
      "-----------------------\n",
      "\n",
      "11.688875\n",
      "http://arxiv.org/pdf/2310.20360\n",
      "page_content='1.2.12 Exponential linear unit (ELU) activation\n",
      "Another popular activation function is the so-called exponential linear unit (ELU) activation function which has been introduced in Clevert et al. [85]. This activation function is the subject of the next notion.\n",
      "Definition 1.2.34 (ELU activation functions) . Let γ ∈ ( -∞ , 0] . Then we say that a is the ELU activation function with asymptotic γ if and only if it holds that a : R → R is the function from R to R which satisfies for all x ∈ R that\n",
      "<!-- formula-not-decoded -->\n",
      "Figure 1.13 ( plots/elu.pdf ): A plot of the ELU activation function with asymptotic -1 , the leaky ReLU activation function with leak factor 1 / 10 , and the ReLU activation function\n",
      "```\n",
      "1 import numpy as np 2 import tensorflow as tf 3 import matplotlib.pyplot as 4 import plot_util 5\n",
      "```\n",
      "```\n",
      "plt\n",
      "```' metadata={'source': 'http://arxiv.org/pdf/2310.20360', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/732', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 49, 'bbox': {'l': 56.26, 't': 630.5170146484376, 'r': 515.905, 'b': 586.5370146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 212]}]}, {'self_ref': '#/texts/733', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 49, 'bbox': {'l': 64.016, 't': 573.7940146484375, 'r': 507.686, 'b': 531.3210146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 236]}]}, {'self_ref': '#/texts/734', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 49, 'bbox': {'l': 200.989, 't': 518.3640146484375, 'r': 509.057, 'b': 486.0140146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 54]}]}, {'self_ref': '#/texts/754', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 49, 'bbox': {'l': 72.767, 't': 269.0590146484376, 'r': 499.832, 'b': 225.08001464843755, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 183]}]}, {'self_ref': '#/texts/755', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'code', 'prov': [{'page_no': 49, 'bbox': {'l': 63.071, 't': 205.44901464843747, 'r': 243.424, 'b': 150.40701464843755, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 97]}]}, {'self_ref': '#/texts/756', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'code', 'prov': [{'page_no': 49, 'bbox': {'l': 251.175, 't': 181.5390146484375, 'r': 268.437, 'b': 173.24001464843752, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 3]}]}], 'headings': ['1.2.12 Exponential linear unit (ELU) activation'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': '11743000372918994593', 'filename': '2310.20360v2.pdf'}}}\n",
      "-----------------------\n",
      "\n",
      "11.6779\n",
      "http://arxiv.org/pdf/2310.20360\n",
      "page_content='1.2.5 Softplus activation\n",
      "Definition 1.2.11 (Softplus activation function) . We say that a is the softplus activation function if and only if it holds that a : R → R is the function from R to R which satisfies for all x ∈ R that\n",
      "<!-- formula-not-decoded -->\n",
      "Figure 1.6 ( plots/softplus.pdf ): A plot of the softplus activation function and the ReLU activation function' metadata={'source': 'http://arxiv.org/pdf/2310.20360', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/492', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 37, 'bbox': {'l': 64.912, 't': 606.0440146484375, 'r': 507.69, 'b': 563.5710146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 202]}]}, {'self_ref': '#/texts/493', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 37, 'bbox': {'l': 229.464, 't': 562.7060146484375, 'r': 509.057, 'b': 547.6180146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 35]}]}, {'self_ref': '#/texts/515', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 37, 'bbox': {'l': 72.767, 't': 365.3400146484375, 'r': 499.828, 'b': 335.8060146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 110]}]}], 'headings': ['1.2.5 Softplus activation'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': '11743000372918994593', 'filename': '2310.20360v2.pdf'}}}\n",
      "-----------------------\n",
      "\n",
      "11.655792\n",
      "http://arxiv.org/pdf/2310.20360\n",
      "page_content='1.2.8 Swish activation\n",
      "Definition 1.2.22 (Swish activation functions) . Let β ∈ R . Then we say that a is the swish activation function with parameter β if and only if it holds that a : R → R is the function from R to R which satisfies for all x ∈ R that\n",
      "<!-- formula-not-decoded -->\n",
      "Figure 1.9 ( plots/swish.pdf ): Aplot of the swish activation function with parameter 1 , the GELU activation function, and the ReLU activation function' metadata={'source': 'http://arxiv.org/pdf/2310.20360', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/616', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 43, 'bbox': {'l': 64.291, 't': 600.4230146484375, 'r': 507.692, 'b': 557.9500146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 231]}]}, {'self_ref': '#/texts/617', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 43, 'bbox': {'l': 229.413, 't': 548.0110146484375, 'r': 509.057, 'b': 521.2750146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 35]}]}, {'self_ref': '#/texts/637', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 43, 'bbox': {'l': 72.182, 't': 360.1060146484375, 'r': 500.057, 'b': 330.5720146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 152]}]}], 'headings': ['1.2.8 Swish activation'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': '11743000372918994593', 'filename': '2310.20360v2.pdf'}}}\n",
      "-----------------------\n",
      "\n",
      "11.646156\n",
      "http://arxiv.org/pdf/2310.20360\n",
      "page_content='1.2.15 Heaviside activation\n",
      "Definition 1.2.41 (Heaviside activation function) . We say that a is the Heaviside activation function (we say that a is the Heaviside step function, we say that a is the unit step function) if and only if it holds that a : R → R is the function from R to R which satisfies for all x ∈ R that\n",
      "<!-- formula-not-decoded -->\n",
      "Figure 1.16 ( plots/heaviside.pdf ): A plot of the Heaviside activation function and the standard logistic activation function' metadata={'source': 'http://arxiv.org/pdf/2310.20360', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/810', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 53, 'bbox': {'l': 64.016, 't': 737.1820146484375, 'r': 507.69, 'b': 680.2640146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 292]}]}, {'self_ref': '#/texts/811', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 53, 'bbox': {'l': 203.281, 't': 667.3070146484375, 'r': 509.057, 'b': 616.3780146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 59]}]}, {'self_ref': '#/texts/826', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 53, 'bbox': {'l': 72.767, 't': 509.6450146484375, 'r': 499.826, 'b': 480.1110146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 126]}]}], 'headings': ['1.2.15 Heaviside activation'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': '11743000372918994593', 'filename': '2310.20360v2.pdf'}}}\n",
      "-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = db.similarity_search_with_score(query=\"activation function\", k=5)\n",
    "for doc, score in results:\n",
    "    print(score)\n",
    "    print(doc.metadata['source'])\n",
    "    print(doc)\n",
    "    print('-----------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a734ed",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db2d84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.901587\n",
      "http://arxiv.org/pdf/2310.20360\n",
      "page_content='1.2.6 Gaussian error linear unit (GELU) activation\n",
      "Another popular activation function is the GELU activation function first introduced in Hendrycks & Gimpel [201]. This activation function is the subject of the next definition.\n",
      "Definition 1.2.15 (GELU activation function) . We say that a is the GELU unit activation function (we say that a is the GELU activation function) if and only if it holds that a : R → R is the function from R to R which satisfies for all x ∈ R that\n",
      "<!-- formula-not-decoded -->' metadata={'source': 'http://arxiv.org/pdf/2310.20360', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/538', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 39, 'bbox': {'l': 56.26, 't': 424.6710146484375, 'r': 515.91, 'b': 395.1370146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 177]}]}, {'self_ref': '#/texts/539', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 39, 'bbox': {'l': 64.016, 't': 383.1220146484375, 'r': 507.65, 'b': 340.6490146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 247]}]}, {'self_ref': '#/texts/540', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 39, 'bbox': {'l': 204.022, 't': 328.9190146484375, 'r': 509.057, 'b': 298.1560146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 54]}]}], 'headings': ['1.2.6 Gaussian error linear unit (GELU) activation'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': '11743000372918994593', 'filename': '2310.20360v2.pdf'}}}\n",
      "-----------------------\n",
      "\n",
      "11.688875\n",
      "http://arxiv.org/pdf/2310.20360\n",
      "page_content='1.2.12 Exponential linear unit (ELU) activation\n",
      "Another popular activation function is the so-called exponential linear unit (ELU) activation function which has been introduced in Clevert et al. [85]. This activation function is the subject of the next notion.\n",
      "Definition 1.2.34 (ELU activation functions) . Let γ ∈ ( -∞ , 0] . Then we say that a is the ELU activation function with asymptotic γ if and only if it holds that a : R → R is the function from R to R which satisfies for all x ∈ R that\n",
      "<!-- formula-not-decoded -->\n",
      "Figure 1.13 ( plots/elu.pdf ): A plot of the ELU activation function with asymptotic -1 , the leaky ReLU activation function with leak factor 1 / 10 , and the ReLU activation function\n",
      "```\n",
      "1 import numpy as np 2 import tensorflow as tf 3 import matplotlib.pyplot as 4 import plot_util 5\n",
      "```\n",
      "```\n",
      "plt\n",
      "```' metadata={'source': 'http://arxiv.org/pdf/2310.20360', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/732', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 49, 'bbox': {'l': 56.26, 't': 630.5170146484376, 'r': 515.905, 'b': 586.5370146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 212]}]}, {'self_ref': '#/texts/733', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 49, 'bbox': {'l': 64.016, 't': 573.7940146484375, 'r': 507.686, 'b': 531.3210146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 236]}]}, {'self_ref': '#/texts/734', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 49, 'bbox': {'l': 200.989, 't': 518.3640146484375, 'r': 509.057, 'b': 486.0140146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 54]}]}, {'self_ref': '#/texts/754', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 49, 'bbox': {'l': 72.767, 't': 269.0590146484376, 'r': 499.832, 'b': 225.08001464843755, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 183]}]}, {'self_ref': '#/texts/755', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'code', 'prov': [{'page_no': 49, 'bbox': {'l': 63.071, 't': 205.44901464843747, 'r': 243.424, 'b': 150.40701464843755, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 97]}]}, {'self_ref': '#/texts/756', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'code', 'prov': [{'page_no': 49, 'bbox': {'l': 251.175, 't': 181.5390146484375, 'r': 268.437, 'b': 173.24001464843752, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 3]}]}], 'headings': ['1.2.12 Exponential linear unit (ELU) activation'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': '11743000372918994593', 'filename': '2310.20360v2.pdf'}}}\n",
      "-----------------------\n",
      "\n",
      "11.6779\n",
      "http://arxiv.org/pdf/2310.20360\n",
      "page_content='1.2.5 Softplus activation\n",
      "Definition 1.2.11 (Softplus activation function) . We say that a is the softplus activation function if and only if it holds that a : R → R is the function from R to R which satisfies for all x ∈ R that\n",
      "<!-- formula-not-decoded -->\n",
      "Figure 1.6 ( plots/softplus.pdf ): A plot of the softplus activation function and the ReLU activation function' metadata={'source': 'http://arxiv.org/pdf/2310.20360', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/492', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 37, 'bbox': {'l': 64.912, 't': 606.0440146484375, 'r': 507.69, 'b': 563.5710146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 202]}]}, {'self_ref': '#/texts/493', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 37, 'bbox': {'l': 229.464, 't': 562.7060146484375, 'r': 509.057, 'b': 547.6180146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 35]}]}, {'self_ref': '#/texts/515', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 37, 'bbox': {'l': 72.767, 't': 365.3400146484375, 'r': 499.828, 'b': 335.8060146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 110]}]}], 'headings': ['1.2.5 Softplus activation'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': '11743000372918994593', 'filename': '2310.20360v2.pdf'}}}\n",
      "-----------------------\n",
      "\n",
      "11.655792\n",
      "http://arxiv.org/pdf/2310.20360\n",
      "page_content='1.2.8 Swish activation\n",
      "Definition 1.2.22 (Swish activation functions) . Let β ∈ R . Then we say that a is the swish activation function with parameter β if and only if it holds that a : R → R is the function from R to R which satisfies for all x ∈ R that\n",
      "<!-- formula-not-decoded -->\n",
      "Figure 1.9 ( plots/swish.pdf ): Aplot of the swish activation function with parameter 1 , the GELU activation function, and the ReLU activation function' metadata={'source': 'http://arxiv.org/pdf/2310.20360', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/616', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 43, 'bbox': {'l': 64.291, 't': 600.4230146484375, 'r': 507.692, 'b': 557.9500146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 231]}]}, {'self_ref': '#/texts/617', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 43, 'bbox': {'l': 229.413, 't': 548.0110146484375, 'r': 509.057, 'b': 521.2750146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 35]}]}, {'self_ref': '#/texts/637', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 43, 'bbox': {'l': 72.182, 't': 360.1060146484375, 'r': 500.057, 'b': 330.5720146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 152]}]}], 'headings': ['1.2.8 Swish activation'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': '11743000372918994593', 'filename': '2310.20360v2.pdf'}}}\n",
      "-----------------------\n",
      "\n",
      "11.646156\n",
      "http://arxiv.org/pdf/2310.20360\n",
      "page_content='1.2.15 Heaviside activation\n",
      "Definition 1.2.41 (Heaviside activation function) . We say that a is the Heaviside activation function (we say that a is the Heaviside step function, we say that a is the unit step function) if and only if it holds that a : R → R is the function from R to R which satisfies for all x ∈ R that\n",
      "<!-- formula-not-decoded -->\n",
      "Figure 1.16 ( plots/heaviside.pdf ): A plot of the Heaviside activation function and the standard logistic activation function' metadata={'source': 'http://arxiv.org/pdf/2310.20360', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/810', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 53, 'bbox': {'l': 64.016, 't': 737.1820146484375, 'r': 507.69, 'b': 680.2640146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 292]}]}, {'self_ref': '#/texts/811', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'formula', 'prov': [{'page_no': 53, 'bbox': {'l': 203.281, 't': 667.3070146484375, 'r': 509.057, 'b': 616.3780146484376, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 59]}]}, {'self_ref': '#/texts/826', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'caption', 'prov': [{'page_no': 53, 'bbox': {'l': 72.767, 't': 509.6450146484375, 'r': 499.826, 'b': 480.1110146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 126]}]}], 'headings': ['1.2.15 Heaviside activation'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': '11743000372918994593', 'filename': '2310.20360v2.pdf'}}}\n",
      "-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "\n",
    "db = ElasticsearchStore(\n",
    "    es_url=\"http://localhost:9200\",\n",
    "    index_name=\"test_index\",\n",
    "    strategy=ElasticsearchStore.BM25RetrievalStrategy(),\n",
    ")\n",
    "\n",
    "results = db.similarity_search_with_score(query=\"activation function\", k=5)\n",
    "for doc, score in results:\n",
    "    print(score)\n",
    "    print(doc.metadata['source'])\n",
    "    print(doc)\n",
    "    print('-----------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86923f06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
