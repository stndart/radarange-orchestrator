{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc7b9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from radarange_orchestrator import llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4f41a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (10000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    }
   ],
   "source": [
    "from radarange_orchestrator.llm_backend import LLM_Config\n",
    "from radarange_orchestrator.tools import time_tool\n",
    "\n",
    "# m = llm(model='qwq-32b@q4_k_m', backend='lmstudio')\n",
    "m = llm(model='QwQ-32B-Q4_K_M.gguf', backend='llama_cpp', config=LLM_Config(gpus=[1], ctx_size=10000))\n",
    "chat = m.chat(tools=[time_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb6d81e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"\"\"What time is it?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57cd061e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history=[UserMessage(role='user', content='What time is it?')] tools=[Tool(name='CurrentTime', description='Gets actual current time in local timezone', func=<function get_time at 0x7a9acf03ad40>)] CODE_EXTS=['.h', '.hpp', '.c', '.cpp', '.cu', '.txt', '.proto', '.py']\n",
      "tool_calls [{'name': 'CurrentTime', 'args': {'tool_input': ''}, 'id': 'CurrentTime_1', 'type': 'tool_call'}]\n",
      "history=[UserMessage(role='user', content='What time is it?'), AssistantMessage(role='assistant', content='<think>\\nOkay, the user is asking, \"What time is it?\" I need to figure out which tool to use here. Let me check the tools provided. There\\'s a function called CurrentTime that gets the actual current time in the local timezone. The parameters require a tool_input, which is a string. \\n\\nWait, the function\\'s parameters have a tool_input, but the user didn\\'t specify a timezone. Maybe the function defaults to the local timezone of the user? The description says it\\'s the local timezone, so maybe the tool_input isn\\'t necessary here. Or perhaps the function expects the user\\'s timezone as input. Hmm, but the user\\'s question is straightforwardâ€”they just want the current time. \\n\\nSince the function\\'s description mentions it gets the local timezone, maybe the tool_input can be left empty or just pass a placeholder. Alternatively, maybe the function automatically detects the local timezone. But according to the parameters, the tool_input is a string. Maybe the user\\'s timezone should be provided here. \\n\\nWait, the parameters for the function have a tool_input parameter of type string. But the user didn\\'t specify a timezone. Maybe the function is designed to take the user\\'s current timezone as input. But since the user hasn\\'t provided that, perhaps I should call the function without any specific input? Or maybe the function can handle it with just an empty string?\\n\\nAlternatively, maybe the tool_input is required. Let me check the parameters again. The properties for the function\\'s parameters have \"tool_input\" as a required field? Looking at the JSON, under parameters, the properties include \"tool_input\", but it\\'s not marked as required. Wait, the parameters are an object with properties, but there\\'s no \"required\" array. So maybe tool_input is optional?\\n\\nWait, the parameters are defined as a type object with properties, but without specifying required fields. So perhaps the function can be called without any arguments. But the user just wants the current time in their local timezone. If the function can get that without needing the input, maybe passing an empty string for tool_input is okay. \\n\\nAlternatively, perhaps the tool_input should be the timezone identifier like \"America/New_York\", but since the user didn\\'t specify, maybe the function is designed to use the local timezone of the server or the user\\'s default. \\n\\nIn any case, according to the function\\'s description, it says it gets the current time in the local timezone, so maybe the tool_input isn\\'t needed here. But the function\\'s parameters require it. Wait, looking at the parameters again:\\n\\n\"parameters\": {\"properties\": {\"tool_input\": {\"type\": \"string\"}}, \"type\": \"object\"}\\n\\nSo the tool expects a string input, but the description says it\\'s for getting the local timezone time. Maybe the tool_input should be the user\\'s timezone, but since the user didn\\'t provide it, perhaps the function can infer it. But since we need to call the function, we have to provide the parameters. Maybe the tool_input should be an empty string, or perhaps \"local\".\\n\\nAlternatively, maybe the tool is designed such that if tool_input is not provided, it defaults to the local timezone. So perhaps pass an empty string. Let me go with that. \\n\\nSo I should call CurrentTime with tool_input as an empty string or maybe just \"local\". Wait, maybe the function expects the user\\'s timezone as a string parameter, but since it\\'s not given, maybe the best way is to call the function with an empty string. Alternatively, perhaps the tool_input is a placeholder. \\n\\nAlternatively, perhaps the user expects the assistant to know their timezone, but since that\\'s not available, maybe the function can handle it. Let me proceed by calling CurrentTime with an empty tool_input. \\n\\nTherefore, the tool call would be:\\n\\n{\"name\": \"CurrentTime\", \"arguments\": {\"tool_input\": \"\"}}\\n\\nWait, but maybe the parameters require the tool_input, so even if it\\'s empty, it\\'s okay. Alternatively, perhaps the function can be called without it, but according to the parameters, it\\'s part of the properties. So including an empty string is necessary. \\n\\nAlright, I\\'ll make the tool call with an empty string for tool_input.\\n</think>\\n\\n<tool_call>\\n{\"name\": \"CurrentTime\", \"arguments\": {\"tool_input\": \"\"}}\\n</tool_call>', finish_reason='tool_call', tool_calls=[{'name': 'CurrentTime', 'args': {'tool_input': ''}, 'id': 'CurrentTime_1', 'type': 'tool_call'}]), ToolMessage(content='2025-07-09 01:33:39', tool_call_id='CurrentTime_1')] tools=[Tool(name='CurrentTime', description='Gets actual current time in local timezone', func=<function get_time at 0x7a9acf03ad40>)] CODE_EXTS=['.h', '.hpp', '.c', '.cpp', '.cu', '.txt', '.proto', '.py']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type Undefined is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chat\u001b[38;5;241m.\u001b[39madd_user_message(message)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(f\"Your message is {m.model.model.model.count_tokens(message)} tokens\")\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens_per_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m chat\u001b[38;5;241m.\u001b[39mshow_final_answer()\n",
      "File \u001b[0;32m~/radarange-orchestrator/radarange_orchestrator/llm.py:230\u001b[0m, in \u001b[0;36mllm.act\u001b[0;34m(self, prompt, tools, on_message, temperature, max_tokens_per_message, max_prediction_rounds, response_format)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama_cpp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_prediction_rounds):\n\u001b[0;32m--> 230\u001b[0m         response: AssistantMessage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrespond\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens_per_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m         on_message(response)\n\u001b[1;32m    237\u001b[0m         chat\u001b[38;5;241m.\u001b[39mappend(response)\n",
      "File \u001b[0;32m~/radarange-orchestrator/radarange_orchestrator/llm.py:135\u001b[0m, in \u001b[0;36mllm.respond\u001b[0;34m(self, prompt, tools, temperature, max_tokens, response_format)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, Chat):\n\u001b[1;32m    133\u001b[0m     chat\u001b[38;5;241m.\u001b[39madd_user_message(prompt)\n\u001b[0;32m--> 135\u001b[0m chat_completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion\n",
      "File \u001b[0;32m~/radarange-orchestrator/radarange_orchestrator/llm_backend.py:113\u001b[0m, in \u001b[0;36mModel.create_chat_completion\u001b[0;34m(self, chat, tools, response_format, temperature, max_tokens, stream)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response_format\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    108\u001b[0m     chat\u001b[38;5;241m.\u001b[39madd_system_message(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124m    User wants you to answer in the following format:\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_format\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m()\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/radarange-orchestrator/radarange_orchestrator/backend/llama_cpp_model.py:131\u001b[0m, in \u001b[0;36mLlamaModel.create_chat_completion\u001b[0;34m(self, chat, tools, response_format, temperature, max_tokens, stream)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# response: CreateChatCompletionResponse\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chat)\n\u001b[0;32m--> 131\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     tool_calls \u001b[38;5;241m=\u001b[39m extract_tool_calls(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    141\u001b[0m     finish_reason \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinish_reason\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tool_calls) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtool_call\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/radarange-orchestrator/.venv/lib/python3.10/site-packages/llama_cpp/llama.py:2003\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[0m\n\u001b[1;32m   1965\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[1;32m   1966\u001b[0m \n\u001b[1;32m   1967\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;124;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[1;32m   1997\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1998\u001b[0m handler \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_handler\n\u001b[1;32m   2000\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_handlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   2001\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format\u001b[38;5;241m.\u001b[39mget_chat_completion_handler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   2002\u001b[0m )\n\u001b[0;32m-> 2003\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/radarange-orchestrator/.venv/lib/python3.10/site-packages/llama_cpp/llama_chat_format.py:590\u001b[0m, in \u001b[0;36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat_completion_handler\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    554\u001b[0m     llama: llama\u001b[38;5;241m.\u001b[39mLlama,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     Iterator[llama_types\u001b[38;5;241m.\u001b[39mCreateChatCompletionStreamResponse],\n\u001b[1;32m    589\u001b[0m ]:\n\u001b[0;32m--> 590\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mchat_formatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m llama\u001b[38;5;241m.\u001b[39mtokenize(\n\u001b[1;32m    598\u001b[0m         result\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    599\u001b[0m         add_bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39madded_special,\n\u001b[1;32m    600\u001b[0m         special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/radarange-orchestrator/.venv/lib/python3.10/site-packages/llama_cpp/llama_chat_format.py:230\u001b[0m, in \u001b[0;36mJinja2ChatFormatter.__call__\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraise_exception\u001b[39m(message: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n\u001b[0;32m--> 230\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m stopping_criteria \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/radarange-orchestrator/.venv/lib/python3.10/site-packages/jinja2/environment.py:1295\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/radarange-orchestrator/.venv/lib/python3.10/site-packages/jinja2/environment.py:942\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 942\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:38\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/radarange-orchestrator/.venv/lib/python3.10/site-packages/jinja2/filters.py:1721\u001b[0m, in \u001b[0;36mdo_tojson\u001b[0;34m(eval_ctx, value, indent)\u001b[0m\n\u001b[1;32m   1718\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   1719\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m indent\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhtmlsafe_json_dumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdumps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/radarange-orchestrator/.venv/lib/python3.10/site-packages/jinja2/utils.py:669\u001b[0m, in \u001b[0;36mhtmlsafe_json_dumps\u001b[0;34m(obj, dumps, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dumps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m     dumps \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m markupsafe\u001b[38;5;241m.\u001b[39mMarkup(\n\u001b[0;32m--> 669\u001b[0m     \u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mu003c\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mu003e\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mu0026\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mu0027\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    674\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Undefined is not JSON serializable"
     ]
    }
   ],
   "source": [
    "chat.add_user_message(message)\n",
    "# print(f\"Your message is {m.model.model.model.count_tokens(message)} tokens\")\n",
    "m.act(chat, max_tokens_per_message = 10000, temperature=0.8, on_message=chat.append)\n",
    "chat.show_final_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237197f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, the user is asking for the current time. I need to use the CurrentTime function provided. The function requires a tool_input parameter, but looking at the example, it seems like it can be an empty string. Let me check the function description again. It says it gets the actual current time in the local timezone. The parameters are a object with tool_input as a string. Since the user didn't specify a particular timezone, maybe the function defaults to the local one. I'll call CurrentTime with an empty string as the argument. That should get the current time. I'll make sure the JSON is correctly formatted to avoid errors.\\n</think>\\n\\nThe current time is <current_time>.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from llama_cpp import ChatCompletionRequestAssistantMessage\n",
    "from radarange_orchestrator.types.history import AssistantMessage\n",
    "from radarange_orchestrator.types.llama_cpp_bindings import to_llama_tool, to_llama_tool_call\n",
    "\n",
    "def to_llama_assistant_message(message: AssistantMessage) -> ChatCompletionRequestAssistantMessage:\n",
    "    return ChatCompletionRequestAssistantMessage(\n",
    "        role='assistant',\n",
    "        content=message.content,\n",
    "        tool_calls=[to_llama_tool_call(tc) for tc in message.tool_calls],\n",
    "        finish_reason=message.finish_reason\n",
    "    )\n",
    "\n",
    "nchat = deepcopy(chat)\n",
    "nchat.history[1] = to_llama_assistant_message(nchat.history[1])\n",
    "\n",
    "response = m.model.model.llm.create_chat_completion(\n",
    "    nchat,\n",
    "    tools=[to_llama_tool(time_tool)],\n",
    ")\n",
    "response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e51123c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking \"What time is it?\" so they want to know the current time. I need to use the CurrentTime function provided. The function requires a tool_input parameter, but the user didn't specify any particular location. Hmm, maybe the tool just needs a placeholder. Let me check the function description again. It says it gets the actual current time in the local timezone. Wait, does the user's local timezone need to be specified here? The parameters for the function have a tool_input of type string. Maybe I should assume the user's timezone, but since I don't have that info, perhaps I can just call the function with an empty string or a default. Alternatively, maybe the function can handle it without additional input. The description mentions \"local timezone\" so maybe the function is set up to get the local time based on the server's settings. I'll proceed by calling CurrentTime with an empty tool_input or just a placeholder like \"user's local timezone\". Let me structure the tool call accordingly.\n",
      "</think>\n",
      "\n",
      "<tool_call>\n",
      "{\"name\": \"CurrentTime\", \"arguments\": {\"tool_input\": \"user's local timezone\"}}\n",
      "</tool_call>\n"
     ]
    }
   ],
   "source": [
    "from radarange_orchestrator.chat import Chat\n",
    "\n",
    "c1 = Chat(message)\n",
    "r1 = m.model.model.llm.create_chat_completion(\n",
    "    c1,\n",
    "    tools=[to_llama_tool(time_tool)],\n",
    ")\n",
    "print(r1['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c55a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from radarange_orchestrator.utils.extract_tool_calls import extract_tool_calls\n",
    "\n",
    "\n",
    "tool_calls = extract_tool_calls(r1['choices'][0]['message']['content'])\n",
    "finish_reason = r1['choices'][0]['finish_reason'] if len(tool_calls) == 0 else 'tool_call'\n",
    "\n",
    "rp1 = AssistantMessage(\n",
    "    role='assistant',\n",
    "    content=r1['choices'][0]['message']['content'],\n",
    "    tool_calls=tool_calls,\n",
    "    finish_reason=finish_reason\n",
    ")\n",
    "c1.append(rp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "036125b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'CurrentTime', 'args': {'tool_input': \"user's local timezone\"}, 'id': 'CurrentTime_2', 'type': 'tool_call'}]\n",
      "content='2025-07-09 01:34:00' name='CurrentTime' tool_call_id='CurrentTime_2'\n"
     ]
    }
   ],
   "source": [
    "print(rp1.tool_calls)\n",
    "t1 = time_tool.run(tool_input=rp1.tool_calls[0]['args'], tool_call_id=rp1.tool_calls[0]['id'])\n",
    "print(t1)\n",
    "c1.append(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26ab6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_fixed = [\n",
    "    c1[0],\n",
    "    to_llama_assistant_message(c1[1]),\n",
    "    c1[2]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5cdd2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking for the current time. I need to use the CurrentTime function. The function requires the user's local timezone, but they didn't specify which one. I should call the function with the placeholder \"user's local timezone\" as instructed. Wait, the example in the tools shows that the tool_input is a string. Maybe I should just pass that exact phrase. Let me check the function parameters again. The tool_input is a string, so I'll use that. I'll make the tool call with that argument. Once I get the response, I can format the answer to show the current time in their timezone.\n",
      "</think>\n",
      "\n",
      "The current time in your local timezone is: **<current_time>**. \n",
      "\n",
      "Please note that I couldn't directly detect your timezone, so you might need to specify your location for more accurate results.\n"
     ]
    }
   ],
   "source": [
    "r2 = m.model.model.llm.create_chat_completion(\n",
    "    c_fixed,\n",
    "    tools=[to_llama_tool(time_tool)],\n",
    ")\n",
    "print(r2['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745dab2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc1c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='Tool call error: Too many arguments to single-input tool CurrentTime.\\n                Consider using StructuredTool instead. Args: []\\nTraceback: <traceback object at 0x72521022c180>', tool_call_id='')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from radarange_orchestrator.tools import ToolCall, InvalidToolCall\n",
    "from radarange_orchestrator.types.history import AssistantMessage\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "tc = ToolCall(\n",
    "    {'name': 'CurrentTime', 'args': {}, 'id': 'CurrentTime_1', 'type': 'tool_call'}\n",
    ")\n",
    "\n",
    "ma = AssistantMessage(\n",
    "    role='assistant',\n",
    "    content='kek',\n",
    "    tool_calls=[tc],\n",
    "    finish_reason='tool_call',\n",
    ")\n",
    "\n",
    "ToolMessage(\n",
    "    content='Tool call error: Too many arguments to single-input tool CurrentTime.\\n                Consider using StructuredTool instead. Args: []\\nTraceback: <traceback object at 0x72521022c180>',\n",
    "    tool_call_id='',\n",
    "    status='success',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from radarange_orchestrator.types.llama_cpp_bindings import to_llama_tool\n",
    "\n",
    "\n",
    "tools = [to_llama_tool(time_tool)]\n",
    "response = m.model.model.llm.create_chat_completion(\n",
    "    message,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550053a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-8a183599-b5cb-4aa6-bd49-ff7dc2455e41\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1752009789,\n",
      "  \"model\": \"/root/radarange-orchestrator/radarange_orchestrator/utils/../../models/QwQ-32B-Q4_K_M.gguf\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"<think>\\nOkay, the user asked for the current time and date. Let me check the tools available. There's a function called CurrentTime that does exactly that. It doesn't require any parameters, so I just need to call it. I'll make sure to format the tool call correctly within the XML tags. Alright, I'll return the function name and an empty arguments object since there's nothing needed here.\\n</think>\\n\\n<tool_call>\\n{\\\"name\\\": \\\"CurrentTime\\\", \\\"arguments\\\": {}}\\n</tool_call>\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 125,\n",
      "    \"completion_tokens\": 100,\n",
      "    \"total_tokens\": 225\n",
      "  }\n",
      "}\n",
      "<think>\n",
      "Okay, the user asked for the current time and date. Let me check the tools available. There's a function called CurrentTime that does exactly that. It doesn't require any parameters, so I just need to call it. I'll make sure to format the tool call correctly within the XML tags. Alright, I'll return the function name and an empty arguments object since there's nothing needed here.\n",
      "</think>\n",
      "\n",
      "<tool_call>\n",
      "{\"name\": \"CurrentTime\", \"arguments\": {}}\n",
      "</tool_call>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(response, indent=2))\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb893be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got tools from call [] and chat tools [Tool(name='CurrentTime', description='Shows current time and date.', func=<function get_time at 0x79eb3e53edd0>, zero_args=True)]\n",
      "act got response role='assistant' content='<think>\\nOkay, the user just asked, \"And the weather is good, isn\\'t it?\" Let me think about how to respond.\\n\\nFirst, looking at the tools provided earlier, there\\'s only the CurrentTime function. The user is now asking about the weather, which isn\\'t covered by the available tools. So I can\\'t use a function call here. \\n\\nI need to let the user know that I don\\'t have access to weather information. But I should do it politely. Maybe say something like, \"I don\\'t have access to current weather data...\" and offer to help with something else. \\n\\nWait, the user might be expecting a follow-up after the time question. Since I can\\'t answer the weather part, I should acknowledge their statement and redirect. Let me check the response structure again. The user\\'s previous question used a tool, but this one can\\'t. So the answer should just be a regular message without any tool calls.\\n\\nHmm, also, the user\\'s message is a bit conversational, so keeping the response friendly is important. Make sure not to make up any weather details. Just state the limitation clearly. Alright, I\\'ll compose a response that informs them of the lack of tool support and offers assistance with other queries.\\n</think>\\n\\nI don\\'t have access to current weather data or the ability to check real-time weather conditions. My capabilities are limited to the tools provided, which currently only include showing the current time and date. Let me know if you\\'d like me to share the current time again or help with something else!' finish_reason='stop' tool_calls=[]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "I don't have access to current weather data or the ability to check real-time weather conditions. My capabilities are limited to the tools provided, which currently only include showing the current time and date. Let me know if you'd like me to share the current time again or help with something else!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"And the weather is good, isn't it?\"\n",
    "chat.add_user_message(prompt)\n",
    "m.act(chat, max_tokens_per_message = 10000, temperature=0.8, on_message=chat.append)\n",
    "chat.show_final_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0235ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
