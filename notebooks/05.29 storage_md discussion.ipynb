{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91437f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from radarange_orchestrator import llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa08b7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (80000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from radarange_orchestrator.tools import all_tools\n",
    "m = llm()\n",
    "chat = m.chat(tools = all_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "158f1cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17425"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Here's some examples of usage of Storage:\n",
    "\n",
    "# storage.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import typing\n",
    "from concurrent.futures import Executor, Future\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "from typing import Any, TypeVar\n",
    "\n",
    "from pydantic import BaseModel, ConfigDict, field_validator\n",
    "\n",
    "from mariotte import config, date, instrument\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class StorageMode(str, Enum):\n",
    "    # try read, write on error, ignore write error, after ignore read errors\n",
    "    Default = \"Default\"\n",
    "    Write = \"Write\"  # fail on write\n",
    "    WriteIgnoreErrors = \"WriteIgnoreErrors\"\n",
    "    Read = \"Read\"  # fail on read\n",
    "    ReadIgnoreErrors = \"ReadIgnoreErrors\"\n",
    "\n",
    "\n",
    "class StorageRequest(BaseModel):\n",
    "    base_path: str\n",
    "    instr: instrument.Instrument\n",
    "    start: datetime\n",
    "    end: datetime\n",
    "    name: str\n",
    "    addt: Any\n",
    "\n",
    "    def full_path(self) -> str:\n",
    "        return os.path.join(\n",
    "            self.base_path,\n",
    "            self.name,\n",
    "            self.instr.market,\n",
    "            self.instr.pair,\n",
    "            date.to_str(self.start),\n",
    "        )\n",
    "\n",
    "    def info(self):\n",
    "        return (self.instr.market, self.instr.pair, self.start, self.end)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.instr.market}/{self.instr.pair} {self.start}\"\n",
    "\n",
    "\n",
    "class InstrumentInterval(BaseModel):\n",
    "    instrument: instrument.Instrument\n",
    "    start: datetime\n",
    "    end: datetime\n",
    "\n",
    "    @field_validator(\"start\", \"end\", mode=\"before\")\n",
    "    @classmethod\n",
    "    def date_from_str(cls, value: str | datetime) -> datetime:\n",
    "        if isinstance(value, str):\n",
    "            return date.from_str(value)\n",
    "        return value\n",
    "\n",
    "    @staticmethod\n",
    "    def from_reader(reader: Any) -> list[InstrumentInterval]:\n",
    "        return typing.cast(\n",
    "            list[InstrumentInterval],\n",
    "            config.load(reader, InstrumentInterval),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def from_file(path: str) -> list[InstrumentInterval]:\n",
    "        with open(path, \"r\") as f:\n",
    "            return InstrumentInterval.from_reader(f)\n",
    "\n",
    "\n",
    "class Storage[T](BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    mode: StorageMode\n",
    "    executor: Executor\n",
    "    base_path: str\n",
    "    name: str\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        instrument: instrument.Instrument,\n",
    "        start: datetime,\n",
    "        end: datetime,\n",
    "    ) -> list[tuple[T, StorageRequest]]:\n",
    "        return self.load_multi(\n",
    "            [\n",
    "                InstrumentInterval(\n",
    "                    instrument=instrument,\n",
    "                    start=start,\n",
    "                    end=end,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def load_multi(\n",
    "        self,\n",
    "        instrs: list[InstrumentInterval],\n",
    "    ) -> list[tuple[T, StorageRequest]]:\n",
    "        return self.wait(self.load_async(instrs))\n",
    "\n",
    "    def load_async(\n",
    "        self,\n",
    "        instrs: list[InstrumentInterval],\n",
    "    ) -> list[tuple[StorageRequest, Future[T | None]]]:\n",
    "        requests: list[StorageRequest] = []\n",
    "\n",
    "        for instr in instrs:\n",
    "            intervals = date.split(instr.start, instr.end, timedelta(hours=12))\n",
    "\n",
    "            for interval in intervals:\n",
    "                req = StorageRequest(\n",
    "                    base_path=self.base_path,\n",
    "                    instr=instr.instrument,\n",
    "                    start=interval[0],\n",
    "                    end=interval[1],\n",
    "                    name=self.name,\n",
    "                    addt=None,\n",
    "                )\n",
    "                requests.append(req)\n",
    "        return list(\n",
    "            zip(\n",
    "                requests,\n",
    "                map(lambda x: self.executor.submit(self.load_req, x), requests),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def wait(\n",
    "        self, tasks: list[tuple[StorageRequest, Future[T | None]]]\n",
    "    ) -> list[tuple[T, StorageRequest]]:\n",
    "        res: list[tuple[T, StorageRequest]] = []\n",
    "        for req, task in tasks:\n",
    "            cur_res = task.result()\n",
    "            if cur_res is None:\n",
    "                continue\n",
    "            res.append((cur_res, req))\n",
    "\n",
    "        print(\n",
    "            f\"Read {len(res)} out of {len(tasks)} ({len(res) / max(len(tasks), 1) * 100:.2f}%)\"\n",
    "        )\n",
    "        return res\n",
    "\n",
    "    def load_req(self, req: StorageRequest) -> T | None:\n",
    "        match self.mode:\n",
    "            case StorageMode.Default:\n",
    "                return self.default_req(req)\n",
    "            case StorageMode.Read:\n",
    "                return self.read_req(req)\n",
    "            case StorageMode.ReadIgnoreErrors:\n",
    "                return self.read_ignore_missing_req(req)\n",
    "            case StorageMode.Write:\n",
    "                return self.write_req(req)\n",
    "            case StorageMode.WriteIgnoreErrors:\n",
    "                self.try_write_req(req)\n",
    "                return None\n",
    "\n",
    "    def default_req(self, req: StorageRequest) -> T | None:\n",
    "        try:\n",
    "            return self.read_req(req)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read {req}:\\n{e}\")\n",
    "            self.try_write_req(req)\n",
    "            return self.read_ignore_missing_req(req)\n",
    "\n",
    "    def read_ignore_missing_req(self, req: StorageRequest) -> T | None:\n",
    "        try:\n",
    "            return self.read_req(req)\n",
    "        except Exception as _:\n",
    "            return None\n",
    "\n",
    "    def read_req(self, req: StorageRequest) -> T:\n",
    "        raise NotImplementedError(\"virtual method\")\n",
    "\n",
    "    def try_write_req(self, req: StorageRequest) -> bool:\n",
    "        try:\n",
    "            self.write_req(req)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Could not write {req}:\\n{e}\")\n",
    "            return False\n",
    "\n",
    "    def write_req(self, req: StorageRequest) -> None:\n",
    "        raise NotImplementedError(\"virtual method\")\n",
    "\n",
    "\n",
    "# storage_features.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import polars as pl\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "from mariotte import instrument, paths\n",
    "from mariotte.backtest import Backtest\n",
    "from mariotte.research.storage import Storage, StorageMode, StorageRequest\n",
    "\n",
    "\n",
    "class FeaturesConfig(BaseModel):\n",
    "    traded_ob_window_size: int\n",
    "    sided_ob_window_size: int\n",
    "\n",
    "\n",
    "class FeaturesExporterModelConfig(BaseModel):\n",
    "    instrument: instrument.InfraInstrument\n",
    "    features_file: str\n",
    "    features: FeaturesConfig\n",
    "\n",
    "\n",
    "class FeaturesData(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    ft: pl.LazyFrame\n",
    "\n",
    "\n",
    "FEATURES_STORAGE_PATH = \"/ft_storage/\"\n",
    "\n",
    "\n",
    "class FeaturesStorage(Storage[FeaturesData]):\n",
    "    executable: str = os.path.join(paths.boyle_path(), \"build/allin_features_exporter\")\n",
    "    features_config: FeaturesConfig\n",
    "    run_config_path: str = \"./configs/run_config.json\"\n",
    "    sim_config_path: str = \"./configs/simulation_config.json\"\n",
    "    verbose: int = 1\n",
    "\n",
    "    @staticmethod\n",
    "    def default(jobs: int) -> FeaturesStorage:\n",
    "        return FeaturesStorage(\n",
    "            features_config=FeaturesConfig(\n",
    "                traded_ob_window_size=500_000_000,\n",
    "                sided_ob_window_size=1_000_000_000,\n",
    "            ),\n",
    "            mode=StorageMode.ReadIgnoreErrors,\n",
    "            executor=ThreadPoolExecutor(max_workers=jobs),\n",
    "            base_path=FEATURES_STORAGE_PATH,\n",
    "            name=\"main\",\n",
    "        )\n",
    "\n",
    "    def paths(self, req: StorageRequest) -> tuple[str, str, str, str]:\n",
    "        path = req.full_path()\n",
    "\n",
    "        features = os.path.join(path, \"features.parquet\")\n",
    "\n",
    "        stdout = os.path.join(path, \"stdout.txt\")\n",
    "        stderr = os.path.join(path, \"stderr.txt\")\n",
    "\n",
    "        return path, features, stdout, stderr\n",
    "\n",
    "    def read_req(self, req: StorageRequest) -> FeaturesData:\n",
    "        _, features, _, _ = self.paths(req)\n",
    "\n",
    "        ft = pl.scan_parquet(features)\n",
    "        ft.collect_schema()\n",
    "\n",
    "        return FeaturesData(ft=ft)\n",
    "\n",
    "    def write_req(self, req: StorageRequest) -> None:\n",
    "        path, features, stdout, stderr = self.paths(req)\n",
    "\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        model_config = FeaturesExporterModelConfig(\n",
    "            instrument=req.instr.to_infra(),\n",
    "            features_file=features,\n",
    "            features=self.features_config,\n",
    "        )\n",
    "\n",
    "        backtest = Backtest(\n",
    "            executable=self.executable,\n",
    "            model=model_config,\n",
    "            run=self.run_config_path,\n",
    "            simulation=self.sim_config_path,\n",
    "            start=req.start,\n",
    "            end=req.end,\n",
    "            orders=\"/dev/null\",\n",
    "            stdout=stdout,\n",
    "            stderr=stderr,\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "        backtest.cmd().local_execute()\n",
    "\n",
    "# storage_md.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import polars as pl\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "from mariotte import instrument, paths\n",
    "from mariotte.backtest import Backtest\n",
    "from mariotte.research.storage import Storage, StorageMode, StorageRequest\n",
    "\n",
    "\n",
    "class MdExporterModelConfig(BaseModel):\n",
    "    orderbook_file: str\n",
    "    trades_file: str\n",
    "    orderbook_levels: int\n",
    "    skip_empty_levels: bool\n",
    "    instrument: instrument.InfraInstrument\n",
    "\n",
    "\n",
    "class MarketData(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    ob: pl.LazyFrame\n",
    "    tr: pl.LazyFrame\n",
    "\n",
    "\n",
    "MD_STORAGE_PATH = \"/md_storage/\"\n",
    "\n",
    "\n",
    "class MdStorage(Storage[MarketData]):\n",
    "    executable: str = os.path.join(paths.boyle_path(), \"build/md_exporter\")\n",
    "    orderbook_levels: int = 40\n",
    "    skip_empty_levels: bool = True\n",
    "    run_config_path: str = \"./configs/run_config.json\"\n",
    "    sim_config_path: str = \"./configs/simulation_config.json\"\n",
    "    verbose: int = 1\n",
    "\n",
    "    @staticmethod\n",
    "    def default(jobs: int) -> MdStorage:\n",
    "        return MdStorage(\n",
    "            mode=StorageMode.ReadIgnoreErrors,\n",
    "            executor=ThreadPoolExecutor(max_workers=jobs),\n",
    "            base_path=MD_STORAGE_PATH,\n",
    "            name=\"main\",\n",
    "        )\n",
    "\n",
    "    def paths(self, req: StorageRequest) -> tuple[str, str, str, str, str]:\n",
    "        path = req.full_path()\n",
    "\n",
    "        orderbook = os.path.join(path, \"orderbook.parquet\")\n",
    "        trades = os.path.join(path, \"trades.parquet\")\n",
    "\n",
    "        stdout = os.path.join(path, \"stdout.txt\")\n",
    "        stderr = os.path.join(path, \"stderr.txt\")\n",
    "\n",
    "        return path, orderbook, trades, stdout, stderr\n",
    "\n",
    "    def read_req(self, req: StorageRequest) -> MarketData:\n",
    "        _, orderbook, trades, _, _ = self.paths(req)\n",
    "\n",
    "        ob = pl.scan_parquet(orderbook)\n",
    "        ob.collect_schema()\n",
    "        tr = pl.scan_parquet(trades)\n",
    "        tr.collect_schema()\n",
    "\n",
    "        return MarketData(ob=ob, tr=tr)\n",
    "\n",
    "    def write_req(self, req: StorageRequest) -> None:\n",
    "        path, orderbook, trades, stdout, stderr = self.paths(req)\n",
    "\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        model_config = MdExporterModelConfig(\n",
    "            orderbook_file=orderbook,\n",
    "            trades_file=trades,\n",
    "            orderbook_levels=self.orderbook_levels,\n",
    "            skip_empty_levels=self.skip_empty_levels,\n",
    "            instrument=req.instr.to_infra(),\n",
    "        )\n",
    "\n",
    "        backtest = Backtest(\n",
    "            executable=self.executable,\n",
    "            model=model_config,\n",
    "            run=self.run_config_path,\n",
    "            simulation=self.sim_config_path,\n",
    "            start=req.start,\n",
    "            end=req.end,\n",
    "            orders=\"/dev/null\",\n",
    "            stdout=stdout,\n",
    "            stderr=stderr,\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "        backtest.cmd().local_execute()\n",
    "\n",
    "# process.py\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from tap import Tap\n",
    "\n",
    "from mariotte.research import process, research\n",
    "from mariotte.research.storage import InstrumentInterval, StorageMode\n",
    "from mariotte.research.storage_features import FeaturesStorage\n",
    "from mariotte.research.storage_md import MdStorage\n",
    "from mariotte.research.storage_processed import ProcessedStorage\n",
    "\n",
    "\n",
    "class ProcessArgs(Tap):\n",
    "    intervals: str\n",
    "    src_name: str = \"main\"\n",
    "    dst_name: str = \"main\"\n",
    "    mode: StorageMode = StorageMode.Default\n",
    "    jobs: int = 1\n",
    "    verbose: int = 2\n",
    "    md_storage: str | None = None\n",
    "    ft_storage: str | None = None\n",
    "    df_storage: str | None = None\n",
    "\n",
    "\n",
    "def run(args: ProcessArgs):\n",
    "    intervals = InstrumentInterval.from_file(args.intervals)\n",
    "\n",
    "    executor = ThreadPoolExecutor(max_workers=args.jobs)\n",
    "\n",
    "    mds = MdStorage.default(jobs=args.jobs)\n",
    "    if args.md_storage is not None:\n",
    "        mds.base_path = args.md_storage\n",
    "    mds.mode = StorageMode.ReadIgnoreErrors\n",
    "    mds.executor = executor\n",
    "    mds.verbose = args.verbose\n",
    "    mds.name = args.src_name\n",
    "\n",
    "    fts = FeaturesStorage.default(jobs=args.jobs)\n",
    "    if args.ft_storage is not None:\n",
    "        fts.base_path = args.ft_storage\n",
    "    fts.mode = StorageMode.ReadIgnoreErrors\n",
    "    fts.executor = executor\n",
    "    fts.verbose = args.verbose\n",
    "    fts.name = args.src_name\n",
    "\n",
    "    md = mds.load_multi(intervals)\n",
    "    ft = fts.load_multi(intervals)\n",
    "\n",
    "    data = research.merge_feeds(md, ft, force=False)\n",
    "\n",
    "    print(f\"Found {len(data)} intervals, md: {len(md)}, ft: {len(ft)}\")\n",
    "    dfs = ProcessedStorage.default(process.process_single)\n",
    "    if args.df_storage is not None:\n",
    "        dfs.base_path = args.df_storage\n",
    "    dfs.mode = args.mode\n",
    "    dfs.executor = executor\n",
    "    dfs.verbose = args.verbose\n",
    "    dfs.name = args.dst_name\n",
    "    df = dfs.wait(dfs.write_async(data))\n",
    "\n",
    "    total = len(data)\n",
    "    loaded = len(df)\n",
    "    print(f\"Read {loaded} out of {total} ({(loaded / total * 100):.2f}%)\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = ProcessArgs(underscores_to_dashes=True).parse_args()\n",
    "    run(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# export_data.py\n",
    "from concurrent.futures import Future, ThreadPoolExecutor\n",
    "\n",
    "from tap import Tap\n",
    "\n",
    "from mariotte import paths\n",
    "from mariotte.build import build_default\n",
    "from mariotte.research.storage import InstrumentInterval, StorageMode, StorageRequest\n",
    "from mariotte.research.storage_features import FeaturesData, FeaturesStorage\n",
    "from mariotte.research.storage_md import MarketData, MdStorage\n",
    "\n",
    "\n",
    "class ExportDataArgs(Tap):\n",
    "    intervals: str\n",
    "    marketdata: bool = False\n",
    "    features: bool = False\n",
    "    no_build: bool = False\n",
    "    name: str = \"main\"\n",
    "    mode: StorageMode = StorageMode.Default\n",
    "    jobs: int = 10\n",
    "    verbose: int = 2\n",
    "    md_storage: str | None = None\n",
    "    ft_storage: str | None = None\n",
    "\n",
    "\n",
    "def run(args: ExportDataArgs):\n",
    "    intervals = InstrumentInterval.from_file(args.intervals)\n",
    "\n",
    "    if not args.no_build:\n",
    "        build_default(verbose=args.verbose)\n",
    "\n",
    "    executor = ThreadPoolExecutor(max_workers=args.jobs)\n",
    "\n",
    "    md = MdStorage.default(jobs=args.jobs)\n",
    "    if args.md_storage is not None:\n",
    "        md.base_path = args.md_storage\n",
    "    md.mode = args.mode\n",
    "    md.executor = executor\n",
    "    md.verbose = args.verbose\n",
    "    md.name = args.name\n",
    "\n",
    "    md.executable = paths.copy_binary(md.executable, None)\n",
    "\n",
    "    ft = FeaturesStorage.default(jobs=args.jobs)\n",
    "    if args.ft_storage is not None:\n",
    "        ft.base_path = args.ft_storage\n",
    "    ft.mode = args.mode\n",
    "    ft.executor = executor\n",
    "    ft.verbose = args.verbose\n",
    "    ft.name = args.name\n",
    "\n",
    "    ft.executable = paths.copy_binary(ft.executable, None)\n",
    "\n",
    "    wait_md: list[tuple[StorageRequest, Future[MarketData | None]]] = []\n",
    "    wait_ft: list[tuple[StorageRequest, Future[FeaturesData | None]]] = []\n",
    "    for interval in intervals:\n",
    "        if args.marketdata:\n",
    "            wait_md = wait_md + md.load_async([interval])\n",
    "        if args.features:\n",
    "            wait_ft = wait_ft + ft.load_async([interval])\n",
    "\n",
    "    md_res = md.wait(wait_md)\n",
    "    ft_res = ft.wait(wait_ft)\n",
    "\n",
    "    total = max(1, len(wait_md) + len(wait_ft))\n",
    "    loaded = len(md_res) + len(ft_res)\n",
    "    print(f\"Read {loaded} out of {total} ({(loaded / total * 100):.2f}%)\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = ExportDataArgs(underscores_to_dashes=True).parse_args()\n",
    "    run(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# research_base.py\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import typing\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob\n",
    "from typing import Any\n",
    "\n",
    "import catboost\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from polars import col\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from mariotte import date, exchange_info, instrument, logs, plot, util\n",
    "from mariotte.research import research\n",
    "from mariotte.research.storage import StorageMode\n",
    "from mariotte.research.storage_features import FeaturesStorage\n",
    "from mariotte.research.storage_md import MdStorage\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Configure Polars\n",
    "pl.Config().set_tbl_rows(50).set_fmt_str_lengths(100)\n",
    "\n",
    "md = MdStorage.default(jobs=1)\n",
    "data = md.load(\n",
    "    instrument=INSTRUMENT,\n",
    "    start=START,\n",
    "    end=END,\n",
    ")\n",
    "\n",
    "LVLS = md.orderbook_levels\n",
    "\n",
    "# fit_research.py\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import typing\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob\n",
    "from typing import Any\n",
    "\n",
    "import catboost\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from polars import col\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from mariotte import date, exchange_info, instrument, logs, plot, util\n",
    "from mariotte.exchange_info import ExchangeInfo\n",
    "from mariotte.research import research\n",
    "from mariotte.research.storage import InstrumentInterval, StorageMode, StorageRequest\n",
    "from mariotte.research.storage_features import FeaturesData, FeaturesStorage\n",
    "from mariotte.research.storage_md import MarketData, MdStorage\n",
    "from mariotte.train import Pool, Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Configure Polars\n",
    "pl.Config().set_tbl_rows(50).set_fmt_str_lengths(100)\n",
    "\n",
    "mds = MdStorage.default(jobs=1)\n",
    "fts = FeaturesStorage.default(jobs=1)\n",
    "md = mds.load_multi(intervals)\n",
    "ft = fts.load_multi(intervals)\n",
    "data = research.merge_feeds(md, ft, force=False)\n",
    "print(len(data))\n",
    "\n",
    "I need to write a new module, that takes place instead of Storage, and covers same tasks\n",
    "For now just describe, what this module does now, try to simplify things, but not too much\n",
    "\"\"\"\n",
    "\n",
    "len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e00b1697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "The existing Storage module manages data storage/retrieval for Market Data, Features, and Processed Data. Key tasks:\n",
       "\n",
       "1. **Core Functionality**:\n",
       "- Implements a generic Storage base class with mode-driven read/write logic\n",
       "- Provides specialized subclasses (MdStorage, FeaturesStorage) for different data types\n",
       "- Handles parallel operations via executors\n",
       "- Integrates with backtest/exporter binaries to generate data\n",
       "- Manages file paths and error handling through modes (Read/Write/Ignore)\n",
       "\n",
       "2. **Key Components**:\n",
       "- **Storage Modes**: Control behavior for read/write operations (e.g., fail on error or ignore)\n",
       "- **Request Handling**: Uses StorageRequest objects to track data requests\n",
       "- **Data Formats**: Works with Parquet files and polars LazyFrames\n",
       "- **External Tools**: Executes binaries (md_exporter, features_exporter) to generate data\n",
       "\n",
       "Proposed Simplified Module Structure:\n",
       "\n",
       "```python\n",
       "from typing import Any, Dict\n",
       "import os\n",
       "import polars as pl\n",
       "from concurrent.futures import ThreadPoolExecutor\n",
       "\n",
       "class BaseStorage:\n",
       "    def __init__(\n",
       "        self,\n",
       "        base_path: str,\n",
       "        mode: str = \"default\",\n",
       "        executor: ThreadPoolExecutor = None,\n",
       "        name: str = \"main\",\n",
       "        verbose: int = 1\n",
       "    ):\n",
       "        self.base_path = base_path\n",
       "        self.mode = mode  # \"read\", \"write\", \"default\", \"ignore\"\n",
       "        self.executor = executor or ThreadPoolExecutor()\n",
       "        self.name = name\n",
       "        self.verbose = verbose\n",
       "\n",
       "    def _get_path(self, req: Dict[str, Any]) -> str:\n",
       "        \"\"\"Construct full path for given request parameters\"\"\"\n",
       "        return os.path.join(\n",
       "            self.base_path,\n",
       "            self.name,\n",
       "            req['market'],\n",
       "            req['pair'],\n",
       "            req['start_date_str']\n",
       "        )\n",
       "\n",
       "    def read(self, req: Dict[str, Any]) -> Any:\n",
       "        \"\"\"Generic read logic implementing mode behavior\"\"\"\n",
       "        path = self._get_path(req)\n",
       "        if not os.path.exists(path) and self.mode in (\"default\", \"read\"):\n",
       "            raise FileNotFoundError(f\"Path {path} doesn't exist\")\n",
       "        elif self.mode == \"ignore\":\n",
       "            return None if not os.path.exists(path) else self._read_data(path)\n",
       "        else:\n",
       "            return self._read_data(path)\n",
       "\n",
       "    def write(self, req: Dict[str, Any]) -> None:\n",
       "        \"\"\"Generic write logic implementing mode behavior\"\"\"\n",
       "        path = self._get_path(req)\n",
       "        if self.mode in (\"write\", \"default\"):\n",
       "            self._generate_data(path, req)\n",
       "            self._save_data(path)\n",
       "        elif self.mode == \"ignore\":\n",
       "            try:\n",
       "                self._generate_data(path, req)\n",
       "                self._save_data(path)\n",
       "            except Exception as e:\n",
       "                print(f\"Warning: Failed to write {path}: {str(e)}\")\n",
       "\n",
       "    def _generate_data(self, path: str, req: Dict[str, Any]) -> None:\n",
       "        \"\"\"Implementation-specific data generation (to be overridden)\"\"\"\n",
       "        raise NotImplementedError\n",
       "\n",
       "    def _read_data(self, path: str) -> Any:\n",
       "        \"\"\"Implementation-specific data reading (to be overridden)\"\"\"\n",
       "        raise NotImplementedError\n",
       "\n",
       "    def _save_data(self, path: str) -> None:\n",
       "        \"\"\"Implementation-specific data saving (to be overridden)\"\"\"\n",
       "        raise NotImplementedError\n",
       "\n",
       "class MdStorage(BaseStorage):\n",
       "    def __init__(\n",
       "        self,\n",
       "        orderbook_levels: int = 40,\n",
       "        **kwargs\n",
       "    ):\n",
       "        super().__init__(**kwargs)\n",
       "        self.orderbook_levels = orderbook_levels\n",
       "\n",
       "    def _generate_data(self, path, req):\n",
       "        # Execute md_exporter binary using req parameters\n",
       "        # Example: run_backtest_command(req, output_path=path)\n",
       "        pass\n",
       "\n",
       "    def _read_data(self, path):\n",
       "        ob_path = os.path.join(path, \"orderbook.parquet\")\n",
       "        tr_path = os.path.join(path, \"trades.parquet\")\n",
       "        return {\n",
       "            \"orderbook\": pl.scan_parquet(ob_path),\n",
       "            \"trades\": pl.scan_parquet(tr_path)\n",
       "        }\n",
       "\n",
       "    def _save_data(self, path):\n",
       "        # Actual saving logic if needed\n",
       "        pass\n",
       "\n",
       "class FeaturesStorage(BaseStorage):\n",
       "    def _generate_data(self, path, req):\n",
       "        # Execute features_exporter binary\n",
       "        pass\n",
       "\n",
       "    def _read_data(self, path):\n",
       "        return pl.scan_parquet(os.path.join(path, \"features.parquet\"))\n",
       "\n",
       "# Usage example:\n",
       "storage = MdStorage(base_path=\"/md_storage/\")\n",
       "data = storage.read({\n",
       "    \"market\": \"binance\",\n",
       "    \"pair\": \"BTC-USDT\",\n",
       "    \"start_date_str\": \"2023-01-01\"\n",
       "})\n",
       "```\n",
       "\n",
       "**Key Simplifications:**\n",
       "\n",
       "1. **Unified Base Class**: Centralizes common logic for:\n",
       "   - Path construction\n",
       "   - Mode-based error handling\n",
       "   - Executor management\n",
       "\n",
       "2. **Simplified Mode Handling**:\n",
       "   - 4 modes instead of 5 (removed ReadIgnoreErrors/WriteIgnoreErrors distinction)\n",
       "   - Clear behavior:\n",
       "     - \"read\": Fail if missing\n",
       "     - \"write\": Only create new data\n",
       "     - \"default\": Read or write if missing\n",
       "     - \"ignore\": Silently handle errors\n",
       "\n",
       "3. **Request Abstraction**:\n",
       "   - Uses simple dictionaries instead of Pydantic models for requests\n",
       "   - Removes the need for StorageRequest class\n",
       "\n",
       "4. **Streamlined Workflow**:\n",
       "   - Separates generation, reading, and saving into distinct methods\n",
       "   - Makes extension for new storage types easier\n",
       "\n",
       "5. **Reduced Overhead**:\n",
       "   - Eliminates template methods and complex mode enums\n",
       "   - Simplifies executor usage through default parameters\n",
       "\n",
       "6. **Clear API**:\n",
       "   - Public methods (read/write) handle mode logic internally\n",
       "   - Private methods handle storage-specific operations\n",
       "\n",
       "**Backward Compatibility Notes**:\n",
       "- Maintains core functionality while removing unused features\n",
       "- Can wrap Pydantic models in adapters if needed for existing code\n",
       "- The simplified mode system is compatible with previous behavior through mapping:\n",
       "  ```python\n",
       "  old_mode            new_mode\n",
       "  -------------------------------\n",
       "  Default             default\n",
       "  Write               write\n",
       "  WriteIgnoreErrors   ignore\n",
       "  Read                read\n",
       "  ReadIgnoreErrors    ignore\n",
       "  ```\n",
       "\n",
       "This approach maintains all core capabilities while reducing complexity by:\n",
       "1. Removing unnecessary abstractions\n",
       "2. Simplifying mode management\n",
       "3. Using standard Python constructs over heavy type systems\n",
       "4. Making the storage implementation more intuitive through inheritance hierarchy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat.add_user_message(prompt)\n",
    "m.act(chat, max_tokens_per_message = 50000, temperature=0.7, on_message=chat.append, max_prediction_rounds=100)\n",
    "chat.show_final_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49581588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Here's a detailed description of the existing Storage module's functionality based on the provided code:\n",
       "\n",
       "### 1. Core Abstraction: `Storage` Base Class\n",
       "```python\n",
       "class Storage[T](BaseModel):\n",
       "    ...\n",
       "```\n",
       "- **Purpose**: Generic data storage interface for different data types (market data, features, processed data)\n",
       "- **Key Features**:\n",
       "  - **Mode-driven operations**: Uses `StorageMode` enum to control behavior:\n",
       "    ```python\n",
       "    class StorageMode(str, Enum):\n",
       "        Default = \"Default\"\n",
       "        Write = \"Write\"\n",
       "        WriteIgnoreErrors = \"WriteIgnoreErrors\"\n",
       "        Read = \"Read\"\n",
       "        ReadIgnoreErrors = \"ReadIgnoreErrors\"\n",
       "    ```\n",
       "  - **Parallel processing**: Uses `Executor` (thread/process pool) for async operations\n",
       "  - **Request handling**:\n",
       "    - `load` methods split time ranges into hourly intervals (via `date.split()`)\n",
       "    - Uses `StorageRequest` objects to track data requests\n",
       "  - **Lifecycle methods**:\n",
       "    - `load_req()`: Actual data loading implementation\n",
       "    - `write_req()`: Actual data generation/write implementation\n",
       "    - Mode-specific variants (e.g., `read_req()`, `try_write_req()`)\n",
       "  \n",
       "### 2. Market Data Storage (`MdStorage`)\n",
       "```python\n",
       "class MdStorage(Storage[MarketData]):\n",
       "    ...\n",
       "```\n",
       "- **Purpose**: Store/produce market data (order books and trades)\n",
       "- **Data format**:\n",
       "  - Orderbook data: Parquet files (`orderbook.parquet`)\n",
       "  - Trades data: Parquet files (`trades.parquet`)\n",
       "- **Implementation details**:\n",
       "  - Depends on external `md_exporter` binary (C++ executable)\n",
       "  - Uses `Backtest` class to execute data generation:\n",
       "    ```python\n",
       "    backtest = Backtest(\n",
       "        executable=self.executable,\n",
       "        model=...,\n",
       "        start=req.start,\n",
       "        end=req.end,\n",
       "        ...\n",
       "    )\n",
       "    backtest.cmd().local_execute()\n",
       "    ```\n",
       "  - Path structure:\n",
       "    ```text\n",
       "    <base_path>/<storage_name>/<market>/<pair>/<date_str>/\n",
       "      ├── orderbook.parquet\n",
       "      ├── trades.parquet\n",
       "      ├── stdout.txt\n",
       "      └── stderr.txt\n",
       "    ```\n",
       "- **Reading data**:\n",
       "  - Uses `pl.scan_parquet()` for lazy loading of Parquet files\n",
       "  - Returns `MarketData` object with Polars LazyFrames\n",
       "\n",
       "### 3. Features Storage (`FeaturesStorage`)\n",
       "```python\n",
       "class FeaturesStorage(Storage[FeaturesData]):\n",
       "    ...\n",
       "```\n",
       "- **Purpose**: Store/produce derived features data\n",
       "- **Data format**:\n",
       "  - Features data: Parquet files (`features.parquet`)\n",
       "- **Implementation details**:\n",
       "  - Depends on external `allin_features_exporter` binary\n",
       "  - Uses similar `Backtest`-based execution as `MdStorage`\n",
       "  - Path structure:\n",
       "    ```text\n",
       "    <base_path>/<storage_name>/<market>/<pair>/<date_str>/\n",
       "      ├── features.parquet\n",
       "      ├── stdout.txt\n",
       "      └── stderr.txt\n",
       "    ```\n",
       "- **Reading data**:\n",
       "  - Returns `FeaturesData` with Polars LazyFrame\n",
       "\n",
       "### 4. Processed Data Storage (`ProcessedStorage`)\n",
       "```python\n",
       "class ProcessedStorage(Storage[ProcessedData]):\n",
       "    ...\n",
       "```\n",
       "- **Purpose**: Store final processed/merged datasets\n",
       "- **Data flow**:\n",
       "  1. Merges market data and features data using `research.merge_feeds()`\n",
       "  2. Saves the combined dataset\n",
       "- **Implementation details**:\n",
       "  - Uses `process.process_single()` for data transformation\n",
       "  - Relies on `Executor` for parallel processing\n",
       "\n",
       "### 5. Key Supporting Components\n",
       "#### a. `StorageRequest` Object\n",
       "```python\n",
       "class StorageRequest(BaseModel):\n",
       "    base_path: str\n",
       "    instr: instrument.Instrument\n",
       "    start: datetime\n",
       "    end: datetime\n",
       "    name: str\n",
       "    addt: Any  # additional data\n",
       "```\n",
       "- Encapsulates all parameters needed to locate/produce data:\n",
       "  - Path components (market, pair, date)\n",
       "  - Time range (start/end)\n",
       "  - Storage configuration (name, base path)\n",
       "\n",
       "#### b. Path Construction\n",
       "```python\n",
       "def full_path(self) -> str:\n",
       "    return os.path.join(\n",
       "        self.base_path,\n",
       "        self.name,\n",
       "        self.instr.market,\n",
       "        self.instr.pair,\n",
       "        date.to_str(self.start)\n",
       "    )\n",
       "```\n",
       "- Generates standardized paths for data storage locations\n",
       "- Uses date formatting from `date.to_str()`\n",
       "\n",
       "#### c. Asynchronous Operations\n",
       "- **`load_async()`**: Returns list of `(StorageRequest, Future)` tuples\n",
       "- **`wait()`**: Waits for async operations to complete and aggregates results\n",
       "- Uses Python `concurrent.futures` for parallelism\n",
       "\n",
       "### 6. External Dependencies\n",
       "- **Binaries**:\n",
       "  - `md_exporter` (market data generation)\n",
       "  - `allin_features_exporter` (features generation)\n",
       "- **Execution Environment**:\n",
       "  - Requires compiled binaries in `./build/` directory\n",
       "  - Depends on `Backtest` class for command execution\n",
       "- **Data Infrastructure**:\n",
       "  - Relies on Parquet file format\n",
       "  - Uses Polars library for data handling\n",
       "\n",
       "### 7. Usage Patterns\n",
       "#### a. Data Export (via `export_data.py`)\n",
       "```python\n",
       "def run(args):\n",
       "    intervals = InstrumentInterval.from_file(...)\n",
       "    md = MdStorage(...)\n",
       "    ft = FeaturesStorage(...)\n",
       "    md_res = md.wait(md.load_async(...))\n",
       "    ft_res = ft.wait(ft.load_async(...))\n",
       "```\n",
       "- Parallel execution of data generation across intervals\n",
       "- Handles both market data and features in separate workflows\n",
       "\n",
       "#### b. Research/Processing (via `fit_research.py`)\n",
       "```python\n",
       "md = mds.load(...)\n",
       "ft = fts.load(...)\n",
       "data = research.merge_feeds(md, ft, force=False)\n",
       "dfs = ProcessedStorage(...)\n",
       "dfs.write_async(data)\n",
       "```\n",
       "- Combines raw data from multiple sources\n",
       "- Persists final processed datasets\n",
       "\n",
       "#### c. Storage Mode Usage\n",
       "```python\n",
       "# Read-only mode (fail on missing data)\n",
       "storage = MdStorage(mode=StorageMode.Read)\n",
       "\n",
       "# Write mode (only generate new data)\n",
       "storage = FeaturesStorage(mode=StorageMode.Write)\n",
       "\n",
       "# Default mode (read or generate if missing)\n",
       "storage = ProcessedStorage(mode=StorageMode.Default)\n",
       "```\n",
       "\n",
       "### 8. Key Design Choices\n",
       "- **Layered architecture**: Clear separation between:\n",
       "  1. Core storage logic (Storage base class)\n",
       "  2. Data type implementations (MdStorage/FeaturesStorage)\n",
       "  3. Workflow coordination (process/export scripts)\n",
       "- **External tool integration**: Uses compiled binaries for performance-critical data generation\n",
       "- **Lazy evaluation**: Uses Polars LazyFrames for deferred execution\n",
       "- **Error handling**: Mode system allows flexible failure handling (e.g., ignore missing data in research workflows)\n",
       "\n",
       "This module forms the core data infrastructure for handling:\n",
       "1. Raw market data ingestion\n",
       "2. Feature engineering\n",
       "3. Final dataset preparation\n",
       "4. Parallel processing of large time ranges\n",
       "5. Production-quality data generation via compiled tools"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "No, don't propose new changes yet. Also, backwards compatibility is unnecessary. Describe only existing functionality.\n",
    "\"\"\"\n",
    "chat.add_user_message(prompt)\n",
    "m.act(chat, max_tokens_per_message = 50000, temperature=0.7, on_message=chat.append, max_prediction_rounds=100)\n",
    "chat.show_final_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e01a7390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Here's a concept design for a generic, mode-driven storage system with parallel execution control:\n",
       "\n",
       "```python\n",
       "from enum import Enum\n",
       "from typing import Any, Dict, Optional, Callable, Union\n",
       "from dataclasses import dataclass\n",
       "from concurrent.futures import Executor, Future\n",
       "\n",
       "class LoadMode(Enum):\n",
       "    GENERATE = \"generate\"\n",
       "    ERROR = \"error\"\n",
       "    SKIP = \"skip\"\n",
       "\n",
       "class GenerateMode(Enum):\n",
       "    RAISE = \"raise\"\n",
       "    IGNORE = \"ignore\"\n",
       "\n",
       "@dataclass\n",
       "class DataKey:\n",
       "    identifier: str  # e.g., \"BTC-USD-2023-01-01\"\n",
       "\n",
       "class Storage:\n",
       "    def __init__(\n",
       "        self,\n",
       "        base_path: str,\n",
       "        load_mode: LoadMode = LoadMode.GENERATE,\n",
       "        generate_mode: GenerateMode = GenerateMode.RAISE,\n",
       "        executor: Optional[Executor] = None,\n",
       "        max_workers: int = 5,\n",
       "        cluster: Optional[Any] = None,  # Assume Ray Cluster object\n",
       "    ):\n",
       "        self.base_path = base_path\n",
       "        self.load_mode = load_mode\n",
       "        self.generate_mode = generate_mode\n",
       "        self.max_workers = max_workers\n",
       "        self.executor = executor or self._setup_executor(cluster)\n",
       "        self.key_to_future: Dict[DataKey, Future] = {}  # Track in-progress tasks\n",
       "\n",
       "    def _setup_executor(self, cluster):\n",
       "        if cluster:\n",
       "            return RayExecutor(cluster, self.max_workers)\n",
       "        else:\n",
       "            from concurrent.futures import ThreadPoolExecutor\n",
       "            return ThreadPoolExecutor(max_workers=self.max_workers)\n",
       "\n",
       "    def get_path(self, key: DataKey) -> str:\n",
       "        return os.path.join(self.base_path, key.identifier)\n",
       "\n",
       "    def exists(self, key: DataKey) -> bool:\n",
       "        return os.path.exists(self.get_path(key))\n",
       "\n",
       "    def load_sync(self, key: DataKey) -> Optional[Any]:\n",
       "        \"\"\"Synchronous blocking load\"\"\"\n",
       "        if self.exists(key):\n",
       "            return self._read_data(key)\n",
       "        else:\n",
       "            return self._handle_missing(key)\n",
       "\n",
       "    def load(self, key: DataKey) -> Future:\n",
       "        \"\"\"Asynchronous load with mode logic\"\"\"\n",
       "        if key in self.key_to_future:\n",
       "            return self.key_to_future[key]\n",
       "        \n",
       "        future = self.executor.submit(self._load_task, key)\n",
       "        self.key_to_future[key] = future\n",
       "        future.add_done_callback(lambda f: self.key_to_future.pop(key, None))\n",
       "        return future\n",
       "\n",
       "    def _load_task(self, key: DataKey) -> Optional[Any]:\n",
       "        try:\n",
       "            return self.load_sync(key)\n",
       "        except Exception as e:\n",
       "            if self.generate_mode == GenerateMode.RAISE:\n",
       "                raise\n",
       "            else:\n",
       "                return None\n",
       "\n",
       "    def _handle_missing(self, key: DataKey) -> Optional[Any]:\n",
       "        if self.load_mode == LoadMode.GENERATE:\n",
       "            return self.generate(key)\n",
       "        elif self.load_mode == LoadMode.ERROR:\n",
       "            raise KeyError(f\"Data not found for {key}\")\n",
       "        else:  # SKIP\n",
       "            return None\n",
       "\n",
       "    def generate(self, key: DataKey) -> Optional[Any]:\n",
       "        \"\"\"Attempt to generate data for missing key\"\"\"\n",
       "        try:\n",
       "            data = self._generate_data(key)\n",
       "            self._save_data(key, data)\n",
       "            return data\n",
       "        except Exception as e:\n",
       "            if self.generate_mode == GenerateMode.RAISE:\n",
       "                raise\n",
       "            else:\n",
       "                logger.warning(f\"Generation failed for {key}: {e}\")\n",
       "                return None\n",
       "\n",
       "    def _generate_data(self, key: DataKey) -> Any:\n",
       "        \"\"\"Implement actual generation logic here\"\"\"\n",
       "        raise NotImplementedError\n",
       "\n",
       "    def _read_data(self, key: DataKey) -> Any:\n",
       "        \"\"\"Implement actual read logic here\"\"\"\n",
       "        raise NotImplementedError\n",
       "\n",
       "    def _save_data(self, key: DataKey, data: Any) -> None:\n",
       "        \"\"\"Implement actual save logic here\"\"\"\n",
       "        raise NotImplementedError\n",
       "\n",
       "# Example usage with configuration options:\n",
       "class MyStorage(Storage):\n",
       "    def _generate_data(self, key):\n",
       "        # Example implementation\n",
       "        return generate_data_func(key)\n",
       "\n",
       "    def _read_data(self, key):\n",
       "        return read_from_disk(self.get_path(key))\n",
       "\n",
       "    def _save_data(self, key, data):\n",
       "        save_to_disk(data, self.get_path(key))\n",
       "\n",
       "# Configuration options\n",
       "my_storage = MyStorage(\n",
       "    base_path=\"/data/storage\",\n",
       "    load_mode=LoadMode.GENERATE,\n",
       "    generate_mode=GenerateMode.IGNORE,\n",
       "    max_workers=10,\n",
       "    cluster=RayCluster(address=\"auto\")  # Optional Ray cluster\n",
       ")\n",
       "```\n",
       "\n",
       "### Key Components & Features:\n",
       "1. **Mode System**:\n",
       "   - `LoadMode`: Controls missing data handling\n",
       "     - GENERATE: Attempt to generate missing data\n",
       "     - ERROR: Raise exception if missing\n",
       "     - SKIP: Return None for missing data\n",
       "   - `GenerateMode`: Controls generation error handling\n",
       "     - RAISE: Propagate exceptions from generation\n",
       "     - IGNORE: Suppress exceptions (return None)\n",
       "\n",
       "2. **Parallel Execution**:\n",
       "   - Works with standard executors (ThreadPool, ProcessPool)\n",
       "   - Ray integration via `cluster` parameter\n",
       "   - Automatic task tracking with `key_to_future` cache\n",
       "   - Limits concurrent tasks via `max_workers`\n",
       "\n",
       "3. **Abstraction Layers**:\n",
       "   - `_generate_data()`: Core data creation logic (to be implemented)\n",
       "   - `_read_data()/_save_data()`: I/O operations (format-specific)\n",
       "   - Path management via `get_path()`\n",
       "\n",
       "4. **Asynchronous API**:\n",
       "   - `load()` returns `Future` for async operations\n",
       "   - `load_sync()` provides blocking access\n",
       "   - Automatic retry management for in-progress tasks\n",
       "\n",
       "5. **Error Handling**:\n",
       "   - Separation between load/generation error handling\n",
       "   - Graceful error suppression with logging\n",
       "   - Clear exception propagation options\n",
       "\n",
       "### Usage Patterns:\n",
       "```python\n",
       "# Asynchronous loading with error suppression\n",
       "future = my_storage.load(DataKey(\"BTC-USD-2023-01\"))\n",
       "data = future.result()  # Block until completion\n",
       "\n",
       "# Bulk loading with parallel execution\n",
       "keys = [DataKey(f\"BTC-USD-{date}\") for date in date_list]\n",
       "futures = [my_storage.load(k) for k in keys]\n",
       "results = [f.result() for f in futures]\n",
       "\n",
       "# Explicit generation (ignores load_mode)\n",
       "data = my_storage.generate(DataKey(\"ETH-USD-2023-02\"))\n",
       "```\n",
       "\n",
       "### Extensibility:\n",
       "- Subclasses implement only data-specific methods (`_generate/read/save`)\n",
       "- Works with any data formats (Parquet, HDF5, etc.)\n",
       "- Can be adapted to cloud storage (S3/GCS) with path adjustments\n",
       "- Executor pluggability allows different parallelism strategies\n",
       "\n",
       "This design maintains the core functionality of the original system while:\n",
       "1. Simplifying the mode system into orthogonal dimensions\n",
       "2. Providing clear separation between core logic and I/O\n",
       "3. Supporting modern async programming patterns\n",
       "4. Allowing integration with distributed computing frameworks\n",
       "5. Improving maintainability through better abstraction boundaries"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Ok, now keep in mind the existing structure, and let's create a concept of a new one. Firstly - ignore all different formats (md, features, etc.), the concept only covers generic case.\n",
    "As the Storage thing generally is used for asynchronously loading data, this is the default behaviour. Skipping errors (if something is not found) is a separate argument.\n",
    "So, modes (can be named differently):\n",
    "Load data. Variants: generate if empty | raise error if empty | skip if empty\n",
    "Generate and save data: Variants: raise errors or skip if generator is working unexpected\n",
    "\n",
    "Both should utilize parallel execution, with two variants: local, or on passed ray cluster (don't dive into details, assume it's Cluster() object)\n",
    "Also, resources allocation control should be present, perhaps, number of concurrent workers.\n",
    "\n",
    "With this in mind, give me a concept code sample. Just structure of methods, arguments (you still can write pydantic or tap classes, if needed) with brief descriptions - no real logic.\n",
    "\"\"\"\n",
    "chat.add_user_message(prompt)\n",
    "m.act(chat, max_tokens_per_message = 50000, temperature=0.7, on_message=chat.append, max_prediction_rounds=100)\n",
    "chat.show_final_answer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9e3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887040b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
