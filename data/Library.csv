"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"WG9SRCIY","preprint","2020","Awasthi, Abhijeet; Sarawagi, Sunita; Goyal, Rasna; Ghosh, Sabyasachi; Piratla, Vihari","Parallel Iterative Edit Models for Local Sequence Transduction","","","","10.48550/arXiv.1910.02893","http://arxiv.org/abs/1910.02893","We present a Parallel Iterative Edit (PIE) model for the problem of local sequence transduction arising in tasks like Grammatical error correction (GEC). Recent approaches are based on the popular encoder-decoder (ED) model for sequence to sequence learning. The ED model auto-regressively captures full dependency among output tokens but is slow due to sequential decoding. The PIE model does parallel decoding, giving up the advantage of modelling full dependency in the output, yet it achieves accuracy competitive with the ED model for four reasons: 1.~predicting edits instead of tokens, 2.~labeling sequences instead of generating sequences, 3.~iteratively refining predictions to capture dependencies, and 4.~factorizing logits over edits and their token argument to harness pre-trained language models like BERT. Experiments on tasks spanning GEC, OCR correction and spell correction demonstrate that the PIE model is an accurate and significantly faster alternative for local sequence transduction.","2020-05-15","2022-11-17 22:59:22","2022-11-17 22:59:22","2022-11-17 22:59:22","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1910.02893 [cs]","","/home/lexi/.zotero-data/storage/2EBX5FAL/Awasthi et al. - 2020 - Parallel Iterative Edit Models for Local Sequence .pdf; /home/lexi/.zotero-data/storage/QL7VXVH4/1910.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1910.02893","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8GN8ARWX","preprint","2020","Wang, Yu; Wang, Yuelin; Liu, Jie; Liu, Zhuo","A Comprehensive Survey of Grammar Error Correction","","","","10.48550/arXiv.2005.06600","http://arxiv.org/abs/2005.06600","Grammar error correction (GEC) is an important application aspect of natural language processing techniques. The past decade has witnessed significant progress achieved in GEC for the sake of increasing popularity of machine learning and deep learning, especially in late 2010s when near human-level GEC systems are available. However, there is no prior work focusing on the whole recapitulation of the progress. We present the first survey in GEC for a comprehensive retrospect of the literature in this area. We first give the introduction of five public datasets, data annotation schema, two important shared tasks and four standard evaluation metrics. More importantly, we discuss four kinds of basic approaches, including statistical machine translation based approach, neural machine translation based approach, classification based approach and language model based approach, six commonly applied performance boosting techniques for GEC systems and two data augmentation methods. Since GEC is typically viewed as a sister task of machine translation, many GEC systems are based on neural machine translation (NMT) approaches, where the neural sequence-to-sequence model is applied. Similarly, some performance boosting techniques are adapted from machine translation and are successfully combined with GEC systems for enhancement on the final performance. Furthermore, we conduct an analysis in level of basic approaches, performance boosting techniques and integrated GEC systems based on their experiment results respectively for more clear patterns and conclusions. Finally, we discuss five prospective directions for future GEC researches.","2020-05-02","2022-11-18 09:22:44","2022-11-18 09:22:44","2022-11-18 09:22:44","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2005.06600 [cs]","","/home/lexi/.zotero-data/storage/4ERCNK9D/Wang et al. - 2020 - A Comprehensive Survey of Grammar Error Correction.pdf; /home/lexi/.zotero-data/storage/JH8GTFXP/2005.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2005.06600","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QLLTV9WN","conferencePaper","2020","Omelianchuk, Kostiantyn; Atrasevych, Vitaliy; Chernodub, Artem; Skurzhanskyi, Oleksandr","GECToR – Grammatical Error Correction: Tag, Not Rewrite","Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications","","","10.18653/v1/2020.bea-1.16","https://www.aclweb.org/anthology/2020.bea-1.16","In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder. Our system is pre-trained on synthetic data and then fine-tuned in two stages: first on errorful corpora, and second on a combination of errorful and error-free parallel corpora. We design custom token-level transformations to map input tokens to target corrections. Our best single-model/ensemble GEC tagger achieves an $F_{0.5}$ of 65.3/66.5 on CoNLL-2014 (test) and $F_{0.5}$ of 72.4/73.6 on BEA-2019 (test). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system. The code and trained models are publicly available.","2020","2022-11-18 11:05:55","2022-11-18 13:41:11","2022-11-18 11:05:55","163-170","","","","","","GECToR – Grammatical Error Correction","","","","","Association for Computational Linguistics","Seattle, WA, USA → Online","en","","","","","DOI.org (Crossref)","","arXiv:2005.12592 [cs]","","/home/lexi/.zotero-data/storage/H3VTCWNF/Omelianchuk et al. - 2020 - GECToR -- Grammatical Error Correction Tag, Not R.pdf; /home/lexi/.zotero-data/storage/NS5BM6HJ/2005.html; /home/lexi/.zotero-data/storage/YMM9Y4LA/Omelianchuk et al. - 2020 - GECToR – Grammatical Error Correction Tag, Not Re.pdf","","read","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications","","","","","","","","","","","","","","",""
"QUX3XQ7K","preprint","2022","Papadimitriou, Isabel; Lopez, Kezia; Jurafsky, Dan","Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models","","","","10.48550/arXiv.2210.05619","http://arxiv.org/abs/2210.05619","While multilingual language models can improve NLP performance on low-resource languages by leveraging higher-resource languages, they also reduce average performance on all languages (the 'curse of multilinguality'). Here we show another problem with multilingual models: grammatical structures in higher-resource languages bleed into lower-resource languages, a phenomenon we call grammatical structure bias. We show this bias via a novel method for comparing the fluency of multilingual models to the fluency of monolingual Spanish and Greek models: testing their preference for two carefully-chosen variable grammatical structures (optional pronoun-drop in Spanish and optional Subject-Verb ordering in Greek). We find that multilingual BERT is biased toward the English-like setting (explicit pronouns and Subject-Verb-Object ordering) as compared to our monolingual control. With our case studies, we hope to bring to light the fine-grained ways in which dominant languages can affect and bias multilingual performance, and encourage more linguistically-aware fluency evaluation.","2022-10-11","2022-11-18 18:58:27","2022-11-18 20:43:08","2022-11-18 18:58:27","","","","","","","Multilingual BERT has an accent","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2210.05619 [cs]","","/home/lexi/.zotero-data/storage/PBM7E2M8/Papadimitriou et al. - 2022 - Multilingual BERT has an accent Evaluating Englis.pdf; /home/lexi/.zotero-data/storage/Y36QFXV6/2210.html","","read","","","","","","","","","","","","","","","","","","","","arXiv:2210.05619","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AIXIEYF4","preprint","2020","Hu, Junjie; Ruder, Sebastian; Siddhant, Aditya; Neubig, Graham; Firat, Orhan; Johnson, Melvin","XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization","","","","10.48550/arXiv.2003.11080","http://arxiv.org/abs/2003.11080","Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.","2020-09-04","2022-11-19 10:40:40","2022-11-19 10:40:40","2022-11-19 10:40:40","","","","","","","XTREME","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2003.11080 [cs]","","/home/lexi/.zotero-data/storage/CWS5HIIA/Hu et al. - 2020 - XTREME A Massively Multilingual Multi-task Benchm.pdf; /home/lexi/.zotero-data/storage/T9MNKPSA/2003.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2003.11080","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CITSATED","webpage","2022","Ruder, Sebastian","The State of Multilingual AI","Sebastian Ruder","","","","https://ruder.io/state-of-multilingual-ai/","This post takes a closer look at the state of multilingual AI. How multilingual are current models in NLP, computer vision, and speech? What are the main recent contributions in this area? What challenges remain and how we can we address them?","2022-11-14","2022-11-19 13:27:47","2022-11-29 06:49:56","2022-11-19 13:27:47","","","","","","","","","","","","","","en","","Blogpost","","","","","Citation Key: ruder-state-of-multilingual-ai","","/home/lexi/.zotero-data/storage/SZ94KM2T/state-of-multilingual-ai.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DBPLVNZL","preprint","2017","Ruder, Sebastian","An overview of gradient descent optimization algorithms","","","","10.48550/arXiv.1609.04747","http://arxiv.org/abs/1609.04747","Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.","2017-06-15","2022-11-19 15:30:42","2022-11-19 15:30:43","2022-11-19 15:30:42","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1609.04747 [cs]","","/home/lexi/.zotero-data/storage/Y2J2QECJ/Ruder - 2017 - An overview of gradient descent optimization algor.pdf; /home/lexi/.zotero-data/storage/HYXU45JF/1609.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1609.04747","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P54E3RGF","webpage","2020","","Why You Should Do NLP Beyond English","Sebastian Ruder","","","","https://ruder.io/nlp-beyond-english/","7000+ languages are spoken around the world but NLP research has mostly focused on English. This post outlines why you should work on languages other than English.","2020-08-01","2022-11-19 15:32:50","2022-11-19 15:32:54","2022-11-19 15:32:50","","","","","","","","","","","","","","en","","","","","","","","","/home/lexi/.zotero-data/storage/ZHW38TDW/nlp-beyond-english.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SISH7KCR","conferencePaper","2022","Ruder, Sebastian; Vulić, Ivan; Søgaard, Anders","Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the Research Manifold","Findings of the Association for Computational Linguistics: ACL 2022","","","10.18653/v1/2022.findings-acl.184","https://aclanthology.org/2022.findings-acl.184","The prototypical NLP experiment trains a standard architecture on labeled English data and optimizes for accuracy, without accounting for other dimensions such as fairness, interpretability, or computational efficiency. We show through a manual classification of recent NLP research papers that this is indeed the case and refer to it as the square one experimental setup. We observe that NLP research often goes beyond the square one setup, e.g, focusing not only on accuracy, but also on fairness or interpretability, but typically only along a single dimension. Most work targeting multilinguality, for example, considers only accuracy; most work on fairness or interpretability considers only English; and so on. Such one-dimensionality of most research means we are only exploring a fraction of the NLP research search space. We provide historical and recent examples of how the square one bias has led researchers to draw false conclusions or make unwise choices, point to promising yet unexplored directions on the research manifold, and make practical recommendations to enable more multi-dimensional research. We open-source the results of our annotations to enable further analysis.","2022-05","2022-11-19 15:35:06","2022-11-19 15:35:12","2022-11-19 15:35:06","2340–2354","","","","","","Square One Bias in NLP","","","","","Association for Computational Linguistics","Dublin, Ireland","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/RDKAJQL5/Ruder et al. - 2022 - Square One Bias in NLP Towards a Multi-Dimensiona.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL-Findings 2022","","","","","","","","","","","","","","",""
"MNKJC85P","conferencePaper","2019","Ruder, Sebastian; Peters, Matthew E.; Swayamdipta, Swabha; Wolf, Thomas","Transfer Learning in Natural Language Processing","Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials","","","10.18653/v1/N19-5004","https://aclanthology.org/N19-5004","The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.","2019-06","2022-11-19 15:47:34","2022-11-19 15:47:34","2022-11-19 15:47:34","15–18","","","","","","","","","","","Association for Computational Linguistics","Minneapolis, Minnesota","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/UP44R8L2/Ruder et al. - 2019 - Transfer Learning in Natural Language Processing.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3Q4MHDJ6","preprint","2020","Conneau, Alexis; Khandelwal, Kartikay; Goyal, Naman; Chaudhary, Vishrav; Wenzek, Guillaume; Guzmán, Francisco; Grave, Edouard; Ott, Myle; Zettlemoyer, Luke; Stoyanov, Veselin","Unsupervised Cross-lingual Representation Learning at Scale","","","","10.48550/arXiv.1911.02116","http://arxiv.org/abs/1911.02116","This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.","2020-04-07","2022-11-19 15:49:24","2022-11-19 15:49:24","2022-11-19 15:49:24","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1911.02116 [cs]","","/home/lexi/.zotero-data/storage/FVCJUJW7/Conneau et al. - 2020 - Unsupervised Cross-lingual Representation Learning.pdf; /home/lexi/.zotero-data/storage/9WUN5LJF/1911.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1911.02116","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JBTL7EY","conferencePaper","2021","Goyal, Naman; Du, Jingfei; Ott, Myle; Anantharaman, Giri; Conneau, Alexis","Larger-Scale Transformers for Multilingual Masked Language Modeling","Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)","","","10.18653/v1/2021.repl4nlp-1.4","https://aclanthology.org/2021.repl4nlp-1.4","Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed and outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests larger capacity models for language understanding may obtain strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.","2021-08","2022-11-19 15:53:58","2022-11-19 15:53:58","2022-11-19 15:53:58","29–33","","","","","","","","","","","Association for Computational Linguistics","Online","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/99G57BZW/Goyal et al. - 2021 - Larger-Scale Transformers for Multilingual Masked .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RXCB3QJ7","conferencePaper","2022","Ahuja, Kabir; Kumar, Shanu; Dandapat, Sandipan; Choudhury, Monojit","Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models","Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2022.acl-long.374","https://aclanthology.org/2022.acl-long.374","Massively Multilingual Transformer based Language Models have been observed to be surprisingly effective on zero-shot transfer across languages, though the performance varies from language to language depending on the pivot language(s) used for fine-tuning. In this work, we build upon some of the existing techniques for predicting the zero-shot performance on a task, by modeling it as a multi-task learning problem. We jointly train predictive models for different tasks which helps us build more accurate predictors for tasks where we have test data in very few languages to measure the actual performance of the model. Our approach also lends us the ability to perform a much more robust feature selection, and identify a common set of features that influence zero-shot performance across a variety of tasks.","2022-05","2022-11-19 16:03:54","2023-09-20 13:12:21","2022-11-19 16:03:54","5454–5467","","","","","","","","","","","Association for Computational Linguistics","Dublin, Ireland","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/DST3DTY5/Ahuja et al. - 2022 - Multi Task Learning For Zero Shot Performance Pred.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL 2022","","","","","","","","","","","","","","",""
"Q2W72QH7","conferencePaper","2022","Lignos, Constantine; Holley, Nolan; Palen-Michel, Chester; Sälevä, Jonne","Toward More Meaningful Resources for Lower-resourced Languages","Findings of the Association for Computational Linguistics: ACL 2022","","","10.18653/v1/2022.findings-acl.44","https://aclanthology.org/2022.findings-acl.44","In this position paper, we describe our perspective on how meaningful resources for lower-resourced languages should be developed in connection with the speakers of those languages. Before advancing that position, we first examine two massively multilingual resources used in language technology development, identifying shortcomings that limit their usefulness. We explore the contents of the names stored in Wikidata for a few lower-resourced languages and find that many of them are not in fact in the languages they claim to be, requiring non-trivial effort to correct. We discuss quality issues present in WikiAnn and evaluate whether it is a useful supplement to hand-annotated data. We then discuss the importance of creating annotations for lower-resourced languages in a thoughtful and ethical way that includes the language speakers as part of the development process. We conclude with recommended guidelines for resource development.","2022-05","2022-11-19 16:04:54","2022-11-19 16:04:54","2022-11-19 16:04:54","523–532","","","","","","","","","","","Association for Computational Linguistics","Dublin, Ireland","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/9L9STIJS/Lignos et al. - 2022 - Toward More Meaningful Resources for Lower-resourc.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL-Findings 2022","","","","","","","","","","","","","","",""
"PNGDDJC6","preprint","2022","Artetxe, Mikel; Aldabe, Itziar; Agerri, Rodrigo; Perez-de-Viñaspre, Olatz; Soroa, Aitor","Does Corpus Quality Really Matter for Low-Resource Languages?","","","","10.48550/arXiv.2203.08111","http://arxiv.org/abs/2203.08111","The vast majority of non-English corpora are derived from automatically filtered versions of CommonCrawl. While prior work has identified major issues on the quality of these datasets (Kreutzer et al., 2021), it is not clear how this impacts downstream performance. Taking representation learning in Basque as a case study, we explore tailored crawling (manually identifying and scraping websites with high-quality content) as an alternative to filtering CommonCrawl. Our new corpus, called EusCrawl, is similar in size to the Basque portion of popular multilingual corpora like CC100 and mC4, yet it has a much higher quality according to native annotators. For instance, 66% of documents are rated as high-quality for EusCrawl, in contrast with <33% for both mC4 and CC100. Nevertheless, we obtain similar results on downstream NLU tasks regardless of the corpus used for pre-training. Our work suggests that NLU performance in low-resource languages is not primarily constrained by the quality of the data, and other factors like corpus size and domain coverage can play a more important role.","2022-10-26","2022-11-19 16:05:35","2022-11-19 16:05:35","2022-11-19 16:05:35","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2203.08111 [cs]","","/home/lexi/.zotero-data/storage/QNJDWKGR/Artetxe et al. - 2022 - Does Corpus Quality Really Matter for Low-Resource.pdf; /home/lexi/.zotero-data/storage/ZEWZZFUN/2203.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2203.08111","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6VPL6ZQ7","conferencePaper","2022","Wang, Benyou; Shang, Lifeng; Lioma, Christina; Jiang, Xin; Yang, Hao; Liu, Qun; Simonsen, Jakob Grue","On Position Embeddings in BERT","","","","","https://openreview.net/forum?id=onxoVA9FxMw","Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space: translation invariance, monotonicity, and symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way. Moreover, we propose a new probing test (called `identical word probing') and mathematical indicators to quantitatively detect the general attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both classification and span prediction benefit from translation invariance and local monotonicity, while symmetry slightly decreases performance; (2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction. We contribute the first formal and quantitative analysis of desiderata for PEs, and a principled discussion about their correlation to the performance of typical downstream tasks.","2022-02-10","2022-11-22 03:04:07","2022-11-22 03:04:10","2022-11-22 03:04:07","","","","","","","","","","","","","","en","","","","","openreview.net","","","","/home/lexi/.zotero-data/storage/NWBBSY7Y/Wang et al. - 2022 - On Position Embeddings in BERT.pdf; /home/lexi/.zotero-data/storage/TPJV44LW/forum.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Learning Representations","","","","","","","","","","","","","","",""
"TJJK5HRU","preprint","2022","Tay, Yi; Dehghani, Mostafa; Bahri, Dara; Metzler, Donald","Efficient Transformers: A Survey","","","","10.48550/arXiv.2009.06732","http://arxiv.org/abs/2009.06732","Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of ""X-former"" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored ""X-former"" models, providing an organized and comprehensive overview of existing work and models across multiple domains.","2022-03-14","2022-11-23 06:51:04","2022-11-23 06:51:04","2022-11-23 06:51:04","","","","","","","Efficient Transformers","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2009.06732 [cs]","","/home/lexi/.zotero-data/storage/RXJTHV4N/Tay et al. - 2022 - Efficient Transformers A Survey.pdf; /home/lexi/.zotero-data/storage/9VWGIR85/2009.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2009.06732","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J4Q44NT9","preprint","2022","Liu, Haokun; Tam, Derek; Muqeeth, Mohammed; Mohta, Jay; Huang, Tenghao; Bansal, Mohit; Raffel, Colin","Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning","","","","10.48550/arXiv.2205.05638","http://arxiv.org/abs/2205.05638","Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new PEFT method called (IA)$^3$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark, attaining super-human performance for the first time and outperforming the state-of-the-art by 6% absolute. All of the code used in our experiments is publicly available.","2022-08-26","2022-11-23 06:51:46","2022-11-23 06:51:46","2022-11-23 06:51:46","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2205.05638 [cs]","","/home/lexi/.zotero-data/storage/FN4JLNQX/Liu et al. - 2022 - Few-Shot Parameter-Efficient Fine-Tuning is Better.pdf; /home/lexi/.zotero-data/storage/4C77NU8R/2205.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2205.05638","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T8WJSAT3","preprint","2019","Houlsby, Neil; Giurgiu, Andrei; Jastrzebski, Stanislaw; Morrone, Bruna; de Laroussilhe, Quentin; Gesmundo, Andrea; Attariyan, Mona; Gelly, Sylvain","Parameter-Efficient Transfer Learning for NLP","","","","10.48550/arXiv.1902.00751","http://arxiv.org/abs/1902.00751","Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.","2019-06-13","2022-11-23 06:52:48","2022-11-23 06:52:48","2022-11-23 06:52:48","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1902.00751 [cs, stat]","","/home/lexi/.zotero-data/storage/BX2T2G59/Houlsby et al. - 2019 - Parameter-Efficient Transfer Learning for NLP.pdf; /home/lexi/.zotero-data/storage/T4GSRN8E/1902.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1902.00751","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VMNLRP3S","conferencePaper","2021","Pfeiffer, Jonas; Vulić, Ivan; Gurevych, Iryna; Ruder, Sebastian","UNKs Everywhere: Adapting Multilingual Language Models to New Scripts","Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing","","","10.18653/v1/2021.emnlp-main.800","https://aclanthology.org/2021.emnlp-main.800","Massively multilingual language models such as multilingual BERT offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to limited capacity and large differences in pretraining data sizes, there is a profound performance gap between resource-rich and resource-poor target languages. The ultimate challenge is dealing with under-resourced languages not covered at all by the models and written in scripts unseen during pretraining. In this work, we propose a series of novel data-efficient methods that enable quick and effective adaptation of pretrained multilingual models to such low-resource languages and unseen scripts. Relying on matrix factorization, our methods capitalize on the existing latent knowledge about multiple languages already available in the pretrained model's embedding matrix. Furthermore, we show that learning of the new dedicated embedding matrix in the target language can be improved by leveraging a small number of vocabulary items (i.e., the so-called lexically overlapping tokens) shared between mBERT's and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model.","2021-11","2022-11-23 06:53:28","2022-11-23 06:53:28","2022-11-23 06:53:28","10186–10203","","","","","","UNKs Everywhere","","","","","Association for Computational Linguistics","Online and Punta Cana, Dominican Republic","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/J5IKYLK2/Pfeiffer et al. - 2021 - UNKs Everywhere Adapting Multilingual Language Mo.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","EMNLP 2021","","","","","","","","","","","","","","",""
"XGBYXJ4I","conferencePaper","2022","Pfeiffer, Jonas; Goyal, Naman; Lin, Xi; Li, Xian; Cross, James; Riedel, Sebastian; Artetxe, Mikel","Lifting the Curse of Multilinguality by Pre-training Modular Transformers","Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","","","10.18653/v1/2022.naacl-main.255","https://aclanthology.org/2022.naacl-main.255","Multilingual pre-trained models are known to suffer from the curse of multilinguality, which causes per-language performance to drop as they cover more languages. We address this issue by introducing language-specific modules, which allows us to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant. In contrast with prior work that learns language-specific components post-hoc, we pre-train the modules of our Cross-lingual Modular (X-Mod) models from the start. Our experiments on natural language inference, named entity recognition and question answering show that our approach not only mitigates the negative interference between languages, but also enables positive transfer, resulting in improved monolingual and cross-lingual performance. Furthermore, our approach enables adding languages post-hoc with no measurable drop in performance, no longer limiting the model usage to the set of pre-trained languages.","2022-07","2022-11-23 06:53:31","2022-11-23 06:53:31","2022-11-23 06:53:31","3479–3495","","","","","","","","","","","Association for Computational Linguistics","Seattle, United States","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/2WYUV9HD/Pfeiffer et al. - 2022 - Lifting the Curse of Multilinguality by Pre-traini.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NAACL-HLT 2022","","","","","","","","","","","","","","",""
"W6JKBSH5","conferencePaper","2020","Pfeiffer, Jonas; Vulić, Ivan; Gurevych, Iryna; Ruder, Sebastian","MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer","Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)","","","10.18653/v1/2020.emnlp-main.617","https://aclanthology.org/2020.emnlp-main.617","The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.","2020-11","2022-11-23 06:53:51","2022-11-23 06:53:51","2022-11-23 06:53:51","7654–7673","","","","","","MAD-X","","","","","Association for Computational Linguistics","Online","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/NLGFZ7HG/Pfeiffer et al. - 2020 - MAD-X An Adapter-Based Framework for Multi-Task C.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","EMNLP 2020","","","","","","","","","","","","","","",""
"Q7XS8UJF","preprint","2022","He, Junxian; Zhou, Chunting; Ma, Xuezhe; Berg-Kirkpatrick, Taylor; Neubig, Graham","Towards a Unified View of Parameter-Efficient Transfer Learning","","","","10.48550/arXiv.2110.04366","http://arxiv.org/abs/2110.04366","Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.","2022-02-02","2022-11-23 06:57:30","2022-11-23 06:57:30","2022-11-23 06:57:30","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2110.04366 [cs]","","/home/lexi/.zotero-data/storage/AMB6VB4L/He et al. - 2022 - Towards a Unified View of Parameter-Efficient Tran.pdf; /home/lexi/.zotero-data/storage/NGIBSQTV/2110.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2110.04366","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M6K2C52R","conferencePaper","2022","Ansell, Alan; Ponti, Edoardo; Korhonen, Anna; Vulić, Ivan","Composable Sparse Fine-Tuning for Cross-Lingual Transfer","Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2022.acl-long.125","https://aclanthology.org/2022.acl-long.125","Fine-tuning the entire set of parameters of a large pretrained model has become the mainstream approach for transfer learning. To increase its efficiency and prevent catastrophic forgetting and interference, techniques like adapters and sparse fine-tuning have been developed. Adapters are modular, as they can be combined to adapt a model towards different facets of knowledge (e.g., dedicated language and/or task adapters). Sparse fine-tuning is expressive, as it controls the behavior of all model components. In this work, we introduce a new fine-tuning method with both these desirable properties. In particular, we learn sparse, real-valued masks based on a simple variant of the Lottery Ticket Hypothesis. Task-specific masks are obtained from annotated data in a source language, and language-specific masks from masked language modeling in a target language. Both these masks can then be composed with the pretrained model. Unlike adapter-based fine-tuning, this method neither increases the number of parameters at inference time nor alters the original model architecture. Most importantly, it outperforms adapters in zero-shot cross-lingual transfer by a large margin in a series of multilingual benchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we additionally find that sparsity is crucial to prevent both 1) interference between the fine-tunings to be composed and 2) overfitting. We release the code and models at https://github.com/cambridgeltl/composable-sft.","2022-05","2022-11-23 07:02:25","2022-11-23 07:02:25","2022-11-23 07:02:25","1778–1796","","","","","","","","","","","Association for Computational Linguistics","Dublin, Ireland","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/GTSRC2C4/Ansell et al. - 2022 - Composable Sparse Fine-Tuning for Cross-Lingual Tr.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL 2022","","","","","","","","","","","","","","",""
"GSDQYCIQ","conferencePaper","2022","Hofmann, Valentin; Schuetze, Hinrich; Pierrehumbert, Janet","An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers","Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)","","","10.18653/v1/2022.acl-short.43","https://aclanthology.org/2022.acl-short.43","We introduce FLOTA (Few Longest Token Approximation), a simple yet effective method to improve the tokenization of pretrained language models (PLMs). FLOTA uses the vocabulary of a standard tokenizer but tries to preserve the morphological structure of words during tokenization. We evaluate FLOTA on morphological gold segmentations as well as a text classification task, using BERT, GPT-2, and XLNet as example PLMs. FLOTA leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise.","2022-05","2022-11-23 07:02:33","2022-11-23 07:02:33","2022-11-23 07:02:33","385–393","","","","","","","","","","","Association for Computational Linguistics","Dublin, Ireland","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/HRJEXV34/Hofmann et al. - 2022 - An Embarrassingly Simple Method to Mitigate Undesi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL 2022","","","","","","","","","","","","","","",""
"TAI9NTEN","conferencePaper","2022","Nzeyimana, Antoine; Niyongabo Rubungo, Andre","KinyaBERT: a Morphology-aware Kinyarwanda Language Model","Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2022.acl-long.367","https://aclanthology.org/2022.acl-long.367","Pre-trained language models such as BERT have been successful at tackling many natural language processing tasks. However, the unsupervised sub-word tokenization methods commonly used in these models (e.g., byte-pair encoding - BPE) are sub-optimal at handling morphologically rich languages. Even given a morphological analyzer, naive sequencing of morphemes into a standard BERT architecture is inefficient at capturing morphological compositionality and expressing word-relative syntactic regularities. We address these challenges by proposing a simple yet effective two-tier BERT architecture that leverages a morphological analyzer and explicitly represents morphological compositionality.Despite the success of BERT, most of its evaluations have been conducted on high-resource languages, obscuring its applicability on low-resource languages. We evaluate our proposed method on the low-resource morphologically rich Kinyarwanda language, naming the proposed model architecture KinyaBERT. A robust set of experimental results reveal that KinyaBERT outperforms solid baselines by 2% in F1 score on a named entity recognition task and by 4.3% in average score of a machine-translated GLUE benchmark. KinyaBERT fine-tuning has better convergence and achieves more robust results on multiple tasks even in the presence of translation noise.","2022-05","2022-11-23 07:03:56","2022-11-23 07:03:56","2022-11-23 07:03:56","5347–5363","","","","","","KinyaBERT","","","","","Association for Computational Linguistics","Dublin, Ireland","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/VBTESLGU/Nzeyimana and Niyongabo Rubungo - 2022 - KinyaBERT a Morphology-aware Kinyarwanda Language.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL 2022","","","","","","","","","","","","","","",""
"KCIKVBD5","conferencePaper","2021","Wang, Xinyi; Ruder, Sebastian; Neubig, Graham","Multi-view Subword Regularization","Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","","","10.18653/v1/2021.naacl-main.40","https://aclanthology.org/2021.naacl-main.40","Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency of predictors between using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark (Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.","2021-06","2022-11-23 07:04:16","2022-11-23 07:04:16","2022-11-23 07:04:16","473–482","","","","","","","","","","","Association for Computational Linguistics","Online","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/P5T3E24Q/Wang et al. - 2021 - Multi-view Subword Regularization.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NAACL-HLT 2021","","","","","","","","","","","","","","",""
"KMI6WVHE","preprint","2020","Levine, Yoav; Lenz, Barak; Lieber, Opher; Abend, Omri; Leyton-Brown, Kevin; Tennenholtz, Moshe; Shoham, Yoav","PMI-Masking: Principled masking of correlated spans","","","","10.48550/arXiv.2010.01825","http://arxiv.org/abs/2010.01825","Masking tokens uniformly at random constitutes a common flaw in the pretraining of Masked Language Models (MLMs) such as BERT. We show that such uniform masking allows an MLM to minimize its training objective by latching onto shallow local signals, leading to pretraining inefficiency and suboptimal downstream performance. To address this flaw, we propose PMI-Masking, a principled masking strategy based on the concept of Pointwise Mutual Information (PMI), which jointly masks a token n-gram if it exhibits high collocation over the corpus. PMI-Masking motivates, unifies, and improves upon prior more heuristic approaches that attempt to address the drawback of random uniform token masking, such as whole-word masking, entity/phrase masking, and random-span masking. Specifically, we show experimentally that PMI-Masking reaches the performance of prior masking approaches in half the training time, and consistently improves performance at the end of training.","2020-10-05","2022-11-23 07:05:36","2022-11-23 07:05:36","2022-11-23 07:05:36","","","","","","","PMI-Masking","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2010.01825 [cs, stat]","","/home/lexi/.zotero-data/storage/JBKVWGXR/Levine et al. - 2020 - PMI-Masking Principled masking of correlated span.pdf; /home/lexi/.zotero-data/storage/V54DGYJF/2010.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2010.01825","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IXN8IGQG","preprint","2021","Ben-Baruch, Emanuel; Ridnik, Tal; Zamir, Nadav; Noy, Asaf; Friedman, Itamar; Protter, Matan; Zelnik-Manor, Lihi","Asymmetric Loss For Multi-Label Classification","","","","10.48550/arXiv.2009.14119","http://arxiv.org/abs/2009.14119","In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss (""ASL""), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity. Implementation is available at: https://github.com/Alibaba-MIIL/ASL.","2021-07-29","2022-11-24 15:59:31","2022-11-24 15:59:31","2022-11-24 15:59:31","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2009.14119 [cs]","","/home/lexi/.zotero-data/storage/TXH5NV9H/Ben-Baruch et al. - 2021 - Asymmetric Loss For Multi-Label Classification.pdf; /home/lexi/.zotero-data/storage/SW9IWY8W/2009.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2009.14119","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PHM53I6M","preprint","2017","Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia","Attention Is All You Need","","","","10.48550/arXiv.1706.03762","http://arxiv.org/abs/1706.03762","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","2017-12-05","2022-11-29 05:44:08","2025-06-12 15:57:35","2022-11-29 05:44:08","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1706.03762 [cs]","","/home/lexi/.zotero-data/storage/WBTSPKVV/Vaswani et al. - 2017 - Attention Is All You Need.pdf; /home/lexi/.zotero-data/storage/6QT7XQJE/1706.html","","milestone","","","","","","","","","","","","","","","","","","","","arXiv:1706.03762","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B8IPLAZ5","conferencePaper","2019","Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina","BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","Proceedings of the 2019 Conference of the North","","","10.18653/v1/N19-1423","http://aclweb.org/anthology/N19-1423","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","2019-05-24","2022-11-29 05:44:21","2022-11-29 06:43:56","2022-11-29 06:41:43","4171-4186","","","","","","BERT","","","","","Association for Computational Linguistics","Minneapolis, Minnesota","en","","","","","DOI.org (Crossref)","","arXiv:1810.04805 [cs] Citation Key: devlin2019BERT","","/home/lexi/.zotero-data/storage/BCQZLGUN/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf; /home/lexi/.zotero-data/storage/9P6EMW5D/1810.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 2019 Conference of the North","","","","","","","","","","","","","","",""
"H4ZBCEAV","preprint","2022","Bapna, Ankur; Caswell, Isaac; Kreutzer, Julia; Firat, Orhan; van Esch, Daan; Siddhant, Aditya; Niu, Mengmeng; Baljekar, Pallavi; Garcia, Xavier; Macherey, Wolfgang; Breiner, Theresa; Axelrod, Vera; Riesa, Jason; Cao, Yuan; Chen, Mia Xu; Macherey, Klaus; Krikun, Maxim; Wang, Pidong; Gutkin, Alexander; Shah, Apurva; Huang, Yanping; Chen, Zhifeng; Wu, Yonghui; Hughes, Macduff","Building Machine Translation Systems for the Next Thousand Languages","","","","10.48550/arXiv.2205.03983","http://arxiv.org/abs/2205.03983","In this paper we share findings from our effort to build practical machine translation (MT) systems capable of translating across over one thousand languages. We describe results in three research domains: (i) Building clean, web-mined datasets for 1500+ languages by leveraging semi-supervised pre-training for language identification and developing data-driven filtering techniques; (ii) Developing practical MT models for under-served languages by leveraging massively multilingual models trained with supervised parallel data for over 100 high-resource languages and monolingual datasets for an additional 1000+ languages; and (iii) Studying the limitations of evaluation metrics for these languages and conducting qualitative analysis of the outputs from our MT models, highlighting several frequent error modes of these types of models. We hope that our work provides useful insights to practitioners working towards building MT systems for currently understudied languages, and highlights research directions that can complement the weaknesses of massively multilingual models in data-sparse settings.","2022-07-06","2022-11-29 15:45:15","2022-11-29 15:46:30","2022-11-29 15:45:15","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2205.03983 [cs]","","/home/lexi/.zotero-data/storage/R5UVNYKX/Bapna et al. - 2022 - Building Machine Translation Systems for the Next .pdf; /home/lexi/.zotero-data/storage/FT9MZIKL/2205.html","","skimmed","","","","","","","","","","","","","","","","","","","","arXiv:2205.03983","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XKQCTDUY","conferencePaper","2020","Card, Dallas; Henderson, Peter; Khandelwal, Urvashi; Jia, Robin; Mahowald, Kyle; Jurafsky, Dan","With Little Power Comes Great Responsibility","","","","10.18653/v1/2020.emnlp-main.745","https://www.aclweb.org/anthology/2020.emnlp-main.745","Dallas Card, Peter Henderson, Urvashi Khandelwal, Robin Jia, Kyle Mahowald, Dan Jurafsky. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.","2020-11","2022-11-29 16:11:28","2022-11-29 16:11:36","2022-11-29 16:11:28","9263-9274","","","","","","","","","","","","","en-us","","","","","aclanthology.lst.uni-saarland.de","","","","/home/lexi/.zotero-data/storage/KUEXU4JW/Card et al. - 2020 - With Little Power Comes Great Responsibility.pdf; /home/lexi/.zotero-data/storage/AP4BWW4K/2020.emnlp-main.745.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)","","","","","","","","","","","","","","",""
"YXTVALUC","preprint","2022","Garg, Shivam; Tsipras, Dimitris; Liang, Percy; Valiant, Gregory","What Can Transformers Learn In-Context? A Case Study of Simple Function Classes","","","","10.48550/arXiv.2208.01066","http://arxiv.org/abs/2208.01066","In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn ""most"" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .","2022-08-01","2022-11-30 08:53:21","2022-11-30 08:53:28","2022-11-30 08:53:21","","","","","","","What Can Transformers Learn In-Context?","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2208.01066 [cs]","","/home/lexi/.zotero-data/storage/2K3MK9BQ/Garg et al. - 2022 - What Can Transformers Learn In-Context A Case Stu.pdf; /home/lexi/.zotero-data/storage/SVI9KJ3M/2208.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2208.01066","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WJBA9LYC","preprint","2022","Power, Alethea; Burda, Yuri; Edwards, Harri; Babuschkin, Igor; Misra, Vedant","Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets","","","","10.48550/arXiv.2201.02177","http://arxiv.org/abs/2201.02177","In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of ""grokking"" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.","2022-01-06","2022-11-30 08:54:27","2022-11-30 08:54:28","2022-11-30 08:54:27","","","","","","","Grokking","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2201.02177 [cs]","","/home/lexi/.zotero-data/storage/9A527NC5/Power et al. - 2022 - Grokking Generalization Beyond Overfitting on Sma.pdf; /home/lexi/.zotero-data/storage/MFHCIASD/2201.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2201.02177","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"78DF3GXQ","preprint","2020","Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan; Le, Quoc V.","XLNet: Generalized Autoregressive Pretraining for Language Understanding","","","","10.48550/arXiv.1906.08237","http://arxiv.org/abs/1906.08237","With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.","2020-01-02","2022-11-30 09:17:57","2022-11-30 09:17:58","2022-11-30 09:17:57","","","","","","","XLNet","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1906.08237 [cs]","","/home/lexi/.zotero-data/storage/RYNKCIS9/Yang et al. - 2020 - XLNet Generalized Autoregressive Pretraining for .pdf; /home/lexi/.zotero-data/storage/IHBRGEYX/1906.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1906.08237","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4YSQNQRP","blogPost","2019","Team, XLNet","A Fair Comparison Study of XLNet and BERT with Large Models","Medium","","","","https://medium.com/@xlnet.team/a-fair-comparison-study-of-xlnet-and-bert-with-large-models-5a4257f59dc0","Several weeks ago, we released our new model XLNet, which outperforms BERT on a variety of benchmarks. Our largest model was trained on…","2019-07-22","2022-11-30 09:21:19","2022-11-30 09:21:23","2022-11-30 09:21:19","","","","","","","","","","","","","","en","","","","","","","","","/home/lexi/.zotero-data/storage/33Z49KIF/a-fair-comparison-study-of-xlnet-and-bert-with-large-models-5a4257f59dc0.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FBU2GDL5","preprint","2019","Liu, Yinhan; Ott, Myle; Goyal, Naman; Du, Jingfei; Joshi, Mandar; Chen, Danqi; Levy, Omer; Lewis, Mike; Zettlemoyer, Luke; Stoyanov, Veselin","RoBERTa: A Robustly Optimized BERT Pretraining Approach","","","","10.48550/arXiv.1907.11692","http://arxiv.org/abs/1907.11692","Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.","2019-07-26","2022-11-30 09:26:37","2022-11-30 09:26:37","2022-11-30 09:26:37","","","","","","","RoBERTa","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1907.11692 [cs]","","/home/lexi/.zotero-data/storage/DF7RGSG9/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf; /home/lexi/.zotero-data/storage/FLGSUB8M/1907.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1907.11692","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D35BKLBU","preprint","2019","Sun, Yu; Wang, Shuohuan; Li, Yukun; Feng, Shikun; Chen, Xuyi; Zhang, Han; Tian, Xin; Zhu, Danxiang; Tian, Hao; Wu, Hua","ERNIE: Enhanced Representation through Knowledge Integration","","","","10.48550/arXiv.1904.09223","http://arxiv.org/abs/1904.09223","We present a novel language representation model enhanced by knowledge called ERNIE (Enhanced Representation through kNowledge IntEgration). Inspired by the masking strategy of BERT, ERNIE is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking. Entity-level strategy masks entities which are usually composed of multiple words.Phrase-level strategy masks the whole phrase which is composed of several words standing together as a conceptual unit.Experimental results show that ERNIE outperforms other baseline methods, achieving new state-of-the-art results on five Chinese natural language processing tasks including natural language inference, semantic similarity, named entity recognition, sentiment analysis and question answering. We also demonstrate that ERNIE has more powerful knowledge inference capacity on a cloze test.","2019-04-19","2022-11-30 09:28:45","2022-11-30 09:28:45","2022-11-30 09:28:45","","","","","","","ERNIE","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1904.09223 [cs]","","/home/lexi/.zotero-data/storage/GN2IHFVE/Sun et al. - 2019 - ERNIE Enhanced Representation through Knowledge I.pdf; /home/lexi/.zotero-data/storage/WULJI4PK/1904.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1904.09223","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6UQK87UV","preprint","2020","Lan, Zhenzhong; Chen, Mingda; Goodman, Sebastian; Gimpel, Kevin; Sharma, Piyush; Soricut, Radu","ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","","","","10.48550/arXiv.1909.11942","http://arxiv.org/abs/1909.11942","Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.","2020-02-08","2022-11-30 09:30:28","2022-11-30 09:30:28","2022-11-30 09:30:28","","","","","","","ALBERT","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1909.11942 [cs]","","/home/lexi/.zotero-data/storage/3H9Q9NQG/Lan et al. - 2020 - ALBERT A Lite BERT for Self-supervised Learning o.pdf; /home/lexi/.zotero-data/storage/LH76HWSS/1909.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1909.11942","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WKWNBJNU","preprint","2020","Clark, Kevin; Luong, Minh-Thang; Le, Quoc V.; Manning, Christopher D.","ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators","","","","10.48550/arXiv.2003.10555","http://arxiv.org/abs/2003.10555","Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.","2020-03-23","2022-11-30 09:32:15","2022-11-30 09:32:15","2022-11-30 09:32:15","","","","","","","ELECTRA","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2003.10555 [cs]","","/home/lexi/.zotero-data/storage/8FYC8FLS/Clark et al. - 2020 - ELECTRA Pre-training Text Encoders as Discriminat.pdf; /home/lexi/.zotero-data/storage/UC5TTDGS/2003.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2003.10555","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2LBG7UKP","preprint","2021","Zaheer, Manzil; Guruganesh, Guru; Dubey, Avinava; Ainslie, Joshua; Alberti, Chris; Ontanon, Santiago; Pham, Philip; Ravula, Anirudh; Wang, Qifan; Yang, Li; Ahmed, Amr","Big Bird: Transformers for Longer Sequences","","","","10.48550/arXiv.2007.14062","http://arxiv.org/abs/2007.14062","Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.","2021-01-08","2022-11-30 09:45:05","2022-11-30 09:45:05","2022-11-30 09:45:05","","","","","","","Big Bird","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2007.14062 [cs, stat]","","/home/lexi/.zotero-data/storage/783JECWA/Zaheer et al. - 2021 - Big Bird Transformers for Longer Sequences.pdf; /home/lexi/.zotero-data/storage/C73YHNB8/2007.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2007.14062","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PP5PIFZG","journalArticle","2020","Qiu, Xipeng; Sun, Tianxiang; Xu, Yige; Shao, Yunfan; Dai, Ning; Huang, Xuanjing","Pre-trained Models for Natural Language Processing: A Survey","Science China Technological Sciences","","1674-7321, 1869-1900","10.1007/s11431-020-1647-3","http://arxiv.org/abs/2003.08271","Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy with four perspectives. Next, we describe how to adapt the knowledge of PTMs to the downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.","2020-10","2022-11-30 10:29:27","2022-11-30 10:29:27","2022-11-30 10:29:26","1872-1897","","10","63","","Sci. China Technol. Sci.","Pre-trained Models for Natural Language Processing","","","","","","","","","","","","arXiv.org","","arXiv:2003.08271 [cs]","","/home/lexi/.zotero-data/storage/SQ4DEY7C/Qiu et al. - 2020 - Pre-trained Models for Natural Language Processing.pdf; /home/lexi/.zotero-data/storage/A4KG4523/2003.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VS9E6GMA","preprint","2020","Chung, Hyung Won; Févry, Thibault; Tsai, Henry; Johnson, Melvin; Ruder, Sebastian","Rethinking embedding coupling in pre-trained language models","","","","10.48550/arXiv.2010.12821","http://arxiv.org/abs/2010.12821","We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art pre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to significantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By reallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on standard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that allocating additional capacity to the output embedding provides benefits to the model that persist through the fine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the number of parameters at the fine-tuning stage.","2020-10-24","2022-11-30 11:06:47","2022-11-30 11:06:47","2022-11-30 11:06:47","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2010.12821 [cs]","","/home/lexi/.zotero-data/storage/DGUC7UKJ/Chung et al. - 2020 - Rethinking embedding coupling in pre-trained langu.pdf; /home/lexi/.zotero-data/storage/FI6N8BKI/2010.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2010.12821","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DKHJBFJJ","conferencePaper","2022","Lialin, Vladislav; Zhao, Kevin; Shivagunde, Namrata; Rumshisky, Anna","Life after BERT: What do Other Muppets Understand about Language?","Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2022.acl-long.227","https://aclanthology.org/2022.acl-long.227","Existing pre-trained transformer analysis works usually focus only on one or two model families at a time, overlooking the variability of the architecture and pre-training objectives. In our work, we utilize the oLMpics bench- mark and psycholinguistic probing datasets for a diverse set of 29 models including T5, BART, and ALBERT. Additionally, we adapt the oLMpics zero-shot setup for autoregres- sive models and evaluate GPT networks of different sizes. Our findings show that none of these models can resolve compositional questions in a zero-shot fashion, suggesting that this skill is not learnable using existing pre-training objectives. Furthermore, we find that global model decisions such as architecture, directionality, size of the dataset, and pre-training objective are not predictive of a model's linguistic capabilities.","2022-05","2022-12-01 08:59:04","2022-12-01 08:59:07","2022-12-01 08:59:04","3180–3193","","","","","","Life after BERT","","","","","Association for Computational Linguistics","Dublin, Ireland","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/34L7NBX3/Lialin et al. - 2022 - Life after BERT What do Other Muppets Understand .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL 2022","","","","","","","","","","","","","","",""
"2EGBTA3S","preprint","2022","Kvinge, Henry; Emerson, Tegan H.; Jorgenson, Grayson; Vasquez, Scott; Doster, Timothy; Lew, Jesse D.","In What Ways Are Deep Neural Networks Invariant and How Should We Measure This?","","","","10.48550/arXiv.2210.03773","http://arxiv.org/abs/2210.03773","It is often said that a deep learning model is ""invariant"" to some specific type of transformation. However, what is meant by this statement strongly depends on the context in which it is made. In this paper we explore the nature of invariance and equivariance of deep learning models with the goal of better understanding the ways in which they actually capture these concepts on a formal level. We introduce a family of invariance and equivariance metrics that allows us to quantify these properties in a way that disentangles them from other metrics such as loss or accuracy. We use our metrics to better understand the two most popular methods used to build invariance into networks: data augmentation and equivariant layers. We draw a range of conclusions about invariance and equivariance in deep learning models, ranging from whether initializing a model with pretrained weights has an effect on a trained model's invariance, to the extent to which invariance learned via training can generalize to out-of-distribution data.","2022-10-07","2022-12-01 09:17:30","2022-12-01 09:17:32","2022-12-01 09:17:30","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2210.03773 [cs]","","/home/lexi/.zotero-data/storage/3JRWR5B7/Kvinge et al. - 2022 - In What Ways Are Deep Neural Networks Invariant an.pdf; /home/lexi/.zotero-data/storage/EK4PL34P/2210.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2210.03773","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y2BPRPVG","preprint","2022","Lee, Kiwon; Cheng, Andrew N.; Paquette, Courtney; Paquette, Elliot","Trajectory of Mini-Batch Momentum: Batch Size Saturation and Convergence in High Dimensions","","","","10.48550/arXiv.2206.01029","http://arxiv.org/abs/2206.01029","We analyze the dynamics of large batch stochastic gradient descent with momentum (SGD+M) on the least squares problem when both the number of samples and dimensions are large. In this setting, we show that the dynamics of SGD+M converge to a deterministic discrete Volterra equation as dimension increases, which we analyze. We identify a stability measurement, the implicit conditioning ratio (ICR), which regulates the ability of SGD+M to accelerate the algorithm. When the batch size exceeds this ICR, SGD+M converges linearly at a rate of $\mathcal{O}(1/\sqrt{\kappa})$, matching optimal full-batch momentum (in particular performing as well as a full-batch but with a fraction of the size). For batch sizes smaller than the ICR, in contrast, SGD+M has rates that scale like a multiple of the single batch SGD rate. We give explicit choices for the learning rate and momentum parameter in terms of the Hessian spectra that achieve this performance.","2022-06-02","2022-12-01 09:17:39","2022-12-01 09:17:39","2022-12-01 09:17:39","","","","","","","Trajectory of Mini-Batch Momentum","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2206.01029 [cs, math, stat]","","/home/lexi/.zotero-data/storage/K7PKNG4Z/Lee et al. - 2022 - Trajectory of Mini-Batch Momentum Batch Size Satu.pdf; /home/lexi/.zotero-data/storage/7RLDFTQW/2206.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2206.01029","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WPWBLE6M","preprint","2022","Anil, Cem; Wu, Yuhuai; Andreassen, Anders; Lewkowycz, Aitor; Misra, Vedant; Ramasesh, Vinay; Slone, Ambrose; Gur-Ari, Guy; Dyer, Ethan; Neyshabur, Behnam","Exploring Length Generalization in Large Language Models","","","","10.48550/arXiv.2207.04901","http://arxiv.org/abs/2207.04901","The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.","2022-11-14","2022-12-01 09:18:02","2022-12-01 09:18:02","2022-12-01 09:18:02","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2207.04901 [cs]","","","","","","","","","","","","","","","","","","","","","","","","arXiv:2207.04901","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DGIPXL7A","conferencePaper","2022","Moreno, Alexander; Wu, Zhenke; Nagesh, Supriya; Dempsey, Walter H.; Rehg, James Matthew","Kernel Multimodal Continuous Attention","","","","","https://openreview.net/forum?id=qmy23tNBvbh","Attention mechanisms take an expectation of a data representation with respect to probability weights. Recently, (Martins et al. 2020, 2021) proposed continuous attention mechanisms, focusing on unimodal attention densities from the exponential and deformed exponential families: the latter has sparse support. (Farinhas et al 2021) extended this to to multimodality via Gaussian mixture attention densities. In this paper, we extend this to kernel exponential families (Canu and Smola 2006) and our new sparse counterpart, kernel deformed exponential families. Theoretically, we show new existence results for both kernel exponential and deformed exponential families, and that the deformed case has similar approximation capabilities to kernel exponential families. Lacking closed form expressions for the context vector, we use numerical integration: we show exponential convergence for both kernel exponential and deformed exponential families. Experiments show that kernel continuous attention often outperforms unimodal continuous attention, and the sparse variant tends to highlight peaks of time series.","2022-10-31","2022-12-01 09:19:18","2022-12-01 09:19:18","2022-12-01 09:19:18","","","","","","","","","","","","","","en","","","","","openreview.net","","","","/home/lexi/.zotero-data/storage/Y7MTDR9F/Moreno et al. - 2022 - Kernel Multimodal Continuous Attention.pdf; /home/lexi/.zotero-data/storage/MLC4KKKB/forum.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Advances in Neural Information Processing Systems","","","","","","","","","","","","","","",""
"57IHB54K","preprint","2022","Orvieto, Antonio; Lacoste-Julien, Simon; Loizou, Nicolas","Dynamics of SGD with Stochastic Polyak Stepsizes: Truly Adaptive Variants and Convergence to Exact Solution","","","","10.48550/arXiv.2205.04583","http://arxiv.org/abs/2205.04583","Recently, Loizou et al. (2021) proposed and analyzed stochastic gradient descent (SGD) with stochastic Polyak stepsize (SPS). The proposed SPS comes with strong convergence guarantees and competitive performance; however, it has two main drawbacks when it is used in non-over-parameterized regimes: (i) It requires a priori knowledge of the optimal mini-batch losses, which are not available when the interpolation condition is not satisfied (e.g., regularized objectives), and (ii) it guarantees convergence only to a neighborhood of the solution. In this work, we study the dynamics and the convergence properties of SGD equipped with new variants of the stochastic Polyak stepsize and provide solutions to both drawbacks of the original SPS. We first show that a simple modification of the original SPS that uses lower bounds instead of the optimal function values can directly solve issue (i). On the other hand, solving issue (ii) turns out to be more challenging and leads us to valuable insights into the method's behavior. We show that if interpolation is not satisfied, the correlation between SPS and stochastic gradients introduces a bias, which effectively distorts the expectation of the gradient signal near minimizers, leading to non-convergence - even if the stepsize is scaled down during training. To fix this issue, we propose DecSPS, a novel modification of SPS, which guarantees convergence to the exact minimizer -- without a priori knowledge of the problem parameters. For strongly-convex optimization problems, DecSPS is the first stochastic adaptive optimization method that converges to the exact solution without restrictive assumptions like bounded iterates/gradients.","2022-07-03","2022-12-01 09:19:40","2022-12-01 09:19:40","2022-12-01 09:19:40","","","","","","","Dynamics of SGD with Stochastic Polyak Stepsizes","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2205.04583 [math]","","/home/lexi/.zotero-data/storage/GQNZFVDS/Orvieto et al. - 2022 - Dynamics of SGD with Stochastic Polyak Stepsizes .pdf; /home/lexi/.zotero-data/storage/VT2GKDAW/2205.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2205.04583","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K9EZH2IR","webpage","","","The Bitter Lesson","","","","","http://www.incompleteideas.net/IncIdeas/BitterLesson.html","","","2022-12-01 12:11:54","2022-12-01 12:11:57","2022-12-01 12:11:54","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/J66LJAAB/BitterLesson.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4B5IGSLA","presentation","","","Transfer Learning in Natural Language Processing","","","","","https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc","","","2022-12-01 20:34:36","2022-12-01 20:34:36","2022-12-01 20:34:36","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J72C9A4G","preprint","2022","Wei, Jason; Tay, Yi; Bommasani, Rishi; Raffel, Colin; Zoph, Barret; Borgeaud, Sebastian; Yogatama, Dani; Bosma, Maarten; Zhou, Denny; Metzler, Donald; Chi, Ed H.; Hashimoto, Tatsunori; Vinyals, Oriol; Liang, Percy; Dean, Jeff; Fedus, William","Emergent Abilities of Large Language Models","","","","10.48550/arXiv.2206.07682","http://arxiv.org/abs/2206.07682","Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.","2022-10-26","2022-12-16 10:36:25","2022-12-16 10:36:29","2022-12-16 10:36:25","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2206.07682 [cs]","","/home/lexi/.zotero-data/storage/4DA2BWYZ/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf; /home/lexi/.zotero-data/storage/PTUP53JY/2206.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2206.07682","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z2XY63KR","preprint","2022","Tay, Yi; Wei, Jason; Chung, Hyung Won; Tran, Vinh Q.; So, David R.; Shakeri, Siamak; Garcia, Xavier; Zheng, Huaixiu Steven; Rao, Jinfeng; Chowdhery, Aakanksha; Zhou, Denny; Metzler, Donald; Petrov, Slav; Houlsby, Neil; Le, Quoc V.; Dehghani, Mostafa","Transcending Scaling Laws with 0.1% Extra Compute","","","","10.48550/arXiv.2210.11399","http://arxiv.org/abs/2210.11399","Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving $\sim$4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.","2022-11-16","2022-12-16 10:39:53","2022-12-16 10:39:53","2022-12-16 10:39:53","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2210.11399 [cs]","","/home/lexi/.zotero-data/storage/EHQVB44B/Tay et al. - 2022 - Transcending Scaling Laws with 0.1% Extra Compute.pdf; /home/lexi/.zotero-data/storage/ZCKXW8PK/2210.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2210.11399","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XYHR7DNZ","preprint","2022","Tay, Yi; Dehghani, Mostafa; Tran, Vinh Q.; Garcia, Xavier; Wei, Jason; Wang, Xuezhi; Chung, Hyung Won; Bahri, Dara; Schuster, Tal; Zheng, Huaixiu Steven; Zhou, Denny; Houlsby, Neil; Metzler, Donald","UL2: Unifying Language Learning Paradigms","","","","10.48550/arXiv.2205.05131","http://arxiv.org/abs/2205.05131","Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. Finally, we show that UL2 20B works well with chain-of-thought prompting and reasoning. We release Flax-based T5X model checkpoints for the 20B model at \url{https://github.com/google-research/google-research/tree/master/ul2}.","2022-10-08","2022-12-16 10:39:58","2022-12-16 10:39:58","2022-12-16 10:39:58","","","","","","","UL2","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2205.05131 [cs]","","/home/lexi/.zotero-data/storage/H7BVCCHT/Tay et al. - 2022 - UL2 Unifying Language Learning Paradigms.pdf; /home/lexi/.zotero-data/storage/XPFEYX6G/2205.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2205.05131","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3YECQBRU","preprint","2022","Chen, Yutian; Song, Xingyou; Lee, Chansoo; Wang, Zi; Zhang, Qiuyi; Dohan, David; Kawakami, Kazuya; Kochanski, Greg; Doucet, Arnaud; Ranzato, Marc'aurelio; Perel, Sagi; de Freitas, Nando","Towards Learning Universal Hyperparameter Optimizers with Transformers","","","","10.48550/arXiv.2205.13320","http://arxiv.org/abs/2205.13320","Meta-learning hyperparameter optimization (HPO) algorithms from prior experiments is a promising approach to improve optimization efficiency over objective functions from a similar distribution. However, existing methods are restricted to learning from experiments sharing the same set of hyperparameters. In this paper, we introduce the OptFormer, the first text-based Transformer HPO framework that provides a universal end-to-end interface for jointly learning policy and function prediction when trained on vast tuning data from the wild, such as Google's Vizier database, one of the world's largest HPO datasets. Our extensive experiments demonstrate that the OptFormer can simultaneously imitate at least 7 different HPO algorithms, which can be further improved via its function uncertainty estimates. Compared to a Gaussian Process, the OptFormer also learns a robust prior distribution for hyperparameter response functions, and can thereby provide more accurate and better calibrated predictions. This work paves the path to future extensions for training a Transformer-based model as a general HPO optimizer.","2022-10-13","2022-12-16 11:17:07","2022-12-16 11:17:08","2022-12-16 11:17:06","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2205.13320 [cs, stat]","","/home/lexi/.zotero-data/storage/VBQIQ4V3/Chen et al. - 2022 - Towards Learning Universal Hyperparameter Optimize.pdf; /home/lexi/.zotero-data/storage/X3A7FCAP/2205.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2205.13320","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A78ZITVG","preprint","2022","Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie; Askell, Amanda; Welinder, Peter; Christiano, Paul; Leike, Jan; Lowe, Ryan","Training language models to follow instructions with human feedback","","","","10.48550/arXiv.2203.02155","http://arxiv.org/abs/2203.02155","Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.","2022-03-04","2023-06-19 10:23:13","2023-09-20 13:12:26","2023-06-19 10:23:13","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2203.02155 [cs]","","/home/lexi/.zotero-data/storage/KQMI7JYE/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf; /home/lexi/.zotero-data/storage/YFGBTZYR/2203.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2203.02155","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5IWZPKNX","videoRecording","2023","ML Trainings","Влад Голощапов  - Моментум истины: Не всем известные свойства оптимизаторов с импульсом - SGD,Adam..","","","","","https://www.youtube.com/watch?v=Npm-awHtfeM","Моментум истины: Не всем известные свойства оптимизаторов с импульсом - SGD, Adam и т.д., напомнившие о себе ошибкой в torch.optim.SGD и визуализация внутреннего состояния обучения Data Fest 2023: https://ods.ai/events/datafestonline2023 Трек ""Random DS/ML"":  https://ods.ai/tracks/df23-random_ds-ml Наши соц.сети: Telegram: https://t.me/datafest Вконтакте: https://vk.com/datafest","2023-06-18","2023-06-19 12:27:39","2023-06-19 12:27:51","2023-06-19 12:27:39","","","","","","","Влад Голощапов  - Моментум истины","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","33:32","","","","","","","","","","","","","","","","","","","","","","","","",""
"AA29NY5B","conferencePaper","2020","An, Bang; Lyu, Jie; Wang, Zhenyi; Li, Chunyuan; Hu, Changwei; Tan, Fei; Zhang, Ruiyi; Hu, Yifan; Chen, Changyou","Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference","Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)","","","10.18653/v1/2020.emnlp-main.17","https://aclanthology.org/2020.emnlp-main.17","The neural attention mechanism plays an important role in many natural language processing applications. In particular, multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. However, without explicit constraining, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model's representation power. In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model's expressiveness. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on multiple tasks.","2020-11","2023-06-21 11:48:16","2023-06-21 11:48:16","2023-06-21 11:48:16","236–255","","","","","","Repulsive Attention","","","","","Association for Computational Linguistics","Online","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/KVYJY9CW/An et al. - 2020 - Repulsive Attention Rethinking Multi-head Attenti.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","EMNLP 2020","","","","","","","","","","","","","","",""
"IRNP5LRQ","webpage","","Abnar, On the Merits of Recurrent Inductive Bias | Samira","On the Merits of Recurrent Inductive Bias | Samira Abnar","","","","","https://samiraabnar.github.io/articles/2020-05/recurrence","In this post, we try to understand the nature of recurrent inductive bias. I will discuss different sources of inductive biases of RNNs and provide empirical...","","2023-06-23 16:02:21","2023-06-23 16:02:21","2023-06-23 16:02:21","","","","","","","","","","","","","","en","","","","","","","","","/home/lexi/.zotero-data/storage/3VQFX5VM/recurrence.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7IWFS2CP","preprint","2023","Amatriain, Xavier","Transformer models: an introduction and catalog","","","","10.48550/arXiv.2302.07730","http://arxiv.org/abs/2302.07730","In the past few years we have seen the meteoric appearance of dozens of foundation models of the Transformer family, all of which have memorable and sometimes funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovations in Transformer models. Our catalog will include models that are trained using self-supervised learning (e.g., BERT or GPT3) as well as those that are further trained using a human-in-the-loop (e.g. the InstructGPT model used by ChatGPT).","2023-05-25","2023-06-23 16:05:16","2023-07-05 14:03:22","2023-06-23 16:05:16","","","","","","","Transformer models","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2302.07730 [cs]","","/home/lexi/.zotero-data/storage/77Q6QRSL/Amatriain - 2023 - Transformer models an introduction and catalog.pdf; /home/lexi/.zotero-data/storage/42LQ6APP/2302.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2302.07730","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YLBRFDW8","preprint","2016","Ba, Jimmy Lei; Kiros, Jamie Ryan; Hinton, Geoffrey E.","Layer Normalization","","","","10.48550/arXiv.1607.06450","http://arxiv.org/abs/1607.06450","Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.","2016-07-21","2023-06-23 16:13:39","2023-06-23 16:13:39","2023-06-23 16:13:39","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1607.06450 [cs, stat]","","/home/lexi/.zotero-data/storage/35W78FWM/Ba et al. - 2016 - Layer Normalization.pdf; /home/lexi/.zotero-data/storage/SEALI4BK/1607.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1607.06450","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"72PVNVMC","webpage","","","The Annotated Transformer","","","","","https://nlp.seas.harvard.edu/annotated-transformer/","","","2023-06-23 16:17:53","2023-06-23 16:17:57","2023-06-23 16:17:53","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/7MXKKJAN/annotated-transformer.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L2W8ULIC","journalArticle","2017","Olah, Chris; Mordvintsev, Alexander; Schubert, Ludwig","Feature Visualization","Distill","","2476-0757","10.23915/distill.00007","https://distill.pub/2017/feature-visualization","How neural networks build up their understanding of images","2017-11-07","2023-06-23 16:27:26","2023-06-23 16:27:26","2023-06-23 16:27:26","e7","","11","2","","Distill","","","","","","","","en","","","","","distill.pub","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NVXQ6ZBH","book","","Molnar, Christoph","Interpretable Machine Learning","","","","","https://christophm.github.io/interpretable-ml-book/","Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.","","2023-06-23 16:28:14","2023-06-23 16:28:14","2023-06-23 16:28:14","","","","","","","","","","","","","","","","","","","christophm.github.io","","","","/home/lexi/.zotero-data/storage/Z7GLD7K7/interpretable-ml-book.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GFFUF2ZR","webpage","2020","","Exploring Explainability for Vision Transformers","Jacob Gildenblat","","","","http://jacobgil.github.io/deeplearning/vision-transformer-explainability","👋 Welcome to my personal tech blog about Deep Learning, Machine Learning and Computer Vision.","2020-12-31","2023-06-23 16:28:36","2023-06-23 16:28:36","2023-06-23 16:28:36","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JJL6XCEV","webpage","","Alammar, Jay","Interfaces for Explaining Transformer Language Models","","","","","https://jalammar.github.io/explaining-transformers/","Interfaces for exploring transformer language models by looking at input saliency and neuron activation.              Explorable #1:  Input saliency of a list of countries generated by a language model          Tap or hover over the output tokens:                                  Explorable #2:  Neuron activation analysis reveals four groups of neurons, each is associated with generating a certain type of token         Tap or hover over the sparklines on the left to isolate a certain factor:                     The Transformer architecture     has been powering a number of the recent advances in NLP. A breakdown of this architecture is provided here . Pre-trained language models based on the architecture,     in both its auto-regressive (models that use their own output as input to next time-steps and that process tokens from left-to-right, like GPT2)     and denoising (models trained by corrupting/masking the input and that process tokens bidirectionally, like BERT)     variants continue to push the envelope in various tasks in NLP and, more recently, in computer vision. Our understanding of why these models work so well, however, still lags behind these developments. This exposition series continues the pursuit to interpret     and visualize     the inner-workings of transformer-based language models. We illustrate how some key interpretability methods apply to transformer-based language models. This article focuses on auto-regressive models, but these methods are applicable to other architectures and tasks as well.       This is the first article in the series. In it, we present explorables and visualizations aiding the intuition of:              Input Saliency methods that score input tokens importance to generating a token.                   Neuron Activations and how individual and groups of model neurons spike in response to         inputs and to produce outputs.      The next article addresses Hidden State Evolution across the layers of the model and what it may tell us about each layer's role.","","2023-06-23 16:30:19","2023-06-23 16:30:19","2023-06-23 16:30:19","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QDIGE26C","preprint","2022","Liu, Ziming; Kitouni, Ouail; Nolte, Niklas; Michaud, Eric J.; Tegmark, Max; Williams, Mike","Towards Understanding Grokking: An Effective Theory of Representation Learning","","","","10.48550/arXiv.2205.10343","http://arxiv.org/abs/2205.10343","We aim to understand grokking, a phenomenon where models generalize long after overfitting their training set. We present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters. We find that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting. We observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion. We find representation learning to occur only in a ""Goldilocks zone"" (including comprehension and grokking) between memorization and confusion. We find on transformers the grokking phase stays closer to the memorization phase (compared to the comprehension phase), leading to delayed generalization. The Goldilocks phase is reminiscent of ""intelligence from starvation"" in Darwinian evolution, where resource limitations drive discovery of more efficient solutions. This study not only provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.","2022-10-14","2023-06-23 19:20:31","2023-06-23 19:20:31","2023-06-23 19:20:31","","","","","","","Towards Understanding Grokking","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2205.10343 [cond-mat, physics:physics]","","/home/lexi/.zotero-data/storage/3YL84SET/Liu et al. - 2022 - Towards Understanding Grokking An Effective Theor.pdf; /home/lexi/.zotero-data/storage/8VIIDX7I/2205.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2205.10343","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VNY73XSY","preprint","2023","Touvron, Hugo; Lavril, Thibaut; Izacard, Gautier; Martinet, Xavier; Lachaux, Marie-Anne; Lacroix, Timothée; Rozière, Baptiste; Goyal, Naman; Hambro, Eric; Azhar, Faisal; Rodriguez, Aurelien; Joulin, Armand; Grave, Edouard; Lample, Guillaume","LLaMA: Open and Efficient Foundation Language Models","","","","10.48550/arXiv.2302.13971","http://arxiv.org/abs/2302.13971","We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.","2023-02-27","2023-06-26 07:51:41","2023-06-26 07:51:41","2023-06-26 07:51:40","","","","","","","LLaMA","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2302.13971 [cs]","","/home/lexi/.zotero-data/storage/IGL5CXIZ/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf; /home/lexi/.zotero-data/storage/M8VV7WKH/2302.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2302.13971","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P7NA3MQG","preprint","2017","Schulman, John; Wolski, Filip; Dhariwal, Prafulla; Radford, Alec; Klimov, Oleg","Proximal Policy Optimization Algorithms","","","","10.48550/arXiv.1707.06347","http://arxiv.org/abs/1707.06347","We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ""surrogate"" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.","2017-08-28","2023-07-04 07:31:48","2023-07-04 07:31:48","2023-07-04 07:31:48","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1707.06347 [cs]","","/home/lexi/.zotero-data/storage/LT9PX48T/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf; /home/lexi/.zotero-data/storage/QDMUIQCC/1707.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1707.06347","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C3AGP34K","preprint","2022","Gao, Leo; Schulman, John; Hilton, Jacob","Scaling Laws for Reward Model Overoptimization","","","","10.48550/arXiv.2210.10760","http://arxiv.org/abs/2210.10760","In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed ""gold-standard"" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-$n$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.","2022-10-19","2023-07-05 08:06:18","2023-07-05 08:06:18","2023-07-05 08:06:18","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2210.10760 [cs, stat]","","/home/lexi/.zotero-data/storage/A3ZZWN27/Gao et al. - 2022 - Scaling Laws for Reward Model Overoptimization.pdf; /home/lexi/.zotero-data/storage/BUMMCBGM/2210.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2210.10760","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"46NSWBD4","videoRecording","2023","ML Trainings","Игорь Котенков - RLHF Intro: from Zero to Aligned Intelligent Systems","","","","","https://www.youtube.com/watch?v=N1AcqGVR_mE","- A story about Text Summarization - What the Alignment is, and what's the problem? - How RLHF works     - Data setup, and why we'd like to follow instructions     - Reward Modeling and PPO - Why RLHF works (and when it doesn't) - ChatGPT improvements - What's next and what to expect? Data Fest 2023: https://ods.ai/events/datafestonline2023 Трек ""Instruct Models"":  https://ods.ai/tracks/df23-instruct-m... Наши соц.сети: Telegram: https://t.me/datafest Вконтакте: https://vk.com/datafest","2023-06-09","2023-07-05 09:09:33","2023-07-10 10:36:11","2023-07-05 09:09:33","","","","","","","Игорь Котенков - RLHF Intro","","","","","","","","","","","","YouTube","","Citation Key: datafestKotenkov2023RLHFIntro","","","","","","","","","","","","","","","","","","","","","","","","","","1:44:11","","","","","","","","","","","","","","","","","","","","","","","","",""
"9IIQMM57","preprint","2023","OpenAI","GPT-4 Technical Report","","","","10.48550/arXiv.2303.08774","http://arxiv.org/abs/2303.08774","We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.","2023-03-27","2023-07-05 11:47:50","2023-07-05 11:47:50","2023-07-05 11:47:50","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2303.08774 [cs]","","/home/lexi/.zotero-data/storage/MFC2YS2P/OpenAI - 2023 - GPT-4 Technical Report.pdf; /home/lexi/.zotero-data/storage/FFF4ZZKI/2303.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2303.08774","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7TBBWLYQ","videoRecording","2023","ML Trainings","Венедиктов Никита - Процесс создания полу синтетического датасета","","","","","https://www.youtube.com/watch?v=8iXmzni4jog","Data Fest 2023: https://ods.ai/events/datafestonline2023 Трек ""Instruct Models"":  https://ods.ai/tracks/df23-instruct-m... Наши соц.сети: Telegram: https://t.me/datafest Вконтакте: https://vk.com/datafest","2023-06-30","2023-07-05 12:33:51","2023-07-05 12:33:51","2023-07-05 12:33:51","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","35:45","","","","","","","","","","","","","","","","","","","","","","","","",""
"RXZNRTX6","videoRecording","2023","ML Trainings","Александр Голубев - Воркшоп по LLM + RLHF","","","","","https://www.youtube.com/watch?v=50x0yDWkHjw","- Для чего нужен RLHF - Примеры работы модели в диалоговом режиме без SFT и RL fine-tuning - Как тюнить большие модели на одной карте - SFT fine-tuning - Обучение RM - Дообучение модели с PPO - Демонстрация результатов было vs стало - Разбор концепций, которые использовали в ходе обучения (LoRA adapters, Int8 quantization, PPO, RM training loss, …) Cсылка на  ноутбук: https://colab.research.google.com/dri... Data Fest 2023: https://ods.ai/events/datafestonline2023 Трек ""Instruct Models"":  https://ods.ai/tracks/df23-instruct-m... Наши соц.сети: Telegram: https://t.me/datafest Вконтакте: https://vk.com/datafest","2023-06-10","2023-07-06 08:09:14","2023-07-10 10:35:40","2023-07-06 08:09:14","","","","","","","","","","","","","","","","","","","YouTube","","Citation Key: datafestGolubev2023Workshop","","","","","","","","","","","","","","","","","","","","","","","","","","55:53","","","","","","","","","","","","","","","","","","","","","","","","",""
"I8D6SQUD","preprint","2021","Hu, Edward J.; Shen, Yelong; Wallis, Phillip; Allen-Zhu, Zeyuan; Li, Yuanzhi; Wang, Shean; Wang, Lu; Chen, Weizhu","LoRA: Low-Rank Adaptation of Large Language Models","","","","10.48550/arXiv.2106.09685","http://arxiv.org/abs/2106.09685","An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.","2021-10-16","2023-07-06 08:29:13","2025-06-12 15:57:05","2023-07-06 08:29:12","","","","","","","LoRA","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2106.09685 [cs]","","/home/lexi/.zotero-data/storage/82MWYM5F/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf; /home/lexi/.zotero-data/storage/W73LFQWP/2106.html","","milestone","","","","","","","","","","","","","","","","","","","","arXiv:2106.09685","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"28Y2WXSP","preprint","2022","Dettmers, Tim; Lewis, Mike; Belkada, Younes; Zettlemoyer, Luke","LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale","","","","10.48550/arXiv.2208.07339","http://arxiv.org/abs/2208.07339","Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.","2022-11-10","2023-07-06 08:29:35","2025-06-12 15:59:47","2023-07-06 08:29:35","","","","","","","LLM.int8()","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2208.07339 [cs]","","/home/lexi/.zotero-data/storage/8877PIEJ/Dettmers et al. - 2022 - LLM.int8() 8-bit Matrix Multiplication for Transf.pdf; /home/lexi/.zotero-data/storage/VIGXLV3C/2208.html","","milestone","","","","","","","","","","","","","","","","","","","","arXiv:2208.07339","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GMH82BIT","webpage","","Karpathy, Andrej","State of GPT","Microsoft Build","","","","https://www.youtube.com/watch?v=bZQun8Y4L2A","Learn about the training pipeline of GPT assistants like ChatGPT, from tokenization to pretraining, supervised finetuning, and Reinforcement Learning from Human Feedback (RLHF). Dive deeper into practical techniques and mental models for the effective use of these models, including prompting strategies, finetuning, the rapidly growing ecosystem of tools, and their future extensions.","","2023-07-10 07:59:24","2024-07-19 19:24:45","2023-07-10 07:59:24","","","","","","","State of GPT","","","","","","","en-US","","","","","","","Citation Key: karpathy2023State","","/home/lexi/.zotero-data/storage/8S8Z9LH4/db3f4859-cd30-4445-a0cd-553c3304f8e2.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J24PHM75","preprint","2023","Fernandes, Patrick; Madaan, Aman; Liu, Emmy; Farinhas, António; Martins, Pedro Henrique; Bertsch, Amanda; de Souza, José G. C.; Zhou, Shuyan; Wu, Tongshuang; Neubig, Graham; Martins, André F. T.","Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation","","","","10.48550/arXiv.2305.00955","http://arxiv.org/abs/2305.00955","Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.","2023-05-31","2023-07-10 08:58:15","2023-07-10 08:58:15","2023-07-10 08:58:15","","","","","","","Bridging the Gap","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2305.00955 [cs]","","/home/lexi/.zotero-data/storage/MXSYK97R/Fernandes et al. - 2023 - Bridging the Gap A Survey on Integrating (Human) .pdf; /home/lexi/.zotero-data/storage/ZIAUA27R/2305.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2305.00955","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QP6Y8X4D","videoRecording","2023","ML Trainings","Николай Зинов - RLHF в Яндексе","","","","","https://www.youtube.com/watch?v=jXGRhjAyAMA","Data Fest 2023: https://ods.ai/events/datafestonline2023 Трек ""Instruct Models"":  https://ods.ai/tracks/df23-instruct-m... Наши соц.сети: Telegram: https://t.me/datafest Вконтакте: https://vk.com/datafest","2023-06-11","2023-07-10 10:38:00","2023-07-10 10:41:30","2023-07-10 10:38:00","","","","","","","","","","","","","","","","","","","YouTube","","Citation Key: datafestZinov2023RLHFYandex","","","","","","","","","","","","","","","","","","","","","","","","","","37:28","","","","","","","","","","","","","","","","","","","","","","","","",""
"QK732DMW","journalArticle","2022","Frantar, Elias; Ashkboos, Saleh; Hoefler, Torsten; Alistarh, Dan","GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers","","","","10.48550/ARXIV.2210.17323","https://arxiv.org/abs/2210.17323","Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq.","2022","2023-09-20 12:41:19","2025-06-12 15:56:33","2023-09-20 12:41:19","","","","","","","GPTQ","","","","","","","","Creative Commons Attribution 4.0 International","","","","DOI.org (Datacite)","","Publisher: arXiv Version Number: 2","","","","milestone","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VFLN4TCJ","journalArticle","2018","Pfeiffer, Michael; Pfeil, Thomas","Deep Learning With Spiking Neurons: Opportunities and Challenges","Frontiers in Neuroscience","","1662-453X","10.3389/fnins.2018.00774","https://www.frontiersin.org/articles/10.3389/fnins.2018.00774","Spiking neural networks (SNNs) are inspired by information processing in biology, where sparse and asynchronous binary signals are communicated and processed in a massively parallel fashion. SNNs on neuromorphic hardware exhibit favorable properties such as low power consumption, fast inference, and event-driven information processing. This makes them interesting candidates for the efficient implementation of deep neural networks, the method of choice for many machine learning tasks. In this review, we address the opportunities that deep spiking networks offer and investigate in detail the challenges associated with training SNNs in a way that makes them competitive with conventional deep learning, but simultaneously allows for efficient mapping to hardware. A wide range of training methods for SNNs is presented, ranging from the conversion of conventional deep networks into SNNs, constrained training before conversion, spiking variants of backpropagation, and biologically motivated variants of STDP. The goal of our review is to define a categorization of SNN training methods, and summarize their advantages and drawbacks. We further discuss relationships between SNNs and binary networks, which are becoming popular for efficient digital hardware implementation. Neuromorphic hardware platforms have great potential to enable deep spiking networks in real-world applications. We compare the suitability of various neuromorphic systems that have been developed over the past years, and investigate potential use cases. Neuromorphic approaches and conventional machine learning should not be considered simply two solutions to the same classes of problems, instead it is possible to identify and exploit their task-specific advantages. Deep SNNs offer great opportunities to work with new types of event-based sensors, exploit temporal codes and local on-chip learning, and we have so far just scratched the surface of realizing these advantages in practical applications.","2018","2023-09-20 13:11:31","2023-09-20 13:12:00","2023-09-20 13:11:31","","","","12","","","Deep Learning With Spiking Neurons","","","","","","","","","","","","Frontiers","","","","/home/lexi/.zotero-data/storage/CC9JCZD3/Pfeiffer and Pfeil - 2018 - Deep Learning With Spiking Neurons Opportunities .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M2FA3TL3","webpage","","","Optimizing your LLM in production","","","","","https://huggingface.co/blog/optimize-llm","We’re on a journey to advance and democratize artificial intelligence through open source and open science.","","2023-09-20 13:13:15","2023-09-20 13:13:15","2023-09-20 13:13:15","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/64EKNJHN/optimize-llm.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J6IENXDE","preprint","2022","Dao, Tri; Fu, Daniel Y.; Ermon, Stefano; Rudra, Atri; Ré, Christopher","FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness","","","","10.48550/arXiv.2205.14135","http://arxiv.org/abs/2205.14135","Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).","2022-06-23","2023-09-20 13:57:54","2025-06-12 15:58:51","2023-09-20 13:57:54","","","","","","","FlashAttention","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2205.14135 [cs]","","/home/lexi/.zotero-data/storage/RKJ72PJ6/Dao et al. - 2022 - FlashAttention Fast and Memory-Efficient Exact At.pdf; /home/lexi/.zotero-data/storage/DKQD83HX/2205.html","","milestone","","","","","","","","","","","","","","","","","","","","arXiv:2205.14135","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KHJ8QNPA","preprint","2023","Lv, Kai; Yang, Yuqing; Liu, Tengxiao; Gao, Qinghui; Guo, Qipeng; Qiu, Xipeng","Full Parameter Fine-tuning for Large Language Models with Limited Resources","","","","10.48550/arXiv.2306.09782","http://arxiv.org/abs/2306.09782","Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory.","2023-06-16","2023-09-20 17:04:50","2025-06-12 16:00:36","2023-09-20 17:04:50","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.09782 [cs]","","/home/lexi/.zotero-data/storage/D5CEEIKT/Lv et al. - 2023 - Full Parameter Fine-tuning for Large Language Mode.pdf; /home/lexi/.zotero-data/storage/G3X7B4Y7/2306.html","","milestone","","","","","","","","","","","","","","","","","","","","arXiv:2306.09782","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RV67YYSW","preprint","2023","Zmitrovich, Dmitry; Abramov, Alexander; Kalmykov, Andrey; Tikhonova, Maria; Taktasheva, Ekaterina; Astafurov, Danil; Baushenko, Mark; Snegirev, Artem; Shavrina, Tatiana; Markov, Sergey; Mikhailov, Vladislav; Fenogenova, Alena","A Family of Pretrained Transformer Language Models for Russian","","","","10.48550/arXiv.2309.10931","http://arxiv.org/abs/2309.10931","Nowadays, Transformer language models (LMs) represent a fundamental component of the NLP research methodologies and applications. However, the development of such models specifically for the Russian language has received little attention. This paper presents a collection of 13 Russian Transformer LMs based on the encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder (ruT5, FRED-T5) models in multiple sizes. Access to these models is readily available via the HuggingFace platform. We provide a report of the model architecture design and pretraining, and the results of evaluating their generalization abilities on Russian natural language understanding and generation datasets and benchmarks. By pretraining and releasing these specialized Transformer LMs, we hope to broaden the scope of the NLP research directions and enable the development of industrial solutions for the Russian language.","2023-09-19","2023-09-21 12:13:42","2023-09-21 12:13:42","2023-09-21 12:13:42","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2309.10931 [cs]","","/home/lexi/.zotero-data/storage/GGML3M9D/Zmitrovich et al. - 2023 - A Family of Pretrained Transformer Language Models.pdf; /home/lexi/.zotero-data/storage/IEZMRXLE/2309.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2309.10931","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E3BM2L2C","preprint","2020","Ziegler, Daniel M.; Stiennon, Nisan; Wu, Jeffrey; Brown, Tom B.; Radford, Alec; Amodei, Dario; Christiano, Paul; Irving, Geoffrey","Fine-Tuning Language Models from Human Preferences","","","","10.48550/arXiv.1909.08593","http://arxiv.org/abs/1909.08593","Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.","2020-01-08","2023-09-22 16:07:46","2023-09-22 16:07:47","2023-09-22 16:07:46","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1909.08593 [cs, stat]","","/home/lexi/.zotero-data/storage/5D6264Z8/Ziegler et al. - 2020 - Fine-Tuning Language Models from Human Preferences.pdf; /home/lexi/.zotero-data/storage/MVJHL6VI/1909.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1909.08593","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PZWTNID9","preprint","2021","He, Pengcheng; Liu, Xiaodong; Gao, Jianfeng; Chen, Weizhu","DeBERTa: Decoding-enhanced BERT with Disentangled Attention","","","","10.48550/arXiv.2006.03654","http://arxiv.org/abs/2006.03654","Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).","2021-10-06","2023-09-22 17:19:36","2023-09-22 17:19:36","2023-09-22 17:19:36","","","","","","","DeBERTa","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2006.03654 [cs]","","/home/lexi/.zotero-data/storage/VRQ9PWDT/He et al. - 2021 - DeBERTa Decoding-enhanced BERT with Disentangled .pdf; /home/lexi/.zotero-data/storage/R3GNUSFE/2006.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2006.03654","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CNG58JM3","webpage","2023","","Зоопарк трансформеров: большой обзор моделей от BERT до Alpaca","Хабр","","","","https://habr.com/ru/companies/just_ai/articles/733110/","Введение В последнее время только ленивый не говорит про революцию ИИ (искусственный интеллект), искусственные нейронные сети позволяют создавать сервисы, которые 10 лет назад показались бы...","2023-05-04","2023-10-11 15:09:15","2023-10-11 15:09:15","2023-10-11 15:09:15","","","","","","","Зоопарк трансформеров","","","","","","","ru","","","","","","","","","/home/lexi/.zotero-data/storage/ZN3M7WEL/733110.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9PU3E45S","webpage","2023","Just AI","Как мы в Just AI создавали и тестировали собственную LLM JustGPT — третью большую языковую модель в России","Хабр","","","","https://habr.com/ru/companies/just_ai/articles/762174/","Хабр, привет! Это Just AI, и мы создали JustGPT – третью большую языковую модель, сопоставимую по качеству ответов с известными LLM для русского языка (GigaChat, YandexGPT). Наша история – про работу над моделью, ее обучение и тестирование. Но в конечном итоге о том, как получить свою LLM на русском языке без космических мощностей и огромных команд.","2023-09-20","2023-10-11 15:09:29","2023-10-11 15:14:22","2023-10-11 15:09:29","","","","","","","","","","","","","","ru","","","","","","","","","/home/lexi/.zotero-data/storage/3TDB2DV6/762174.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3UD5V3BZ","webpage","","","Conditional Pretraining of Large Language Models | LAION","","","","","https://laion.ai/notes/cpretrain","<h2><a id=""introduction"" class=""anchor"" href=""#introduction"" aria-hidden=""true""><svg aria-hidden=""true"" class=""octicon octicon-link"" height=""16"" version=""1.1...","","2023-10-11 17:35:57","2023-10-11 17:35:57","2023-10-11 17:35:57","","","","","","","","","","","","","","en","","","","","","","","","/home/lexi/.zotero-data/storage/PEJBL5Z2/cpretrain.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2Z6HULWE","preprint","2023","Zheng, Lianmin; Chiang, Wei-Lin; Sheng, Ying; Li, Tianle; Zhuang, Siyuan; Wu, Zhanghao; Zhuang, Yonghao; Li, Zhuohan; Lin, Zi; Xing, Eric P.; Gonzalez, Joseph E.; Stoica, Ion; Zhang, Hao","LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset","","","","10.48550/arXiv.2309.11998","http://arxiv.org/abs/2309.11998","Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.","2023-09-29","2023-10-13 17:10:49","2023-10-13 17:10:49","2023-10-13 17:10:49","","","","","","","LMSYS-Chat-1M","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2309.11998 [cs]","","/home/lexi/.zotero-data/storage/S7VX8ULN/Zheng et al. - 2023 - LMSYS-Chat-1M A Large-Scale Real-World LLM Conver.pdf; /home/lexi/.zotero-data/storage/67K2INI6/2309.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2309.11998","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UXVAXBZK","videoRecording","2023","Yandex for ML","«GigaChat: наш опыт обучения LLM»","","","","","https://www.youtube.com/watch?v=lnhCrqyQGeA","«GigaChat: наш опыт обучения LLM» Дани Эль-Айясс, Исполнительный директор, SberDevices Большие языковые модели (LLM) являются новым этапом развития интеллектуальных систем, решающих задачи на естественном языке. Помимо глубокого понимания структуры языка, LLM также формируют знания о мире во время обучения на больших корпусах текста, что выводит их на новый качественный уровень, открывая новые эмерджентные способности, такие как подходы, основанные на нулевом или крайне мало числе примеров (Few-Shot/Zero-Shot Learning/Prompting) и построение рассуждений (Reasoning). Но обучение подобных моделей является сложной задачей, требующей большого количества ресурсов и тонкой настройки. В докладе мы хотим поделится нашим опытом обучения модели ruGPT-3.5 — большой языковой модели от Сбера, лежащей в основе генеративного сервиса GigaChat, начиная с предобучения и заканчивая RLHF-пайплайном.","2023-10-06","2023-11-02 14:04:46","2023-11-02 14:04:46","2023-11-02 14:04:46","","","","","","","«GigaChat","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","41:42","","","","","","","","","","","","","","","","","","","","","","","","",""
"DCV3Z9DT","preprint","2023","Yang, Jingfeng; Jin, Hongye; Tang, Ruixiang; Han, Xiaotian; Feng, Qizhang; Jiang, Haoming; Yin, Bing; Hu, Xia","Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond","","","","10.48550/arXiv.2304.13712","http://arxiv.org/abs/2304.13712","This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at \url{https://github.com/Mooler0410/LLMsPracticalGuide}.","2023-04-27","2023-11-02 14:48:58","2023-11-02 14:48:58","2023-11-02 14:48:58","","","","","","","Harnessing the Power of LLMs in Practice","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2304.13712 [cs]","","/home/lexi/.zotero-data/storage/BPL8YYCA/Yang et al. - 2023 - Harnessing the Power of LLMs in Practice A Survey.pdf; /home/lexi/.zotero-data/storage/RPIN5ISV/2304.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2304.13712","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"27V7VYIT","preprint","2023","Touvron, Hugo; Martin, Louis; Stone, Kevin; Albert, Peter; Almahairi, Amjad; Babaei, Yasmine; Bashlykov, Nikolay; Batra, Soumya; Bhargava, Prajjwal; Bhosale, Shruti; Bikel, Dan; Blecher, Lukas; Ferrer, Cristian Canton; Chen, Moya; Cucurull, Guillem; Esiobu, David; Fernandes, Jude; Fu, Jeremy; Fu, Wenyin; Fuller, Brian; Gao, Cynthia; Goswami, Vedanuj; Goyal, Naman; Hartshorn, Anthony; Hosseini, Saghar; Hou, Rui; Inan, Hakan; Kardas, Marcin; Kerkez, Viktor; Khabsa, Madian; Kloumann, Isabel; Korenev, Artem; Koura, Punit Singh; Lachaux, Marie-Anne; Lavril, Thibaut; Lee, Jenya; Liskovich, Diana; Lu, Yinghai; Mao, Yuning; Martinet, Xavier; Mihaylov, Todor; Mishra, Pushkar; Molybog, Igor; Nie, Yixin; Poulton, Andrew; Reizenstein, Jeremy; Rungta, Rashi; Saladi, Kalyan; Schelten, Alan; Silva, Ruan; Smith, Eric Michael; Subramanian, Ranjan; Tan, Xiaoqing Ellen; Tang, Binh; Taylor, Ross; Williams, Adina; Kuan, Jian Xiang; Xu, Puxin; Yan, Zheng; Zarov, Iliyan; Zhang, Yuchen; Fan, Angela; Kambadur, Melanie; Narang, Sharan; Rodriguez, Aurelien; Stojnic, Robert; Edunov, Sergey; Scialom, Thomas","Llama 2: Open Foundation and Fine-Tuned Chat Models","","","","10.48550/arXiv.2307.09288","http://arxiv.org/abs/2307.09288","In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.","2023-07-19","2023-11-09 10:45:01","2023-11-09 10:46:53","2023-11-09 10:45:01","","","","","","","Llama 2","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2307.09288 [cs]","","/home/lexi/.zotero-data/storage/WKPNC8X8/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf; /home/lexi/.zotero-data/storage/TP5HMSK8/2307.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2307.09288","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DK49Y8NI","preprint","2022","Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Buchatskaya, Elena; Cai, Trevor; Rutherford, Eliza; Casas, Diego de Las; Hendricks, Lisa Anne; Welbl, Johannes; Clark, Aidan; Hennigan, Tom; Noland, Eric; Millican, Katie; Driessche, George van den; Damoc, Bogdan; Guy, Aurelia; Osindero, Simon; Simonyan, Karen; Elsen, Erich; Rae, Jack W.; Vinyals, Oriol; Sifre, Laurent","Training Compute-Optimal Large Language Models","","","","10.48550/arXiv.2203.15556","http://arxiv.org/abs/2203.15556","We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.","2022-03-29","2023-11-09 10:50:06","2023-11-09 10:50:37","2023-11-09 10:50:06","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2203.15556 [cs]","","/home/lexi/.zotero-data/storage/7ASIXYUV/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf; /home/lexi/.zotero-data/storage/2FYQNUEK/2203.html","","soon","","","","","","","","","","","","","","","","","","","","arXiv:2203.15556","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PZFIEYJZ","preprint","2022","Chung, Hyung Won; Hou, Le; Longpre, Shayne; Zoph, Barret; Tay, Yi; Fedus, William; Li, Yunxuan; Wang, Xuezhi; Dehghani, Mostafa; Brahma, Siddhartha; Webson, Albert; Gu, Shixiang Shane; Dai, Zhuyun; Suzgun, Mirac; Chen, Xinyun; Chowdhery, Aakanksha; Castro-Ros, Alex; Pellat, Marie; Robinson, Kevin; Valter, Dasha; Narang, Sharan; Mishra, Gaurav; Yu, Adams; Zhao, Vincent; Huang, Yanping; Dai, Andrew; Yu, Hongkun; Petrov, Slav; Chi, Ed H.; Dean, Jeff; Devlin, Jacob; Roberts, Adam; Zhou, Denny; Le, Quoc V.; Wei, Jason","Scaling Instruction-Finetuned Language Models","","","","10.48550/arXiv.2210.11416","http://arxiv.org/abs/2210.11416","Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.","2022-12-06","2023-11-09 12:35:28","2023-11-09 12:35:50","2023-11-09 12:35:28","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2210.11416 [cs]","","/home/lexi/.zotero-data/storage/UYZY3BT3/Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf; /home/lexi/.zotero-data/storage/9KTPF8RJ/2210.html","","soon","","","","","","","","","","","","","","","","","","","","arXiv:2210.11416","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3ALN479I","videoRecording","2023","Yandex for ML","Генеративные модели и внешняя информация","","","","","https://www.youtube.com/watch?v=dCseMnRr5FU","«Генеративные модели и внешняя информация» Светлана Маргасова, Разработчик отдела NLP группы YandexGPT Alignment, Яндекс Поиск Александр Кайгородов, Разработчик отдела NLP группы YandexGPT Alignment, Яндекс Поиск Современные генеративные модели способы галлюцинировать и путать факты. Одно из решений этих проблем — использовать доступ к внешней информации, опираясь на которую, мы можем выполнять как обусловленную генерацию (Retrieval Augmented Generation), так и фактологическую оценку имеющихся генераций (Fact-Check). В доклад","2023-10-06","2023-11-10 10:17:17","2023-11-10 10:17:57","2023-11-10 10:17:16","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","43:57","","","","","","","","","","","","","","","","","","","","","","","","",""
"K2BYUDGG","webpage","2023","Radovanovic, Matt Bornstein, Rajko","Emerging Architectures for LLM Applications","Andreessen Horowitz","","","","https://a16z.com/emerging-architectures-for-llm-applications/","A reference architecture for the LLM app stack. It shows the most common systems, tools, and design patterns used by AI startups and tech companies.","2023-06-20","2023-11-10 12:43:56","2023-11-10 12:43:57","2023-11-10 12:43:49","","","","","","","","","","","","","","en","","","","","","","","","/home/lexi/.zotero-data/storage/TICNGHNK/emerging-architectures-for-llm-applications.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BW8SIRD8","webpage","","","Prompting Techniques – Nextra","","","","","https://www.promptingguide.ai/techniques","A Comprehensive Overview of Prompt Engineering","","2023-11-10 13:00:58","2023-11-10 13:00:58","2023-11-10 13:00:58","","","","","","","","","","","","","","en","","","","","","","","","/home/lexi/.zotero-data/storage/Q9JMJFE6/techniques.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4WLXSRKG","preprint","2023","Wang, Yaqing; Wu, Jialin; Dabral, Tanmaya; Zhang, Jiageng; Brown, Geoff; Lu, Chun-Ta; Liu, Frederick; Liang, Yi; Pang, Bo; Bendersky, Michael; Soricut, Radu","Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling","","","","10.48550/arXiv.2310.12100","http://arxiv.org/abs/2310.12100","Large language models (LLMs) and vision language models (VLMs) demonstrate excellent performance on a wide range of tasks by scaling up parameter counts from O(10^9) to O(10^{12}) levels and further beyond. These large scales make it impossible to adapt and deploy fully specialized models given a task of interest. Parameter-efficient fine-tuning (PEFT) emerges as a promising direction to tackle the adaptation and serving challenges for such large models. We categorize PEFT techniques into two types: intrusive and non-intrusive. Intrusive PEFT techniques directly change a model's internal architecture. Though more flexible, they introduce significant complexities for training and serving. Non-intrusive PEFT techniques leave the internal architecture unchanged and only adapt model-external parameters, such as embeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT technique that achieves competitive performance compared to SoTA intrusive PEFT (LoRA) and full model fine-tuning (FT) on various tasks. We evaluate using both text-only and multimodal tasks, with experiments that account for both parameter-count scaling and training regime (with and without instruction tuning).","2023-10-18","2023-11-18 09:05:21","2023-11-18 09:05:21","2023-11-18 09:05:21","","","","","","","Non-Intrusive Adaptation","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2310.12100 [cs]","","/home/lexi/.zotero-data/storage/KZJ8LMYD/Wang et al. - 2023 - Non-Intrusive Adaptation Input-Centric Parameter-.pdf; /home/lexi/.zotero-data/storage/YPPJC97H/2310.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2310.12100","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E6VWQ5MN","preprint","2023","Xiao, Guangxuan; Tian, Yuandong; Chen, Beidi; Han, Song; Lewis, Mike","Efficient Streaming Language Models with Attention Sinks","","","","10.48550/arXiv.2309.17453","http://arxiv.org/abs/2309.17453","Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.","2023-09-29","2023-11-18 09:45:45","2023-11-18 09:45:45","2023-11-18 09:45:45","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2309.17453 [cs]","","/home/lexi/.zotero-data/storage/Z5P8TCLA/Xiao et al. - 2023 - Efficient Streaming Language Models with Attention.pdf; /home/lexi/.zotero-data/storage/CAVP83GI/2309.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2309.17453","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GFGPSJCZ","videoRecording","2023","Hyung Won Chung","Large Language Models (in 2023)","","","","","https://www.youtube.com/watch?v=dbo3kNKPaUA","I gave a talk at Seoul National University. I titled the talk “Large Language Models (in 2023)”. This was an ambitious attempt to summarize our exploding field. Trying to summarize the field forced me to think about what really matters in the field. While scaling undeniably stands out, its far-reaching implications are more nuanced. I share my thoughts on scaling from three angles: 1:02 1) Change in perspective is necessary because some abilities only emerge at a certain scale. Even if some abilities don’t work with the current generation LLMs, we should not claim that it doesn’t work. Rather, we should think it doesn’t work yet. Once larger models are available many conclusions change. This also means that some conclusions from the past are invalidated and we need to constantly unlearn intuitions built on top of such ideas. 7:12 2) From first-principles, scaling up the Transformer amounts to efficiently doing matrix multiplications with many, many machines. I see many researchers in the field of LLM who are not familiar with how scaling is actually done. This section is targeted for technical audiences who want to understand what it means to train large models. 27:52 3) I talk about what we should think about for further scaling (think 10000x GPT-4 scale). To me scaling isn’t just doing the same thing with more machines. It entails finding the inductive bias that is the bottleneck in further scaling. I believe that the maximum likelihood objective function is the bottleneck in achieving the scale of 10000x GPT-4 level. Learning the objective function with an expressive neural net is the next paradigm that is a lot more scalable. With the compute cost going down exponentially, scalable methods eventually win. Don’t compete with that. In all of these sections, I strive to describe everything from first-principles. In an extremely fast moving field like LLM, no one can keep up. I believe that understanding the core ideas by deriving from first-principles is the only scalable approach. Disclaimer: I give my personal opinions and the talk material doesn't reflect my employer's opinion in any way.","2023-10-05","2023-11-18 09:46:00","2023-11-18 09:46:00","2023-11-18 09:46:00","","","","","","","","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","49:06","","","","","","","","","","","","","","","","","","","","","","","","",""
"C3YPTAAY","preprint","2023","von Oswald, Johannes; Niklasson, Eyvind; Schlegel, Maximilian; Kobayashi, Seijin; Zucchet, Nicolas; Scherrer, Nino; Miller, Nolan; Sandler, Mark; Arcas, Blaise Agüera y; Vladymyrov, Max; Pascanu, Razvan; Sacramento, João","Uncovering mesa-optimization algorithms in Transformers","","","","10.48550/arXiv.2309.05858","http://arxiv.org/abs/2309.05858","Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention layer, the mesa-layer, that explicitly and efficiently solves optimization problems specified in context. We find that this layer can lead to improved performance in synthetic and preliminary language modeling experiments, adding weight to our hypothesis that mesa-optimization is an important operation hidden within the weights of trained Transformers.","2023-09-11","2024-02-20 08:43:42","2024-02-20 08:44:32","2024-02-20 08:43:25","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2309.05858 [cs]","","/home/lexi/.zotero-data/storage/Q38GL62I/von Oswald et al. - 2023 - Uncovering mesa-optimization algorithms in Transfo.pdf; /home/lexi/.zotero-data/storage/MXSBETKC/2309.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2309.05858","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BGI6T82J","preprint","2023","von Oswald, Johannes; Niklasson, Eyvind; Randazzo, Ettore; Sacramento, João; Mordvintsev, Alexander; Zhmoginov, Andrey; Vladymyrov, Max","Transformers learn in-context by gradient descent","","","","10.48550/arXiv.2212.07677","http://arxiv.org/abs/2212.07677","At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .","2023-05-31","2024-02-20 08:45:07","2024-02-20 08:45:07","2024-02-20 08:45:07","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2212.07677 [cs]","","/home/lexi/.zotero-data/storage/QGSVXYGZ/von Oswald et al. - 2023 - Transformers learn in-context by gradient descent.pdf; /home/lexi/.zotero-data/storage/5WAEEPKQ/2212.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2212.07677","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8A4KYGDV","preprint","2023","Dai, Damai; Sun, Yutao; Dong, Li; Hao, Yaru; Ma, Shuming; Sui, Zhifang; Wei, Furu","Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers","","","","10.48550/arXiv.2212.10559","http://arxiv.org/abs/2212.10559","Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at \url{https://aka.ms/icl}.","2023-05-15","2024-02-20 08:45:36","2024-02-20 08:45:36","2024-02-20 08:45:36","","","","","","","Why Can GPT Learn In-Context?","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2212.10559 [cs]","","/home/lexi/.zotero-data/storage/I233VTBD/Dai et al. - 2023 - Why Can GPT Learn In-Context Language Models Impl.pdf; /home/lexi/.zotero-data/storage/AXH78T8W/2212.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2212.10559","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"359P5UXP","preprint","2023","Gesmundo, Andrea; Maile, Kaitlin","Composable Function-preserving Expansions for Transformer Architectures","","","","10.48550/arXiv.2308.06103","http://arxiv.org/abs/2308.06103","Training state-of-the-art neural networks requires a high cost in terms of compute and time. Model scale is recognized to be a critical factor to achieve and improve the state-of-the-art. Increasing the scale of a neural network normally requires restarting from scratch by randomly initializing all the parameters of the model, as this implies a change of architecture's parameters that does not allow for a straightforward transfer of knowledge from smaller size models. In this work, we propose six composable transformations to incrementally increase the size of transformer-based neural networks while preserving functionality, allowing to expand the capacity of the model as needed. We provide proof of exact function preservation under minimal initialization constraints for each transformation. The proposed methods may enable efficient training pipelines for larger and more powerful models by progressively expanding the architecture throughout training.","2023-08-11","2024-02-20 08:46:01","2024-02-20 08:46:01","2024-02-20 08:46:01","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2308.06103 [cs]","","/home/lexi/.zotero-data/storage/D6UMJIRU/Gesmundo and Maile - 2023 - Composable Function-preserving Expansions for Tran.pdf; /home/lexi/.zotero-data/storage/3D3AENN9/2308.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2308.06103","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N4R9YMD9","preprint","2022","Bai, Yuntao; Kadavath, Saurav; Kundu, Sandipan; Askell, Amanda; Kernion, Jackson; Jones, Andy; Chen, Anna; Goldie, Anna; Mirhoseini, Azalia; McKinnon, Cameron; Chen, Carol; Olsson, Catherine; Olah, Christopher; Hernandez, Danny; Drain, Dawn; Ganguli, Deep; Li, Dustin; Tran-Johnson, Eli; Perez, Ethan; Kerr, Jamie; Mueller, Jared; Ladish, Jeffrey; Landau, Joshua; Ndousse, Kamal; Lukosuite, Kamile; Lovitt, Liane; Sellitto, Michael; Elhage, Nelson; Schiefer, Nicholas; Mercado, Noemi; DasSarma, Nova; Lasenby, Robert; Larson, Robin; Ringer, Sam; Johnston, Scott; Kravec, Shauna; Showk, Sheer El; Fort, Stanislav; Lanham, Tamera; Telleen-Lawton, Timothy; Conerly, Tom; Henighan, Tom; Hume, Tristan; Bowman, Samuel R.; Hatfield-Dodds, Zac; Mann, Ben; Amodei, Dario; Joseph, Nicholas; McCandlish, Sam; Brown, Tom; Kaplan, Jared","Constitutional AI: Harmlessness from AI Feedback","","","","10.48550/arXiv.2212.08073","http://arxiv.org/abs/2212.08073","As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.","2022-12-15","2023-09-22 11:44:43","2024-03-28 14:08:12","2024-02-20 08:56:56","","","","","","","Constitutional AI","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2212.08073 [cs]","","/home/lexi/.zotero-data/storage/D9AJEUB4/Bai et al. - 2022 - Constitutional AI Harmlessness from AI Feedback.pdf; /home/lexi/.zotero-data/storage/2XT7FY3V/2212.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2212.08073","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6UMNNDND","preprint","2023","Lee, Harrison; Phatale, Samrat; Mansoor, Hassan; Mesnard, Thomas; Ferret, Johan; Lu, Kellie; Bishop, Colton; Hall, Ethan; Carbune, Victor; Rastogi, Abhinav; Prakash, Sushant","RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback","","","","10.48550/arXiv.2309.00267","http://arxiv.org/abs/2309.00267","Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences. However, gathering high-quality human preference labels can be a time-consuming and expensive endeavor. RL from AI Feedback (RLAIF), introduced by Bai et al., offers a promising alternative that leverages a powerful off-the-shelf LLM to generate preferences in lieu of human annotators. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, RLAIF achieves comparable or superior performance to RLHF, as rated by human evaluators. Furthermore, RLAIF demonstrates the ability to outperform a supervised fine-tuned baseline even when the LLM preference labeler is the same size as the policy. In another experiment, directly prompting the LLM for reward scores achieves superior performance to the canonical RLAIF setup, where LLM preference labels are first distilled into a reward model. Finally, we conduct extensive studies on techniques for generating aligned AI preferences. Our results suggest that RLAIF can achieve human-level performance, offering a potential solution to the scalability limitations of RLHF.","2023-11-30","2023-09-22 11:44:06","2024-03-28 14:08:56","2024-02-20 08:57:04","","","","","","","RLAIF","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2309.00267 [cs]","","/home/lexi/.zotero-data/storage/FJ8VBD3U/Lee et al. - 2023 - RLAIF Scaling Reinforcement Learning from Human F.pdf; /home/lexi/.zotero-data/storage/NBK5QEEY/Lee et al. - 2023 - RLAIF Scaling Reinforcement Learning from Human F.pdf; /home/lexi/.zotero-data/storage/7FR3SRVU/2309.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2309.00267","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JXI7DNHE","preprint","2024","Chen, Angelica; Phang, Jason; Parrish, Alicia; Padmakumar, Vishakh; Zhao, Chen; Bowman, Samuel R.; Cho, Kyunghyun","Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs","","","","10.48550/arXiv.2305.14279","http://arxiv.org/abs/2305.14279","Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -- hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.","2024-02-02","2024-02-20 09:19:48","2024-02-20 09:19:54","2024-02-20 09:19:48","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2305.14279 [cs]","","/home/lexi/.zotero-data/storage/G9JGTQ8G/Chen et al. - 2024 - Two Failures of Self-Consistency in the Multi-Step.pdf; /home/lexi/.zotero-data/storage/GWBN226Y/2305.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2305.14279","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HCZVSLAW","preprint","2023","Ahmed, Toufique; Devanbu, Premkumar","Better patching using LLM prompting, via Self-Consistency","","","","10.48550/arXiv.2306.00108","http://arxiv.org/abs/2306.00108","Large Language models (LLMs) can be induced to solve non-trivial problems with ""few-shot"" prompts including illustrative problem-solution examples. Now if the few-shots also include ""chain of thought"" (CoT) explanations, which are of the form problem-explanation-solution, LLMs will generate a ""explained"" solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] (S-C) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant S-C (or even CoT) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the S-C approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.","2023-08-16","2024-02-20 09:20:01","2024-02-20 09:20:01","2024-02-20 09:20:01","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.00108 [cs]","","/home/lexi/.zotero-data/storage/DGYQLADC/Ahmed and Devanbu - 2023 - Better patching using LLM prompting, via Self-Cons.pdf; /home/lexi/.zotero-data/storage/F7CEEUD7/2306.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2306.00108","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9HNE7V9Z","preprint","2023","Naveed, Humza; Khan, Asad Ullah; Qiu, Shi; Saqib, Muhammad; Anwar, Saeed; Usman, Muhammad; Akhtar, Naveed; Barnes, Nick; Mian, Ajmal","A Comprehensive Overview of Large Language Models","","","","10.48550/arXiv.2307.06435","http://arxiv.org/abs/2307.06435","Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.","2023-12-27","2024-02-20 09:20:26","2024-02-20 09:20:30","2024-02-20 09:20:26","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2307.06435 [cs]","","/home/lexi/.zotero-data/storage/IN79X8IN/Naveed et al. - 2023 - A Comprehensive Overview of Large Language Models.pdf; /home/lexi/.zotero-data/storage/PPQJ8PV4/2307.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2307.06435","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8NPCIDQB","preprint","2023","Chen, Zhipeng; Zhou, Kun; Zhang, Beichen; Gong, Zheng; Zhao, Wayne Xin; Wen, Ji-Rong","ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models","","","","10.48550/arXiv.2305.14323","http://arxiv.org/abs/2305.14323","Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose ChatCoT, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs (e.g., ChatGPT). In ChatCoT, we model the chain-of-thought (CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting. At each turn, LLMs can either interact with tools or perform the reasoning. Our approach can effectively leverage the multi-turn conversation ability of chat-based LLMs, and integrate the thought chain following and tools manipulation in a unified way. Specially, we initialize the early turns of the conversation by the knowledge about tools, tasks, and reasoning format, and propose an iterative tool-augmented reasoning step to perform step-by-step tool-augmented reasoning. The experiment results on two complex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of ChatCoT on complex reasoning tasks, achieving a 7.9% relative improvement over the state-of-the-art baseline. Our code and data are available at: \url{https://github.com/RUCAIBOX/ChatCoT}.","2023-11-06","2024-02-20 09:21:16","2024-02-20 09:21:16","2024-02-20 09:21:16","","","","","","","ChatCoT","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2305.14323 [cs]","","/home/lexi/.zotero-data/storage/JTHZSUW9/Chen et al. - 2023 - ChatCoT Tool-Augmented Chain-of-Thought Reasoning.pdf; /home/lexi/.zotero-data/storage/3G7GHDEG/2305.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2305.14323","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XBZCTT9C","preprint","2023","Zhu, Wenhao; Lv, Yunzhe; Dong, Qingxiu; Yuan, Fei; Xu, Jingjing; Huang, Shujian; Kong, Lingpeng; Chen, Jiajun; Li, Lei","Extrapolating Large Language Models to Non-English by Aligning Languages","","","","10.48550/arXiv.2308.04948","http://arxiv.org/abs/2308.04948","Existing large language models show disparate capability across different languages, due to the imbalance in the training data. Their performances on English tasks are often stronger than on tasks of other languages. In this paper, we empower pre-trained LLMs on non-English languages by building semantic alignment across languages. We start from targeting individual languages by performing cross-lingual instruction-tuning (CoIT) on LLaMA, i.e. tuning it with translation task data and cross-lingual general task data to obtain cross-lingual models (x-LLaMAs), and formulate underlying scaling laws to investigate the advantages of using scalable translation data. Then we perform multilingual instruction-tuning (MuIT) with mixed resources to build multilingual m-LLaMA. We also illustrate how we leverage the scaling laws to optimize data allocation in a resource-constrained setting. Experiment results on cross-lingual benchmarks XQUAD and MLQA show that x-LLaMAs surpass the English instruction-tuned counterpart (Alpaca) by an average of 27.83% across six non-English languages. Evaluation results on translation dataset Flores-101 show that x-LLaMAs outperform previous LLaMA-based models by an average of 18.89%. Encouragingly, m-LLaMA achieves comparable performance to x-LLaMAs on individual languages and demonstrates the ability to follow multilingual instructions. Further analysis on response content and representation space reveals the alignment of the multilingual semantic space within the middle layers of m-LLaMA.","2023-10-09","2024-02-20 09:21:46","2024-02-20 09:21:47","2024-02-20 09:21:46","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2308.04948 [cs]","","/home/lexi/.zotero-data/storage/ZNYGME47/Zhu et al. - 2023 - Extrapolating Large Language Models to Non-English.pdf; /home/lexi/.zotero-data/storage/XLKA43UW/2308.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2308.04948","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IDYVDRR4","preprint","2023","Razzhigaev, Anton; Mikhalchuk, Matvey; Goncharova, Elizaveta; Oseledets, Ivan; Dimitrov, Denis; Kuznetsov, Andrey","The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models","","","","10.48550/arXiv.2311.05928","http://arxiv.org/abs/2311.05928","In this study, we present an investigation into the anisotropy dynamics and intrinsic dimension of embeddings in transformer architectures, focusing on the dichotomy between encoders and decoders. Our findings reveal that the anisotropy profile in transformer decoders exhibits a distinct bell-shaped curve, with the highest anisotropy concentrations in the middle layers. This pattern diverges from the more uniformly distributed anisotropy observed in encoders. In addition, we found that the intrinsic dimension of embeddings increases in the initial phases of training, indicating an expansion into higher-dimensional space. Which is then followed by a compression phase towards the end of training with dimensionality decrease, suggesting a refinement into more compact representations. Our results provide fresh insights to the understanding of encoders and decoders embedding properties.","2023-11-10","2024-02-20 09:28:32","2024-02-20 09:28:32","2024-02-20 09:28:32","","","","","","","The Shape of Learning","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2311.05928 [cs, math]","","/home/lexi/.zotero-data/storage/BZZBJV4H/Razzhigaev et al. - 2023 - The Shape of Learning Anisotropy and Intrinsic Di.pdf; /home/lexi/.zotero-data/storage/8NHW53G3/2311.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2311.05928","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MFPNFBHR","preprint","2022","Hazarika, Devamanyu; Namazifar, Mahdi; Hakkani-Tür, Dilek","Zero-Shot Controlled Generation with Encoder-Decoder Transformers","","","","10.48550/arXiv.2106.06411","http://arxiv.org/abs/2106.06411","Controlling neural network-based models for natural language generation (NLG) has broad applications in numerous areas such as machine translation, document summarization, and dialog systems. Approaches that enable such control in a zero-shot manner would be of great importance as, among other reasons, they remove the need for additional annotated data and training. In this work, we propose novel approaches for controlling encoder-decoder transformer-based NLG models in zero-shot. This is done by introducing three control knobs, namely, attention biasing, decoder mixing, and context augmentation, that are applied to these models at generation time. These knobs control the generation process by directly manipulating trained NLG models (e.g., biasing cross-attention layers) to realize the desired attributes in the generated outputs. We show that not only are these NLG models robust to such manipulations, but also their behavior could be controlled without an impact on their generation performance. These results, to the best of our knowledge, are the first of their kind. Through these control knobs, we also investigate the role of transformer decoder's self-attention module and show strong evidence that its primary role is maintaining fluency of sentences generated by these models. Based on this hypothesis, we show that alternative architectures for transformer decoders could be viable options. We also study how this hypothesis could lead to more efficient ways for training encoder-decoder transformer models.","2022-04-06","2024-02-20 09:31:15","2024-02-20 09:31:15","2024-02-20 09:31:15","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2106.06411 [cs]","","/home/lexi/.zotero-data/storage/MK8LNQ9U/Hazarika et al. - 2022 - Zero-Shot Controlled Generation with Encoder-Decod.pdf; /home/lexi/.zotero-data/storage/CYNI8GFI/2106.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2106.06411","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UY337F8U","webpage","","","What Makes a Dialog Agent Useful?","","","","","https://huggingface.co/blog/dialog-agents","We’re on a journey to advance and democratize artificial intelligence through open source and open science.","","2024-02-20 09:33:18","2024-02-20 09:33:18","2024-02-20 09:33:18","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/LLCT5QR9/dialog-agents.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G9ABWSRP","computerProgram","2024","Mooler0410","Mooler0410/LLMsPracticalGuide","","","","","https://github.com/Mooler0410/LLMsPracticalGuide","A curated list of practical guide resources of LLMs (LLMs Tree, Examples, Papers)","2024-02-20","2024-02-20 09:35:57","2024-02-20 09:35:59","2024-02-20 09:35:57","","","","","","","","","","","","","","","","","","","GitHub","","original-date: 2023-04-23T04:22:20Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6TVPDUBW","preprint","2023","Bubeck, Sébastien; Chandrasekaran, Varun; Eldan, Ronen; Gehrke, Johannes; Horvitz, Eric; Kamar, Ece; Lee, Peter; Lee, Yin Tat; Li, Yuanzhi; Lundberg, Scott; Nori, Harsha; Palangi, Hamid; Ribeiro, Marco Tulio; Zhang, Yi","Sparks of Artificial General Intelligence: Early experiments with GPT-4","","","","10.48550/arXiv.2303.12712","http://arxiv.org/abs/2303.12712","Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.","2023-04-13","2024-02-20 09:36:31","2024-02-20 09:36:32","2024-02-20 09:36:31","","","","","","","Sparks of Artificial General Intelligence","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2303.12712 [cs]","","/home/lexi/.zotero-data/storage/J227ZVBR/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf; /home/lexi/.zotero-data/storage/FRSNDPAW/2303.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2303.12712","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WKKDFPII","preprint","2021","Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal, Naman; Küttler, Heinrich; Lewis, Mike; Yih, Wen-tau; Rocktäschel, Tim; Riedel, Sebastian; Kiela, Douwe","Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks","","","","10.48550/arXiv.2005.11401","http://arxiv.org/abs/2005.11401","Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.","2021-04-12","2024-02-20 09:54:48","2024-02-20 09:54:50","2024-02-20 09:54:48","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2005.11401 [cs] version: 4","","/home/lexi/.zotero-data/storage/LAIKEH36/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Inten.pdf; /home/lexi/.zotero-data/storage/2U9NTFB3/2005.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2005.11401","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9FFLP82G","preprint","2022","Gao, Luyu; Ma, Xueguang; Lin, Jimmy; Callan, Jamie","Precise Zero-Shot Dense Retrieval without Relevance Labels","","","","10.48550/arXiv.2212.10496","http://arxiv.org/abs/2212.10496","While dense retrieval has been shown effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance label is available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is unreal and may contain false details. Then, an unsupervised contrastively learned encoder~(e.g. Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity. This second step ground the generated document to the actual corpus, with the encoder's dense bottleneck filtering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers, across various tasks (e.g. web search, QA, fact verification) and languages~(e.g. sw, ko, ja).","2022-12-20","2023-11-19 20:32:55","2024-03-28 14:08:52","2024-02-20 09:56:44","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2212.10496 [cs]","","/home/lexi/.zotero-data/storage/3SCE3HRK/Gao et al. - 2022 - Precise Zero-Shot Dense Retrieval without Relevanc.pdf; /home/lexi/.zotero-data/storage/R6THVGCX/2212.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2212.10496","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QI8QPFYW","preprint","2023","Liu, Jiongnan; Jin, Jiajie; Wang, Zihan; Cheng, Jiehan; Dou, Zhicheng; Wen, Ji-Rong","RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit","","","","10.48550/arXiv.2306.05212","http://arxiv.org/abs/2306.05212","Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrieval-augmented LLMs). Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition, by incorporating external knowledge, retrieval-augmented LLMs can answer in-domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval-augmented LLM systems, we develop RETA-LLM, a {RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems. Compared with previous retrieval-augmented LLM systems, RETA-LLM provides more plug-and-play modules to support better interaction between IR systems and LLMs, including {request rewriting, document retrieval, passage extraction, answer generation, and fact checking} modules. Our toolkit is publicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.","2023-06-08","2024-02-20 10:00:32","2024-02-20 10:00:32","2024-02-20 10:00:32","","","","","","","RETA-LLM","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.05212 [cs]","","/home/lexi/.zotero-data/storage/AP7Q83BZ/Liu et al. - 2023 - RETA-LLM A Retrieval-Augmented Large Language Mod.pdf; /home/lexi/.zotero-data/storage/Q4KFMLNJ/2306.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2306.05212","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RH7XD9NC","webpage","2022","","Самая большая BERT-подобная модель на русском, которая поместится на ваш компьютер","Хабр","","","","https://habr.com/ru/companies/yandex/articles/688234/","Привет, это снова Максим Рябинин, исследователь в Yandex Research. В прошлом году я рассказывал на Хабре о том, как вместе с Hugging Face, Университетом Торонто и волонтёрами мы обучили...","2022-09-15","2024-02-20 10:15:47","2024-02-20 10:15:51","2024-02-20 10:15:47","","","","","","","","","","","","","","ru","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6DF8TVM3","videoRecording","2023","VLP Tutorial 2023","[CVPR2023 Tutorial Talk] Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4","","","","","https://www.youtube.com/watch?v=mkI7EPD1vp8","CVPR 2023 Tutorial on ""Recent Advances in Vision Foundation Models"" - Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4 - By Chunyuan Li (Microsoft)","2023-06-20","2024-02-20 10:27:14","2024-02-20 10:27:14","2024-02-20 10:27:14","","","","","","","[CVPR2023 Tutorial Talk] Large Multimodal Models","","","","","","","","","","","","YouTube","","","","","","","","","","","","","","","","","","","","","","","","","","","","42:41","","","","","","","","","","","","","","","","","","","","","","","","",""
"7Z8UWNF5","preprint","2023","Schick, Timo; Dwivedi-Yu, Jane; Dessì, Roberto; Raileanu, Roberta; Lomeli, Maria; Zettlemoyer, Luke; Cancedda, Nicola; Scialom, Thomas","Toolformer: Language Models Can Teach Themselves to Use Tools","","","","10.48550/arXiv.2302.04761","http://arxiv.org/abs/2302.04761","Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.","2023-02-09","2024-02-20 10:28:59","2024-02-20 10:28:59","2024-02-20 10:28:59","","","","","","","Toolformer","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2302.04761 [cs]","","/home/lexi/.zotero-data/storage/KC3I4Q9U/Schick et al. - 2023 - Toolformer Language Models Can Teach Themselves t.pdf; /home/lexi/.zotero-data/storage/EJBGN63V/2302.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2302.04761","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WAZRKFXJ","preprint","2024","Gu, Yuxian; Dong, Li; Wei, Furu; Huang, Minlie","MiniLLM: Knowledge Distillation of Large Language Models","","","","10.48550/arXiv.2306.08543","http://arxiv.org/abs/2306.08543","Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named MiniLLM. Extensive experiments in the instruction-following setting show that MiniLLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model families with 120M to 13B parameters. Our code, data, and model checkpoints can be found in \url{https://github.com/microsoft/LMOps/tree/main/minillm}.","2024-02-28","2024-03-01 08:30:51","2024-03-01 08:31:19","2024-03-01 08:30:51","","","","","","","MiniLLM","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.08543 [cs]","","/home/lexi/.zotero-data/storage/WC29FGCT/Gu et al. - 2024 - MiniLLM Knowledge Distillation of Large Language .pdf; /home/lexi/.zotero-data/storage/PYI8ZCES/2306.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2306.08543","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RXIAHLBC","preprint","2023","Hsieh, Cheng-Yu; Li, Chun-Liang; Yeh, Chih-Kuan; Nakhost, Hootan; Fujii, Yasuhisa; Ratner, Alexander; Krishna, Ranjay; Lee, Chen-Yu; Pfister, Tomas","Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes","","","","10.48550/arXiv.2305.02301","http://arxiv.org/abs/2305.02301","Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100% of the dataset. We release the code at: https://github.com/google-research/distilling-step-by-step .","2023-07-05","2024-03-01 08:32:28","2024-03-01 08:32:28","2024-03-01 08:32:28","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2305.02301 [cs]","","/home/lexi/.zotero-data/storage/ZNTDES5Z/Hsieh et al. - 2023 - Distilling Step-by-Step! Outperforming Larger Lang.pdf; /home/lexi/.zotero-data/storage/3QWQH3A5/2305.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2305.02301","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4WS6U2M5","preprint","2020","Sanh, Victor; Debut, Lysandre; Chaumond, Julien; Wolf, Thomas","DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","","","","10.48550/arXiv.1910.01108","http://arxiv.org/abs/1910.01108","As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.","2020-02-29","2024-03-01 08:40:27","2024-03-01 08:40:27","2024-03-01 08:40:27","","","","","","","DistilBERT, a distilled version of BERT","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1910.01108 [cs]","","/home/lexi/.zotero-data/storage/PN8Y5UVT/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf; /home/lexi/.zotero-data/storage/42DPDW3R/1910.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1910.01108","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q6VBMGSH","preprint","2020","Wang, Wenhui; Wei, Furu; Dong, Li; Bao, Hangbo; Yang, Nan; Zhou, Ming","MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers","","","","10.48550/arXiv.2002.10957","http://arxiv.org/abs/2002.10957","Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.","2020-04-05","2024-03-01 08:42:59","2024-03-01 08:42:59","2024-03-01 08:42:59","","","","","","","MiniLM","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2002.10957 [cs]","","/home/lexi/.zotero-data/storage/WXSZZ8SF/Wang et al. - 2020 - MiniLM Deep Self-Attention Distillation for Task-.pdf; /home/lexi/.zotero-data/storage/PPWM269A/2002.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2002.10957","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YL9DPMFU","preprint","2023","Trivedi, Aashka; Udagawa, Takuma; Merler, Michele; Panda, Rameswar; El-Kurdi, Yousef; Bhattacharjee, Bishwaranjan","Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models","","","","10.48550/arXiv.2303.09639","http://arxiv.org/abs/2303.09639","Large pretrained language models have achieved state-of-the-art results on a variety of downstream tasks. Knowledge Distillation (KD) into a smaller student model addresses their inefficiency, allowing for deployment in resource-constrained environments. However, KD can be ineffective when the student is manually selected from a set of existing options, since it can be a sub-optimal choice within the space of all possible student architectures. We develop multilingual KD-NAS, the use of Neural Architecture Search (NAS) guided by KD to find the optimal student architecture for task agnostic distillation from a multilingual teacher. In each episode of the search process, a NAS controller predicts a reward based on the distillation loss and latency of inference. The top candidate architectures are then distilled from the teacher on a small proxy set. Finally the architecture(s) with the highest reward is selected, and distilled on the full training corpus. KD-NAS can automatically trade off efficiency and effectiveness, and recommends architectures suitable to various latency budgets. Using our multi-layer hidden state distillation process, our KD-NAS student model achieves a 7x speedup on CPU inference (2x on GPU) compared to a XLM-Roberta Base Teacher, while maintaining 90% performance, and has been deployed in 3 software offerings requiring large throughput, low latency and deployment on CPU.","2023-10-13","2024-03-01 09:19:48","2024-03-01 09:19:48","2024-03-01 09:19:48","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2303.09639 [cs]","","/home/lexi/.zotero-data/storage/9AM6EJ9W/Trivedi et al. - 2023 - Neural Architecture Search for Effective Teacher-S.pdf; /home/lexi/.zotero-data/storage/JFRF4ZWG/2303.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2303.09639","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8SCHYXN3","preprint","2023","Udagawa, Takuma; Trivedi, Aashka; Merler, Michele; Bhattacharjee, Bishwaranjan","A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models","","","","10.48550/arXiv.2310.08797","http://arxiv.org/abs/2310.08797","Large language models have become a vital component in modern NLP, achieving state of the art performance in a variety of tasks. However, they are often inefficient for real-world deployment due to their expensive inference costs. Knowledge distillation is a promising technique to improve their efficiency while retaining most of their effectiveness. In this paper, we reproduce, compare and analyze several representative methods for task-agnostic (general-purpose) distillation of Transformer language models. Our target of study includes Output Distribution (OD) transfer, Hidden State (HS) transfer with various layer mapping strategies, and Multi-Head Attention (MHA) transfer based on MiniLMv2. Through our extensive experiments, we study the effectiveness of each method for various student architectures in both monolingual (English) and multilingual settings. Overall, we show that MHA transfer based on MiniLMv2 is generally the best option for distillation and explain the potential reasons behind its success. Moreover, we show that HS transfer remains as a competitive baseline, especially under a sophisticated layer mapping strategy, while OD transfer consistently lags behind other approaches. Findings from this study helped us deploy efficient yet effective student models for latency-critical applications.","2023-10-12","2024-03-01 09:38:02","2024-03-01 09:38:02","2024-03-01 09:38:02","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2310.08797 [cs]","","/home/lexi/.zotero-data/storage/BULX6PNS/Udagawa et al. - 2023 - A Comparative Analysis of Task-Agnostic Distillati.pdf; /home/lexi/.zotero-data/storage/HXST55VL/2310.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2310.08797","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BHAI5AJ2","preprint","2024","Tang, Yehui; Wang, Yunhe; Guo, Jianyuan; Tu, Zhijun; Han, Kai; Hu, Hailin; Tao, Dacheng","A Survey on Transformer Compression","","","","10.48550/arXiv.2402.05964","http://arxiv.org/abs/2402.05964","Large models based on the Transformer architecture play increasingly vital roles in artificial intelligence, particularly within the realms of natural language processing (NLP) and computer vision (CV). Model compression methods reduce their memory and computational cost, which is a necessary step to implement the transformer models on practical devices. Given the unique architecture of transformer, featuring alternative attention and Feedforward Neural Network (FFN) modules, specific compression techniques are required. The efficiency of these compression methods is also paramount, as it is usually impractical to retrain large models on the entire training dataset.This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to transformer models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design. In each category, we discuss compression methods for both CV and NLP tasks, highlighting common underlying principles. At last, we delve into the relation between various compression methods, and discuss the further directions in this domain.","2024-02-05","2024-03-01 09:38:13","2024-03-01 09:38:13","2024-03-01 09:38:13","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2402.05964 [cs]","","/home/lexi/.zotero-data/storage/365FZTK7/Tang et al. - 2024 - A Survey on Transformer Compression.pdf; /home/lexi/.zotero-data/storage/ZJABJKQU/2402.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2402.05964","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7PP22D5D","preprint","2023","Zhang, Li Lyna; Wang, Xudong; Xu, Jiahang; Zhang, Quanlu; Wang, Yujing; Yang, Yuqing; Zheng, Ningxin; Cao, Ting; Yang, Mao","SpaceEvo: Hardware-Friendly Search Space Design for Efficient INT8 Inference","","","","10.48550/arXiv.2303.08308","http://arxiv.org/abs/2303.08308","The combination of Neural Architecture Search (NAS) and quantization has proven successful in automatically designing low-FLOPs INT8 quantized neural networks (QNN). However, directly applying NAS to design accurate QNN models that achieve low latency on real-world devices leads to inferior performance. In this work, we find that the poor INT8 latency is due to the quantization-unfriendly issue: the operator and configuration (e.g., channel width) choices in prior art search spaces lead to diverse quantization efficiency and can slow down the INT8 inference speed. To address this challenge, we propose SpaceEvo, an automatic method for designing a dedicated, quantization-friendly search space for each target hardware. The key idea of SpaceEvo is to automatically search hardware-preferred operators and configurations to construct the search space, guided by a metric called Q-T score to quantify how quantization-friendly a candidate search space is. We further train a quantized-for-all supernet over our discovered search space, enabling the searched models to be directly deployed without extra retraining or quantization. Our discovered models establish new SOTA INT8 quantized accuracy under various latency constraints, achieving up to 10.1% accuracy improvement on ImageNet than prior art CNNs under the same latency. Extensive experiments on diverse edge devices demonstrate that SpaceEvo consistently outperforms existing manually-designed search spaces with up to 2.5x faster speed while achieving the same accuracy.","2023-03-14","2024-03-01 10:14:55","2024-03-01 10:14:55","2024-03-01 10:14:55","","","","","","","SpaceEvo","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2303.08308 [cs]","","/home/lexi/.zotero-data/storage/XTHK35PF/Zhang et al. - 2023 - SpaceEvo Hardware-Friendly Search Space Design fo.pdf; /home/lexi/.zotero-data/storage/3F8CUYY6/2303.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2303.08308","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WXK5MIZB","preprint","2023","Mecharbat, Lotfi Abdelkrim; Benmeziane, Hadjer; Ouarnoughi, Hamza; Niar, Smail","HyT-NAS: Hybrid Transformers Neural Architecture Search for Edge Devices","","","","10.48550/arXiv.2303.04440","http://arxiv.org/abs/2303.04440","Vision Transformers have enabled recent attention-based Deep Learning (DL) architectures to achieve remarkable results in Computer Vision (CV) tasks. However, due to the extensive computational resources required, these architectures are rarely implemented on resource-constrained platforms. Current research investigates hybrid handcrafted convolution-based and attention-based models for CV tasks such as image classification and object detection. In this paper, we propose HyT-NAS, an efficient Hardware-aware Neural Architecture Search (HW-NAS) including hybrid architectures targeting vision tasks on tiny devices. HyT-NAS improves state-of-the-art HW-NAS by enriching the search space and enhancing the search strategy as well as the performance predictors. Our experiments show that HyT-NAS achieves a similar hypervolume with less than ~5x training evaluations. Our resulting architecture outperforms MLPerf MobileNetV1 by 6.3% accuracy improvement with 3.5x less number of parameters on Visual Wake Words.","2023-03-28","2024-03-01 10:21:01","2024-03-01 10:21:01","2024-03-01 10:21:01","","","","","","","HyT-NAS","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2303.04440 [cs]","","/home/lexi/.zotero-data/storage/VMMIV7PZ/Mecharbat et al. - 2023 - HyT-NAS Hybrid Transformers Neural Architecture S.pdf; /home/lexi/.zotero-data/storage/RX4L2Q44/2303.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2303.04440","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PPUBRSBL","preprint","2023","Zhao, Kaiqi; Zhao, Ming","Poster: Self-Supervised Quantization-Aware Knowledge Distillation","","","","","http://arxiv.org/abs/2309.13220","Quantization-aware training (QAT) achieves competitive performance and is widely used for image classification tasks in model compression. Existing QAT works start with a pre-trained full-precision model and perform quantization during retraining. However, these works require supervision from the ground-truth labels whereas sufficient labeled data are infeasible in real-world environments. Also, they suffer from accuracy loss due to reduced precision, and no algorithm consistently achieves the best or the worst performance on every model architecture. To address the aforementioned limitations, this paper proposes a novel Self-Supervised Quantization-Aware Knowledge Distillation framework (SQAKD). SQAKD unifies the forward and backward dynamics of various quantization functions, making it flexible for incorporating the various QAT works. With the full-precision model as the teacher and the low-bit model as the student, SQAKD reframes QAT as a cooptimization problem that simultaneously minimizes the KL-Loss (i.e., the Kullback-Leibler divergence loss between the teacher’s and student’s penultimate outputs) and the discretization error (i.e., the difference between the full-precision weights/activations and their quantized counterparts). This optimization is achieved in a self-supervised manner without labeled data. The evaluation shows that SQAKD significantly improves the performance of various state-of-the-art QAT works (e.g., PACT, LSQ, DoReFa, and EWGS). SQAKD establishes stronger baselines and does not require extensive labeled training data, potentially making stateof-the-art QAT research more accessible.","2023-09-22","2024-03-01 10:23:27","2024-03-01 10:23:27","2024-03-01 10:23:27","","","","","","","Poster","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2309.13220 [cs]","","/home/lexi/.zotero-data/storage/VDFJH22P/Zhao and Zhao - 2023 - Poster Self-Supervised Quantization-Aware Knowled.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2309.13220","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T32VXIGM","preprint","2019","Kim, Jangho; Bhalgat, Yash; Lee, Jinwon; Patel, Chirag; Kwak, Nojun","QKD: Quantization-aware Knowledge Distillation","","","","10.48550/arXiv.1911.12491","http://arxiv.org/abs/1911.12491","Quantization and Knowledge distillation (KD) methods are widely used to reduce memory and power consumption of deep neural networks (DNNs), especially for resource-constrained edge devices. Although their combination is quite promising to meet these requirements, it may not work as desired. It is mainly because the regularization effect of KD further diminishes the already reduced representation power of a quantized model. To address this short-coming, we propose Quantization-aware Knowledge Distillation (QKD) wherein quantization and KD are care-fully coordinated in three phases. First, Self-studying (SS) phase fine-tunes a quantized low-precision student network without KD to obtain a good initialization. Second, Co-studying (CS) phase tries to train a teacher to make it more quantizaion-friendly and powerful than a fixed teacher. Finally, Tutoring (TU) phase transfers knowledge from the trained teacher to the student. We extensively evaluate our method on ImageNet and CIFAR-10/100 datasets and show an ablation study on networks with both standard and depthwise-separable convolutions. The proposed QKD outperformed existing state-of-the-art methods (e.g., 1.3% improvement on ResNet-18 with W4A4, 2.6% on MobileNetV2 with W4A4). Additionally, QKD could recover the full-precision accuracy at as low as W3A3 quantization on ResNet and W6A6 quantization on MobilenetV2.","2019-11-27","2024-03-01 10:29:56","2024-03-01 10:29:56","2024-03-01 10:29:56","","","","","","","QKD","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1911.12491 [cs]","","/home/lexi/.zotero-data/storage/HS7VHNAU/Kim et al. - 2019 - QKD Quantization-aware Knowledge Distillation.pdf; /home/lexi/.zotero-data/storage/W2CEXYDY/1911.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1911.12491","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B6VMI5KJ","preprint","2016","Han, Song; Mao, Huizi; Dally, William J.","Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding","","","","10.48550/arXiv.1510.00149","http://arxiv.org/abs/1510.00149","Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce ""deep compression"", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.","2016-02-15","2024-03-01 10:45:22","2024-03-01 10:45:22","2024-03-01 10:45:22","","","","","","","Deep Compression","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1510.00149 [cs]","","/home/lexi/.zotero-data/storage/H7INLP5G/Han et al. - 2016 - Deep Compression Compressing Deep Neural Networks.pdf; /home/lexi/.zotero-data/storage/G93T44MM/1510.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1510.00149","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TTZQIYJF","preprint","2021","Wang, Wenhui; Bao, Hangbo; Huang, Shaohan; Dong, Li; Wei, Furu","MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers","","","","10.48550/arXiv.2012.15828","http://arxiv.org/abs/2012.15828","We generalize deep self-attention distillation in MiniLM (Wang et al., 2020) by only using self-attention relation distillation for task-agnostic compression of pretrained Transformers. In particular, we define multi-head self-attention relations as scaled dot-product between the pairs of query, key, and value vectors within each self-attention module. Then we employ the above relational knowledge to train the student model. Besides its simplicity and unified principle, more favorably, there is no restriction in terms of the number of student's attention heads, while most previous work has to guarantee the same head number between teacher and student. Moreover, the fine-grained self-attention relations tend to fully exploit the interaction knowledge learned by Transformer. In addition, we thoroughly examine the layer selection strategy for teacher models, rather than just relying on the last layer as in MiniLM. We conduct extensive experiments on compressing both monolingual and multilingual pretrained models. Experimental results demonstrate that our models distilled from base-size and large-size teachers (BERT, RoBERTa and XLM-R) outperform the state-of-the-art.","2021-06-27","2024-03-01 10:45:42","2024-03-01 10:45:42","2024-03-01 10:45:42","","","","","","","MiniLMv2","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2012.15828 [cs]","","/home/lexi/.zotero-data/storage/EAJVS7W7/Wang et al. - 2021 - MiniLMv2 Multi-Head Self-Attention Relation Disti.pdf; /home/lexi/.zotero-data/storage/BMU3NA5A/2012.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2012.15828","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8WDVD4UQ","preprint","2020","Kim, Young Jin; Awadalla, Hany Hassan","FastFormers: Highly Efficient Transformer Models for Natural Language Understanding","","","","10.48550/arXiv.2010.13382","http://arxiv.org/abs/2010.13382","Transformer-based models are the state-of-the-art for Natural Language Understanding (NLU) applications. Models are getting bigger and better on various tasks. However, Transformer models remain computationally challenging since they are not efficient at inference-time compared to traditional approaches. In this paper, we present FastFormers, a set of recipes to achieve efficient inference-time performance for Transformer-based models on various NLU tasks. We show how carefully utilizing knowledge distillation, structured pruning and numerical optimization can lead to drastic improvements on inference efficiency. We provide effective recipes that can guide practitioners to choose the best settings for various NLU tasks and pretrained models. Applying the proposed recipes to the SuperGLUE benchmark, we achieve from 9.8x up to 233.9x speed-up compared to out-of-the-box models on CPU. On GPU, we also achieve up to 12.4x speed-up with the presented methods. We show that FastFormers can drastically reduce cost of serving 100 million requests from 4,223 USD to just 18 USD on an Azure F16s_v2 instance. This translates to a sustainable runtime by reducing energy consumption 6.9x - 125.8x according to the metrics used in the SustaiNLP 2020 shared task.","2020-10-26","2024-03-01 10:46:41","2024-03-01 10:46:41","2024-03-01 10:46:41","","","","","","","FastFormers","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2010.13382 [cs]","","/home/lexi/.zotero-data/storage/HQ9Q2DF9/Kim and Awadalla - 2020 - FastFormers Highly Efficient Transformer Models f.pdf; /home/lexi/.zotero-data/storage/PBJVASQD/2010.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2010.13382","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y2DZU2LK","preprint","2021","Ji, Mingi; Heo, Byeongho; Park, Sungrae","Show, Attend and Distill:Knowledge Distillation via Attention-based Feature Matching","","","","10.48550/arXiv.2102.02973","http://arxiv.org/abs/2102.02973","Knowledge distillation extracts general knowledge from a pre-trained teacher network and provides guidance to a target student network. Most studies manually tie intermediate features of the teacher and student, and transfer knowledge through pre-defined links. However, manual selection often constructs ineffective links that limit the improvement from the distillation. There has been an attempt to address the problem, but it is still challenging to identify effective links under practical scenarios. In this paper, we introduce an effective and efficient feature distillation method utilizing all the feature levels of the teacher without manually selecting the links. Specifically, our method utilizes an attention-based meta-network that learns relative similarities between features, and applies identified similarities to control distillation intensities of all possible pairs. As a result, our method determines competent links more efficiently than the previous approach and provides better performance on model compression and transfer learning tasks. Further qualitative analyses and ablative studies describe how our method contributes to better distillation. The implementation code is available at github.com/clovaai/attention-feature-distillation.","2021-02-04","2024-03-01 10:55:51","2024-03-01 10:55:51","2024-03-01 10:55:51","","","","","","","Show, Attend and Distill","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2102.02973 [cs]","","/home/lexi/.zotero-data/storage/P9VNBAYE/Ji et al. - 2021 - Show, Attend and DistillKnowledge Distillation vi.pdf; /home/lexi/.zotero-data/storage/ALD6IPZG/2102.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2102.02973","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"683UB5NK","preprint","2023","Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J.","Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","","","","10.48550/arXiv.1910.10683","http://arxiv.org/abs/1910.10683","Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.","2023-09-19","2024-03-01 15:37:11","2024-03-01 15:37:12","2024-03-01 15:37:11","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1910.10683 [cs, stat] version: 4","","/home/lexi/.zotero-data/storage/TZ8RXC5G/Raffel et al. - 2023 - Exploring the Limits of Transfer Learning with a U.pdf; /home/lexi/.zotero-data/storage/CU7W3EFS/1910.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1910.10683","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S5C3UIRN","preprint","2023","Hu, Chengming; Li, Xuan; Liu, Dan; Wu, Haolun; Chen, Xi; Wang, Ju; Liu, Xue","Teacher-Student Architecture for Knowledge Distillation: A Survey","","","","10.48550/arXiv.2308.04268","http://arxiv.org/abs/2308.04268","Although Deep neural networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. To tackle this issue, Teacher-Student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. With the help of Teacher-Student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. Different from existing KD surveys that primarily focus on knowledge compression, this survey first explores Teacher-Student architectures across multiple distillation objectives. This survey presents an introduction to various knowledge representations and their corresponding optimization objectives. Additionally, we provide a systematic overview of Teacher-Student architectures with representative learning algorithms and effective distillation schemes. This survey also summarizes recent applications of Teacher-Student architectures across multiple purposes, including classification, recognition, generation, ranking, and regression. Lastly, potential research directions in KD are investigated, focusing on architecture design, knowledge quality, and theoretical studies of regression-based learning, respectively. Through this comprehensive survey, industry practitioners and the academic community can gain valuable insights and guidelines for effectively designing, learning, and applying Teacher-Student architectures on various distillation objectives.","2023-08-08","2024-03-04 07:35:43","2024-03-04 07:35:43","2024-03-04 07:35:43","","","","","","","Teacher-Student Architecture for Knowledge Distillation","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2308.04268 [cs]","","/home/lexi/.zotero-data/storage/USPVT4NH/Hu et al. - 2023 - Teacher-Student Architecture for Knowledge Distill.pdf; /home/lexi/.zotero-data/storage/P6YEJAN8/2308.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2308.04268","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3BYMAARS","preprint","2021","Gupta, Manish; Agrawal, Puneet","Compression of Deep Learning Models for Text: A Survey","","","","10.48550/arXiv.2008.05221","http://arxiv.org/abs/2008.05221","In recent years, the fields of natural language processing (NLP) and information retrieval (IR) have made tremendous progress thanksto deep learning models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and Long Short-Term Memory (LSTMs)networks, and Transformer [120] based models like Bidirectional Encoder Representations from Transformers (BERT) [24], GenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network (MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer transformer (T5) [95], T-NLG [98] and GShard [63]. But these models are humongous in size. On the other hand,real world applications demand small model size, low response times and low computational power wattage. In this survey, wediscuss six different types of methods (Pruning, Quantization, Knowledge Distillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic Transformer based methods) for compression of such models to enable their deployment in real industry NLP projects.Given the critical need of building applications with efficient and small models, and the large amount of recently published work inthis area, we believe that this survey organizes the plethora of work done by the 'deep learning for NLP' community in the past fewyears and presents it as a coherent story.","2021-06-13","2024-03-04 07:57:41","2024-03-04 07:57:41","2024-03-04 07:57:41","","","","","","","Compression of Deep Learning Models for Text","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2008.05221 [cs]","","/home/lexi/.zotero-data/storage/ZUKQX3S9/Gupta and Agrawal - 2021 - Compression of Deep Learning Models for Text A Su.pdf; /home/lexi/.zotero-data/storage/R87HJZI2/2008.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2008.05221","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3HSG5RC9","preprint","2023","Calderon, Nitay; Mukherjee, Subhabrata; Reichart, Roi; Kantor, Amir","A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training","","","","10.48550/arXiv.2305.02031","http://arxiv.org/abs/2305.02031","Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model for a specific NLG task and a specific dataset. Typically in real-world applications, in addition to labeled data there is abundant unlabeled task-specific data, which is crucial for attaining high compression rates via KD. In this work, we conduct a systematic study of task-specific KD techniques for various NLG tasks under realistic assumptions. We discuss the special characteristics of NLG distillation and particularly the exposure bias problem. Following, we derive a family of Pseudo-Target (PT) augmentation methods, substantially extending prior work on sequence-level KD. We propose the Joint-Teaching method, which applies word-level KD to multiple PTs generated by both the teacher and the student. Finally, we validate our findings in an extreme setup with no labeled examples using GPT-4 as the teacher. Our study provides practical model design observations and demonstrates the effectiveness of PT training for task-specific KD in NLG.","2023-05-26","2024-03-04 08:11:59","2024-03-04 08:11:59","2024-03-04 08:11:59","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2305.02031 [cs]","","/home/lexi/.zotero-data/storage/BBQSJXAI/Calderon et al. - 2023 - A Systematic Study of Knowledge Distillation for N.pdf; /home/lexi/.zotero-data/storage/MFY8W4CS/2305.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2305.02031","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NN5D456L","preprint","2020","Jiao, Xiaoqi; Yin, Yichun; Shang, Lifeng; Jiang, Xin; Chen, Xiao; Li, Linlin; Wang, Fang; Liu, Qun","TinyBERT: Distilling BERT for Natural Language Understanding","","","","10.48550/arXiv.1909.10351","http://arxiv.org/abs/1909.10351","Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny-BERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture he general-domain as well as the task-specific knowledge in BERT. TinyBERT with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERTBASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT with 4 layers is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only about 28% parameters and about 31% inference time of them. Moreover, TinyBERT with 6 layers performs on-par with its teacher BERTBASE.","2020-10-15","2024-03-04 08:52:49","2024-03-04 08:52:49","2024-03-04 08:52:49","","","","","","","TinyBERT","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1909.10351 [cs]","","/home/lexi/.zotero-data/storage/IQSG78PX/Jiao et al. - 2020 - TinyBERT Distilling BERT for Natural Language Und.pdf; /home/lexi/.zotero-data/storage/MLXYFN6W/1909.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1909.10351","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q3ESMDWX","preprint","2023","Jiang, Yuxin; Chan, Chunkit; Chen, Mingyang; Wang, Wei","Lion: Adversarial Distillation of Proprietary Large Language Models","","","","10.48550/arXiv.2305.12870","http://arxiv.org/abs/2305.12870","The practice of transferring knowledge from a sophisticated, proprietary large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher model to a set of instructions. Nevertheless, they overlooked the possibility of incorporating any reciprocal ""feedback""--identifying challenging instructions where the student model's performance falls short--to boost the student model's proficiency iteratively. To this end, we propose a novel adversarial distillation framework for a more efficient knowledge transfer. Leveraging the versatile role adaptability of LLMs, we prompt the teacher model to identify ""hard"" instructions and generate new ""hard"" instructions for the student model, creating a three-stage adversarial loop of imitation, discrimination, and generation. By applying this adversarial framework, we successfully transfer knowledge from ChatGPT to a student model (named Lion), using a mere 70k training data. Our results show that Lion-13B not only achieves comparable open-ended generation capabilities to ChatGPT but surpasses conventional state-of-the-art (SOTA) instruction-tuned models like Vicuna-13B by 55.4% in challenging zero-shot reasoning benchmarks such as BIG-Bench Hard (BBH) and 16.7% on AGIEval. Code and model can be found at https://github.com/YJiangcm/Lion.","2023-10-13","2024-03-04 09:12:14","2024-03-04 09:12:14","2024-03-04 09:12:14","","","","","","","Lion","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2305.12870 [cs]","","/home/lexi/.zotero-data/storage/L9MHA73L/Jiang et al. - 2023 - Lion Adversarial Distillation of Proprietary Larg.pdf; /home/lexi/.zotero-data/storage/V8VKT8N3/2305.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2305.12870","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3C82NPGT","preprint","2022","Fang, Gongfan; Mo, Kanya; Wang, Xinchao; Song, Jie; Bei, Shitao; Zhang, Haofei; Song, Mingli","Up to 100$\times$ Faster Data-free Knowledge Distillation","","","","10.48550/arXiv.2112.06253","http://arxiv.org/abs/2112.06253","Data-free knowledge distillation (DFKD) has recently been attracting increasing attention from research communities, attributed to its capability to compress a model only using synthetic data. Despite the encouraging results achieved, state-of-the-art DFKD methods still suffer from the inefficiency of data synthesis, making the data-free training process extremely time-consuming and thus inapplicable for large-scale tasks. In this work, we introduce an efficacious scheme, termed as FastDFKD, that allows us to accelerate DFKD by a factor of orders of magnitude. At the heart of our approach is a novel strategy to reuse the shared common features in training data so as to synthesize different data instances. Unlike prior methods that optimize a set of data independently, we propose to learn a meta-synthesizer that seeks common features as the initialization for the fast data synthesis. As a result, FastDFKD achieves data synthesis within only a few steps, significantly enhancing the efficiency of data-free training. Experiments over CIFAR, NYUv2, and ImageNet demonstrate that the proposed FastDFKD achieves 10$\times$ and even 100$\times$ acceleration while preserving performances on par with state of the art. Code is available at \url{https://github.com/zju-vipa/Fast-Datafree}.","2022-02-24","2024-03-04 09:15:50","2024-03-04 09:15:50","2024-03-04 09:15:50","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2112.06253 [cs]","","/home/lexi/.zotero-data/storage/SW3HFRH3/Fang et al. - 2022 - Up to 100$times$ Faster Data-free Knowledge Disti.pdf; /home/lexi/.zotero-data/storage/FBZAXUW6/2112.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2112.06253","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4L28TI7Q","preprint","2018","Polino, Antonio; Pascanu, Razvan; Alistarh, Dan","Model compression via distillation and quantization","","","","10.48550/arXiv.1802.05668","http://arxiv.org/abs/1802.05668","Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method, differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model. We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.","2018-02-15","2024-03-04 09:29:12","2024-03-04 09:29:12","2024-03-04 09:29:12","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1802.05668 [cs]","","/home/lexi/.zotero-data/storage/ZCZAN65H/Polino et al. - 2018 - Model compression via distillation and quantizatio.pdf; /home/lexi/.zotero-data/storage/I8TCVQTU/1802.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1802.05668","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UHNS95TD","preprint","2017","Mishra, Asit; Marr, Debbie","Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy","","","","10.48550/arXiv.1711.05852","http://arxiv.org/abs/1711.05852","Deep learning networks have achieved state-of-the-art accuracies on computer vision workloads like image classification and object detection. The performant systems, however, typically involve big models with numerous parameters. Once trained, a challenging aspect for such top performing models is deployment on resource constrained inference systems - the models (often deep networks or wide networks or both) are compute and memory intensive. Low-precision numerics and model compression using knowledge distillation are popular techniques to lower both the compute requirements and memory footprint of these deployed models. In this paper, we study the combination of these two techniques and show that the performance of low-precision networks can be significantly improved by using knowledge distillation techniques. Our approach, Apprentice, achieves state-of-the-art accuracies using ternary precision and 4-bit precision for variants of ResNet architecture on ImageNet dataset. We present three schemes using which one can apply knowledge distillation techniques to various stages of the train-and-deploy pipeline.","2017-11-15","2024-03-04 09:29:14","2024-03-04 09:29:14","2024-03-04 09:29:14","","","","","","","Apprentice","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1711.05852 [cs]","","/home/lexi/.zotero-data/storage/IQUYWXU7/Mishra and Marr - 2017 - Apprentice Using Knowledge Distillation Technique.pdf; /home/lexi/.zotero-data/storage/JZFWNSWI/1711.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1711.05852","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IIL8FRFS","preprint","2022","Li, Zheng; Wang, Zijian; Tan, Ming; Nallapati, Ramesh; Bhatia, Parminder; Arnold, Andrew; Xiang, Bing; Roth, Dan","DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization","","","","10.48550/arXiv.2203.11239","http://arxiv.org/abs/2203.11239","Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve state-of-the-art performance on many generative NLP tasks. However, such models pose a great challenge in resource-constrained scenarios owing to their large memory requirements and high latency. To alleviate this issue, we propose to jointly distill and quantize the model, where knowledge is transferred from the full-precision teacher model to the quantized and distilled low-precision student model. Empirical analyses show that, despite the challenging nature of generative tasks, we were able to achieve a 16.5x model footprint compression ratio with little performance drop relative to the full-precision counterparts on multiple summarization and QA datasets. We further pushed the limit of compression ratio to 27.7x and presented the performance-efficiency trade-off for generative tasks using pre-trained models. To the best of our knowledge, this is the first work aiming to effectively distill and quantize sequence-to-sequence pre-trained models for language generation tasks.","2022-03-21","2024-03-04 09:44:44","2024-03-04 09:44:44","2024-03-04 09:44:44","","","","","","","DQ-BART","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2203.11239 [cs]","","/home/lexi/.zotero-data/storage/AXJGKER3/Li et al. - 2022 - DQ-BART Efficient Sequence-to-Sequence Model via .pdf; /home/lexi/.zotero-data/storage/NX2P8NRB/2203.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2203.11239","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DRMNG7WX","preprint","2015","Hinton, Geoffrey; Vinyals, Oriol; Dean, Jeff","Distilling the Knowledge in a Neural Network","","","","10.48550/arXiv.1503.02531","http://arxiv.org/abs/1503.02531","A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.","2015-03-09","2024-03-04 10:11:37","2024-03-24 19:35:11","2024-03-04 10:11:37","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1503.02531 [cs, stat]","","/home/lexi/.zotero-data/storage/8LJ74UAY/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf; /home/lexi/.zotero-data/storage/MGRDYRJ5/1503.html","","base","","","","","","","","","","","","","","","","","","","","arXiv:1503.02531","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SQWGHEAI","preprint","2018","Li, Hao; Xu, Zheng; Taylor, Gavin; Studer, Christoph; Goldstein, Tom","Visualizing the Loss Landscape of Neural Nets","","","","10.48550/arXiv.1712.09913","http://arxiv.org/abs/1712.09913","Neural network training relies on our ability to find ""good"" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple ""filter normalization"" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.","2018-11-07","2024-03-04 18:50:22","2024-03-04 18:50:22","2024-03-04 18:50:22","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1712.09913 [cs, stat]","","/home/lexi/.zotero-data/storage/695N77B9/Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets.pdf; /home/lexi/.zotero-data/storage/KHYH2PAQ/1712.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1712.09913","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LKFF8HLR","conferencePaper","2022","Chiang, Ping-yeh; Ni, Renkun; Miller, David Yu; Bansal, Arpit; Geiping, Jonas; Goldblum, Micah; Goldstein, Tom","Loss Landscapes are All You Need: Neural Network Generalization Can Be Explained Without the Implicit Bias of Gradient Descent","","","","","https://openreview.net/forum?id=QC10RmRbZy9","It is commonly believed that the implicit regularization of optimizers is needed for neural networks to generalize in the overparameterized regime. In this paper, we observe experimentally that this implicit regularization behavior is {\em generic}, i.e. it does not depend strongly on the choice of optimizer. We demonstrate this by training neural networks using several gradient-free optimizers, which do not benefit from properties that are often attributed to gradient-based optimizers. This includes a guess-and-check optimizer that generates uniformly random parameter vectors until finding one that happens to achieve perfect train accuracy, and a zeroth-order Pattern Search optimizer that uses no gradient computations. In the low sample and few-shot regimes, where zeroth order optimizers are most computationally tractable, we find that these non-gradient optimizers achieve test accuracy comparable to SGD. The code to reproduce results can be found at https://github.com/Ping-C/optimizer .","2022-09-29","2024-03-04 18:51:03","2024-03-04 18:51:03","2024-03-04 18:51:03","","","","","","","Loss Landscapes are All You Need","","","","","","","en","","","","","openreview.net","","","","/home/lexi/.zotero-data/storage/GU5J9R6E/Chiang et al. - 2022 - Loss Landscapes are All You Need Neural Network G.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","The Eleventh International Conference on Learning Representations","","","","","","","","","","","","","","",""
"Q7NA8WS6","preprint","2024","Zou, Yixiong; Liu, Yicong; Hu, Yiman; Li, Yuhua; Li, Ruixuan","Flatten Long-Range Loss Landscapes for Cross-Domain Few-Shot Learning","","","","","http://arxiv.org/abs/2403.00567","Cross-domain few-shot learning (CDFSL) aims to acquire knowledge from limited training data in the target domain by leveraging prior knowledge transferred from source domains with abundant training samples. CDFSL faces challenges in transferring knowledge across dissimilar domains and fine-tuning models with limited training data. To address these challenges, we initially extend the analysis of loss landscapes from the parameter space to the representation space, which allows us to simultaneously interpret the transferring and fine-tuning difficulties of CDFSL models. We observe that sharp minima in the loss landscapes of the representation space result in representations that are hard to transfer and fine-tune. Moreover, existing flatness-based methods have limited generalization ability due to their short-range flatness. To enhance the transferability and facilitate fine-tuning, we introduce a simple yet effective approach to achieve long-range flattening of the minima in the loss landscape. This approach considers representations that are differently normalized as minima in the loss landscape and flattens the high-loss region in the middle by randomly sampling interpolated representations. We implement this method as a new normalization layer that replaces the original one in both CNNs and ViTs. This layer is simple and lightweight, introducing only a minimal number of additional parameters. Experimental results on 8 datasets demonstrate that our approach outperforms state-of-the-art methods in terms of average accuracy. Moreover, our method achieves performance improvements of up to 9\% compared to the current best approaches on individual datasets. Our code will be released.","2024-03-01","2024-03-04 18:51:58","2024-03-04 18:51:58","2024-03-04 18:51:58","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2403.00567 null","","/home/lexi/.zotero-data/storage/FJ6ZXNK6/Zou et al. - 2024 - Flatten Long-Range Loss Landscapes for Cross-Domai.pdf; /home/lexi/.zotero-data/storage/ETBFKXIX/2403.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2403.00567","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F7JBJM7Y","journalArticle","2023","Abdulkadirov, Ruslan; Lyakhov, Pavel; Nagornov, Nikolay","Survey of Optimization Algorithms in Modern Neural Networks","Mathematics","","2227-7390","10.3390/math11112466","https://www.mdpi.com/2227-7390/11/11/2466","The main goal of machine learning is the creation of self-learning algorithms in many areas of human activity. It allows a replacement of a person with artificial intelligence in seeking to expand production. The theory of artificial neural networks, which have already replaced humans in many problems, remains the most well-utilized branch of machine learning. Thus, one must select appropriate neural network architectures, data processing, and advanced applied mathematics tools. A common challenge for these networks is achieving the highest accuracy in a short time. This problem is solved by modifying networks and improving data pre-processing, where accuracy increases along with training time. Bt using optimization methods, one can improve the accuracy without increasing the time. In this review, we consider all existing optimization algorithms that meet in neural networks. We present modifications of optimization algorithms of the first, second, and information-geometric order, which are related to information geometry for Fisher–Rao and Bregman metrics. These optimizers have significantly influenced the development of neural networks through geometric and probabilistic tools. We present applications of all the given optimization algorithms, considering the types of neural networks. After that, we show ways to develop optimization algorithms in further research using modern neural networks. Fractional order, bilevel, and gradient-free optimizers can replace classical gradient-based optimizers. Such approaches are induced in graph, spiking, complex-valued, quantum, and wavelet neural networks. Besides pattern recognition, time series prediction, and object detection, there are many other applications in machine learning: quantum computations, partial differential, and integrodifferential equations, and stochastic processes.","2023-01","2024-03-04 19:08:56","2024-03-04 19:08:56","2024-03-04 19:08:56","2466","","11","11","","","","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","Number: 11 Publisher: Multidisciplinary Digital Publishing Institute","","/home/lexi/.zotero-data/storage/S357XWIZ/Abdulkadirov et al. - 2023 - Survey of Optimization Algorithms in Modern Neural.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QA54U73B","preprint","2023","Wu, Xiaoxia; Li, Cheng; Aminabadi, Reza Yazdani; Yao, Zhewei; He, Yuxiong","Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases","","","","10.48550/arXiv.2301.12017","http://arxiv.org/abs/2301.12017","Improving the deployment efficiency of transformer-based language models has been challenging given their high computation and memory cost. While INT8 quantization has recently been shown to be effective in reducing both the memory cost and latency while preserving model accuracy, it remains unclear whether we can leverage INT4 (which doubles peak hardware throughput) to achieve further latency improvement. In this study, we explore the feasibility of employing INT4 weight and activation (W4A4) quantization for language models. Our findings indicate that W4A4 quantization introduces no to negligible accuracy degradation for encoder-only and encoder-decoder models, but causes a significant accuracy drop for decoder-only models. To materialize the performance gain using W4A4, we develop a highly optimized end-to-end W4A4 encoder inference pipeline supporting different quantization strategies. Our INT4 pipeline is $8.5\times$ faster for latency-oriented scenarios and up to $3\times$ for throughput-oriented scenarios compared to the inference of FP16, and improves the SOTA BERT INT8 performance from FasterTransformer by up to $1.7\times$. We provide insights into the failure cases when applying W4A4 to decoder-only models, and further explore the compatibility of INT4 quantization with other compression methods, like pruning and layer reduction.","2023-05-30","2024-03-05 10:40:59","2024-03-05 10:40:59","2024-03-05 10:40:59","","","","","","","Understanding INT4 Quantization for Transformer Models","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2301.12017 [cs]","","/home/lexi/.zotero-data/storage/89WEEI8Y/Wu et al. - 2023 - Understanding INT4 Quantization for Transformer Mo.pdf; /home/lexi/.zotero-data/storage/SK8NL9N8/2301.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2301.12017","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J24Y6DLL","preprint","2024","Agarwal, Rishabh; Vieillard, Nino; Zhou, Yongchao; Stanczyk, Piotr; Ramos, Sabela; Geist, Matthieu; Bachem, Olivier","On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes","","","","10.48550/arXiv.2306.13649","http://arxiv.org/abs/2306.13649","Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, current KD methods for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during training and those generated by the student during inference. To address this issue, we introduce Generalized Knowledge Distillation (GKD). Instead of solely relying on a fixed set of output sequences, GKD trains the student on its self-generated output sequences by leveraging feedback from the teacher on such sequences. Unlike supervised KD approaches, GKD also offers the flexibility to employ alternative loss functions between the student and teacher, which can be useful when the student lacks the expressivity to mimic the teacher's distribution. Furthermore, GKD facilitates the seamless integration of distillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for distilling auto-regressive language models on summarization, translation, and arithmetic reasoning tasks, and task-agnostic distillation for instruction-tuning.","2024-01-16","2024-03-05 13:26:53","2024-03-05 13:26:53","2024-03-05 13:26:53","","","","","","","On-Policy Distillation of Language Models","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.13649 [cs]","","/home/lexi/.zotero-data/storage/MP2TFD5J/Agarwal et al. - 2024 - On-Policy Distillation of Language Models Learnin.pdf; /home/lexi/.zotero-data/storage/KEJBA3HZ/2306.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2306.13649","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"54WPRCRP","preprint","2024","Zhu, Yuqi; Wang, Xiaohan; Chen, Jing; Qiao, Shuofei; Ou, Yixin; Yao, Yunzhi; Deng, Shumin; Chen, Huajun; Zhang, Ningyu","LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities","","","","10.48550/arXiv.2305.13168","http://arxiv.org/abs/2305.13168","This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extraction task and the development of the corresponding VINE dataset. Based on these empirical findings, we further propose AutoKG, a multi-agent-based approach employing LLMs and external sources for KG construction and reasoning. We anticipate that this research can provide invaluable insights for future undertakings in the field of knowledge graphs. The code and datasets are in https://github.com/zjunlp/AutoKG.","2024-02-22","2024-03-15 08:29:40","2024-03-15 08:30:05","2024-03-15 08:29:40","","","","","","","LLMs for Knowledge Graph Construction and Reasoning","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2305.13168 [cs]","","/home/lexi/.zotero-data/storage/DA3FJDNX/Zhu et al. - 2024 - LLMs for Knowledge Graph Construction and Reasonin.pdf; /home/lexi/.zotero-data/storage/XIZM95WG/2305.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2305.13168","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SRPCRMYY","webpage","2023","Ruder, Sebastian","NLP News #66: ✨ Flashier Attention, 🤐 Gzip classifiers","","","","","https://newsletter.ruder.io/p/flashier-attention-gzip-classifiers","This newsletter discusses attention that enables modeling long sequences and simple but surprisingly competitive classifiers such as based on gzip compression.","2023-08-14","2024-03-15 15:55:53","2024-03-15 15:58:52","2024-03-15 15:55:53","","","","","","","NLP News #66","","","","","","","en","","","","","","","","","/home/lexi/.zotero-data/storage/ULRFFATG/flashier-attention-gzip-classifiers.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YDR4UPXC","preprint","2019","He, Yihui; Lin, Ji; Liu, Zhijian; Wang, Hanrui; Li, Li-Jia; Han, Song","AMC: AutoML for Model Compression and Acceleration on Mobile Devices","","","","10.48550/arXiv.1802.03494","http://arxiv.org/abs/1802.03494","Model compression is a critical technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted heuristics and rule-based policies that require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverage reinforcement learning to provide the model compression policy. This learning-based compression policy outperforms conventional rule-based compression policy by having higher compression ratio, better preserving the accuracy and freeing human labor. Under 4x FLOPs reduction, we achieved 2.7% better accuracy than the handcrafted model compression policy for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet and achieved 1.81x speedup of measured inference latency on an Android phone and 1.43x speedup on the Titan XP GPU, with only 0.1% loss of ImageNet Top-1 accuracy.","2019-01-15","2024-03-24 09:42:28","2024-03-24 12:23:49","2024-03-24 09:42:28","","","","","","","AMC","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1802.03494 [cs]","","/home/lexi/.zotero-data/storage/GH92JX6A/He et al. - 2019 - AMC AutoML for Model Compression and Acceleration.pdf; /home/lexi/.zotero-data/storage/B7Z45T7X/1802.html","","skimmed; base","","","","","","","","","","","","","","","","","","","","arXiv:1802.03494","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IWYGF68Z","preprint","2020","Renda, Alex; Frankle, Jonathan; Carbin, Michael","Comparing Rewinding and Fine-tuning in Neural Network Pruning","","","","10.48550/arXiv.2003.02389","http://arxiv.org/abs/2003.02389","Many neural network pruning algorithms proceed in three steps: train the network to completion, remove unwanted structure to compress the network, and retrain the remaining structure to recover lost accuracy. The standard retraining technique, fine-tuning, trains the unpruned weights from their final trained values using a small fixed learning rate. In this paper, we compare fine-tuning to alternative retraining techniques. Weight rewinding (as proposed by Frankle et al., (2019)), rewinds unpruned weights to their values from earlier in training and retrains them from there using the original training schedule. Learning rate rewinding (which we propose) trains the unpruned weights from their final values using the same learning rate schedule as weight rewinding. Both rewinding techniques outperform fine-tuning, forming the basis of a network-agnostic pruning algorithm that matches the accuracy and compression ratios of several more network-specific state-of-the-art techniques.","2020-03-04","2024-03-24 09:56:48","2024-03-24 10:30:08","2024-03-24 09:56:48","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2003.02389 [cs, stat]","","/home/lexi/.zotero-data/storage/MGTMRSAE/Renda et al. - 2020 - Comparing Rewinding and Fine-tuning in Neural Netw.pdf; /home/lexi/.zotero-data/storage/CELZZPD4/2003.html","","skimmed","","","","","","","","","","","","","","","","","","","","arXiv:2003.02389","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LJZQVNHG","preprint","2019","Frankle, Jonathan; Carbin, Michael","The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks","","","","10.48550/arXiv.1803.03635","http://arxiv.org/abs/1803.03635","Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the ""lottery ticket hypothesis:"" dense, randomly-initialized, feed-forward networks contain subnetworks (""winning tickets"") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.","2019-03-04","2024-03-24 10:12:26","2024-03-24 11:00:16","2024-03-24 10:12:26","","","","","","","The Lottery Ticket Hypothesis","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1803.03635 [cs]","","/home/lexi/.zotero-data/storage/I5E8MZWR/Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf; /home/lexi/.zotero-data/storage/USYCPWVB/1803.html","","skimmed; base","","","","","","","","","","","","","","","","","","","","arXiv:1803.03635","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P8QFYWJH","preprint","2018","Zoph, Barret; Vasudevan, Vijay; Shlens, Jonathon; Le, Quoc V.","Learning Transferable Architectures for Scalable Image Recognition","","","","10.48550/arXiv.1707.07012","http://arxiv.org/abs/1707.07012","Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the ""NASNet search space"") which enables transferability. In our experiments, we search for the best convolutional layer (or ""cell"") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named ""NASNet architecture"". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.","2018-04-11","2024-03-24 11:11:54","2024-03-24 11:11:54","2024-03-24 11:11:54","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1707.07012 [cs, stat]","","/home/lexi/.zotero-data/storage/HYL5UNIB/Zoph et al. - 2018 - Learning Transferable Architectures for Scalable I.pdf; /home/lexi/.zotero-data/storage/J4IMS7ZW/1707.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1707.07012","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W56LXJBL","preprint","2017","Ashok, Anubhav; Rhinehart, Nicholas; Beainy, Fares; Kitani, Kris M.","N2N Learning: Network to Network Compression via Policy Gradient Reinforcement Learning","","","","10.48550/arXiv.1709.06030","http://arxiv.org/abs/1709.06030","While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints. Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics. Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task. In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning. Our approach takes a larger `teacher' network as input and outputs a compressed `student' network derived from the `teacher' network. In the first stage of our method, a recurrent policy network aggressively removes layers from the large `teacher' model. In the second stage, another recurrent policy network carefully reduces the size of each remaining layer. The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network. Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network. Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input `teacher' network. We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller `teacher' networks can be used to rapidly speed up training on larger `teacher' networks.","2017-12-17","2024-03-24 11:13:16","2024-03-24 11:13:16","2024-03-24 11:13:16","","","","","","","N2N Learning","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1709.06030 [cs, stat]","","/home/lexi/.zotero-data/storage/XPHNKNC5/Ashok et al. - 2017 - N2N Learning Network to Network Compression via P.pdf; /home/lexi/.zotero-data/storage/29MZW5HY/1709.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1709.06030","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R4Z65X6X","preprint","2017","Cai, Han; Chen, Tianyao; Zhang, Weinan; Yu, Yong; Wang, Jun","Efficient Architecture Search by Network Transformation","","","","10.48550/arXiv.1707.04873","http://arxiv.org/abs/1707.04873","Techniques for automatically designing deep neural network architectures such as reinforcement learning based approaches have recently shown promising results. However, their success is based on vast computational resources (e.g. hundreds of GPUs), making them difficult to be widely used. A noticeable limitation is that they still design and train each network from scratch during the exploration of the architecture space, which is highly inefficient. In this paper, we propose a new framework toward efficient architecture search by exploring the architecture space based on the current network and reusing its weights. We employ a reinforcement learning agent as the meta-controller, whose action is to grow the network depth or layer width with function-preserving transformations. As such, the previously validated networks can be reused for further exploration, thus saves a large amount of computational cost. We apply our method to explore the architecture space of the plain convolutional neural networks (no skip-connections, branching etc.) on image benchmark datasets (CIFAR-10, SVHN) with restricted computational resources (5 GPUs). Our method can design highly competitive networks that outperform existing networks using the same design scheme. On CIFAR-10, our model without skip-connections achieves 4.23\% test error rate, exceeding a vast majority of modern architectures and approaching DenseNet. Furthermore, by applying our method to explore the DenseNet architecture space, we are able to achieve more accurate networks with fewer parameters.","2017-11-21","2024-03-24 11:13:21","2024-03-24 11:13:21","2024-03-24 11:13:21","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1707.04873 [cs]","","/home/lexi/.zotero-data/storage/H7PYRMKQ/Cai et al. - 2017 - Efficient Architecture Search by Network Transform.pdf; /home/lexi/.zotero-data/storage/XQ4N9RA9/1707.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1707.04873","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D395K5MZ","journalArticle","2023","Li, Zhuo; Li, Hengyi; Meng, Lin","Model Compression for Deep Neural Networks: A Survey","Computers","","2073-431X","10.3390/computers12030060","https://www.mdpi.com/2073-431X/12/3/60","Currently, with the rapid development of deep learning, deep neural networks (DNNs) have been widely applied in various computer vision tasks. However, in the pursuit of performance, advanced DNN models have become more complex, which has led to a large memory footprint and high computation demands. As a result, the models are difficult to apply in real time. To address these issues, model compression has become a focus of research. Furthermore, model compression techniques play an important role in deploying models on edge devices. This study analyzed various model compression methods to assist researchers in reducing device storage space, speeding up model inference, reducing model complexity and training costs, and improving model deployment. Hence, this paper summarized the state-of-the-art techniques for model compression, including model pruning, parameter quantization, low-rank decomposition, knowledge distillation, and lightweight model design. In addition, this paper discusses research challenges and directions for future work.","2023-03","2024-03-24 12:59:12","2024-03-24 12:59:18","2024-03-24 12:59:12","60","","3","12","","","Model Compression for Deep Neural Networks","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","Number: 3 Publisher: Multidisciplinary Digital Publishing Institute","","/home/lexi/.zotero-data/storage/GNF9AJBE/Li et al. - 2023 - Model Compression for Deep Neural Networks A Surv.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6KIQJY84","journalArticle","2021","Gou, Jianping; Yu, Baosheng; Maybank, Stephen John; Tao, Dacheng","Knowledge Distillation: A Survey","International Journal of Computer Vision","","0920-5691, 1573-1405","10.1007/s11263-021-01453-z","http://arxiv.org/abs/2006.05525","In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher-student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.","2021-06","2024-03-24 19:31:25","2024-03-24 19:31:25","2024-03-24 19:31:25","1789-1819","","6","129","","Int J Comput Vis","Knowledge Distillation","","","","","","","","","","","","arXiv.org","","arXiv:2006.05525 [cs, stat]","","/home/lexi/.zotero-data/storage/YSTR39LB/Gou et al. - 2021 - Knowledge Distillation A Survey.pdf; /home/lexi/.zotero-data/storage/HTRSKVQP/2006.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C7PYT75I","preprint","2023","Dravid, Amil; Gandelsman, Yossi; Efros, Alexei A.; Shocher, Assaf","Rosetta Neurons: Mining the Common Units in a Model Zoo","","","","10.48550/arXiv.2306.09346","http://arxiv.org/abs/2306.09346","Do different neural networks, trained for various vision tasks, share some common representations? In this paper, we demonstrate the existence of common features we call ""Rosetta Neurons"" across a range of models with different architectures, different tasks (generative and discriminative), and different types of supervision (class-supervised, text-supervised, self-supervised). We present an algorithm for mining a dictionary of Rosetta Neurons across several popular vision models: Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50, BigGAN, StyleGAN-2, StyleGAN-XL. Our findings suggest that certain visual concepts and structures are inherently embedded in the natural world and can be learned by different models regardless of the specific task or architecture, and without the use of semantic labels. We can visualize shared concepts directly due to generative models included in our analysis. The Rosetta Neurons facilitate model-to-model translation enabling various inversion-based manipulations, including cross-class alignments, shifting, zooming, and more, without the need for specialized training.","2023-06-16","2024-03-24 20:05:55","2024-03-24 20:05:55","2024-03-24 20:05:55","","","","","","","Rosetta Neurons","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.09346 [cs]","","/home/lexi/.zotero-data/storage/IZA7S67M/Dravid et al. - 2023 - Rosetta Neurons Mining the Common Units in a Mode.pdf; /home/lexi/.zotero-data/storage/GD8VNDH2/2306.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2306.09346","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SXJ7KN38","preprint","2023","Li, Xuanlin; Fang, Yunhao; Liu, Minghua; Ling, Zhan; Tu, Zhuowen; Su, Hao","Distilling Large Vision-Language Model with Out-of-Distribution Generalizability","","","","10.48550/arXiv.2307.03135","http://arxiv.org/abs/2307.03135","Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a small- or mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enriching the teacher's language representations with informative and finegrained semantic attributes to effectively distinguish between different labels. We propose several metrics and conduct extensive experiments to investigate their techniques. The results demonstrate significant improvements in zero-shot and few-shot student performance on open-vocabulary out-of-distribution classification, highlighting the effectiveness of our proposed approaches. Poster: https://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf Code: https://github.com/xuanlinli17/large_vlm_distillation_ood","2023-10-11","2024-03-24 20:11:59","2024-03-24 20:12:00","2024-03-24 20:11:59","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2307.03135 [cs]","","/home/lexi/.zotero-data/storage/WRPY2TF8/Li et al. - 2023 - Distilling Large Vision-Language Model with Out-of.pdf; /home/lexi/.zotero-data/storage/F39AF98V/2307.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2307.03135","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N57F55HG","preprint","2023","Shi, Baifeng; Gai, Siyu; Darrell, Trevor; Wang, Xin","TOAST: Transfer Learning via Attention Steering","","","","10.48550/arXiv.2305.15542","http://arxiv.org/abs/2305.15542","Transfer learning involves adapting a pre-trained model to novel downstream tasks. However, we observe that current transfer learning methods often fail to focus on task-relevant features. In this work, we explore refocusing model attention for transfer learning. We introduce Top-Down Attention Steering (TOAST), a novel transfer learning algorithm that keeps the pre-trained backbone frozen, selects task-relevant features in the output, and feeds those features back to the model to steer the attention to the task-specific features. By refocusing the attention only, TOAST achieves state-of-the-art results on a number of transfer learning benchmarks, while having a small number of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt tuning, TOAST substantially improves performance across a range of fine-grained visual classification datasets (e.g., 81.1% -> 86.2% on FGVC). TOAST also outperforms the fully fine-tuned Alpaca and Vicuna models on instruction-following language generation. Code is available at https://github.com/bfshi/TOAST.","2023-07-11","2024-03-24 20:18:06","2024-03-24 20:18:06","2024-03-24 20:18:06","","","","","","","TOAST","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2305.15542 [cs]","","/home/lexi/.zotero-data/storage/KUWSMY35/Shi et al. - 2023 - TOAST Transfer Learning via Attention Steering.pdf; /home/lexi/.zotero-data/storage/GGBFF3RR/2305.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2305.15542","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7QS496KQ","conferencePaper","2024","Yan, Caixia; Chang, Xiaojun; Li, Zhihui; Yao, Lina; Luo, Minnan; Zheng, Qinghua","Masked Distillation Advances Self-Supervised Transformer Architecture Search","","","","","https://openreview.net/forum?id=LUpC8KTvdV&referrer=%5Bthe%20profile%20of%20Zhihui%20Li%5D(%2Fprofile%3Fid%3D~Zhihui_Li1)","Transformer architecture search (TAS) has achieved remarkable progress in automating the neural architecture design process of vision transformers. Recent TAS advancements have discovered outstanding transformer architectures while saving tremendous labor from human experts. However, it is still cumbersome to deploy these methods in real-world applications due to the expensive costs of data labeling under the supervised learning paradigm. To this end, this paper proposes a masked image modelling (MIM) based self-supervised neural architecture search method specifically designed for vision transformers, termed as MaskTAS, which completely avoids the expensive costs of data labeling inherited from supervised learning. Based on the one-shot NAS framework, MaskTAS requires to train various weight-sharing subnets, which can easily diverged without strong supervision in MIM-based self-supervised learning. For this issue, we design the search space of MaskTAS as a siamesed teacher-student architecture to distill knowledge from pre-trained networks, allowing for efficient training of the transformer supernet. To achieve self-supervised transformer architecture search, we further design a novel unsupervised evaluation metric for the evolutionary search algorithm, where each candidate of the student branch is rated by measuring its consistency with the larger teacher network. Extensive experiments demonstrate that the searched architectures can achieve state-of-the-art accuracy on CIFAR-10, CIFAR-100, and ImageNet datasets even without using manual labels. Moreover, the proposed MaskTAS can generalize well to various data domains and tasks by searching specialized transformer architectures in self-supervised manner.","2024-01-16","2024-03-24 20:22:52","2024-03-24 20:24:38","2024-03-24 20:22:52","","","","","","","","","","","","","","en","","","","","openreview.net","","","","/home/lexi/.zotero-data/storage/CRD56INL/Yan et al. - 2023 - Masked Distillation Advances Self-Supervised Trans.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","The Twelfth International Conference on Learning Representations","","","","","","","","","","","","","","",""
"NRGZRTT3","conferencePaper","2021","Xu, Jin; Tan, Xu; Luo, Renqian; Song, Kaitao; Li, Jian; Qin, Tao; Liu, Tie-Yan","NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search","Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining","","","10.1145/3447548.3467262","http://arxiv.org/abs/2105.14444","While pre-trained language models (e.g., BERT) have achieved impressive results on different natural language processing tasks, they have large numbers of parameters and suffer from big computational and memory costs, which make them difficult for real-world deployment. Therefore, model compression is necessary to reduce the computation and memory cost of pre-trained models. In this work, we aim to compress BERT and address the following two challenging practical issues: (1) The compression algorithm should be able to output multiple compressed models with different sizes and latencies, in order to support devices with different memory and latency limitations; (2) The algorithm should be downstream task agnostic, so that the compressed models are generally applicable for different downstream tasks. We leverage techniques in neural architecture search (NAS) and propose NAS-BERT, an efficient method for BERT compression. NAS-BERT trains a big supernet on a search space containing a variety of architectures and outputs multiple compressed models with adaptive sizes and latency. Furthermore, the training of NAS-BERT is conducted on standard self-supervised pre-training tasks (e.g., masked language model) and does not depend on specific downstream tasks. Thus, the compressed models can be used across various downstream tasks. The technical challenge of NAS-BERT is that training a big supernet on the pre-training task is extremely costly. We employ several techniques including block-wise search, search space pruning, and performance approximation to improve search efficiency and accuracy. Extensive experiments on GLUE and SQuAD benchmark datasets demonstrate that NAS-BERT can find lightweight models with better accuracy than previous approaches, and can be directly applied to different downstream tasks with adaptive model sizes for different requirements of memory or latency.","2021-08-14","2024-03-24 20:25:32","2024-03-24 20:25:32","2024-03-24 20:25:32","1933-1943","","","","","","NAS-BERT","","","","","","","","","","","","arXiv.org","","arXiv:2105.14444 [cs]","","/home/lexi/.zotero-data/storage/MWUUJ44P/Xu et al. - 2021 - NAS-BERT Task-Agnostic and Adaptive-Size BERT Com.pdf; /home/lexi/.zotero-data/storage/F3NJDU6C/2105.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CFYNW5RP","preprint","2022","Xu, Canwen; McAuley, Julian","A Survey on Model Compression and Acceleration for Pretrained Language Models","","","","10.48550/arXiv.2202.07105","http://arxiv.org/abs/2202.07105","Despite achieving state-of-the-art performance on many NLP tasks, the high energy cost and long inference delay prevent Transformer-based pretrained language models (PLMs) from seeing broader adoption including for edge and mobile computing. Efficient NLP research aims to comprehensively consider computation, time and carbon emission for the entire life-cycle of NLP, including data preparation, model training and inference. In this survey, we focus on the inference stage and review the current state of model compression and acceleration for pretrained language models, including benchmarks, metrics and methodology.","2022-11-29","2024-03-24 20:45:27","2024-03-24 20:45:27","2024-03-24 20:45:27","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2202.07105 [cs]","","/home/lexi/.zotero-data/storage/VXELSHBZ/Xu and McAuley - 2022 - A Survey on Model Compression and Acceleration for.pdf; /home/lexi/.zotero-data/storage/Z4RBAZRW/2202.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2202.07105","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"56E6LVA2","preprint","2023","Treviso, Marcos; Lee, Ji-Ung; Ji, Tianchu; van Aken, Betty; Cao, Qingqing; Ciosici, Manuel R.; Hassid, Michael; Heafield, Kenneth; Hooker, Sara; Raffel, Colin; Martins, Pedro H.; Martins, André F. T.; Forde, Jessica Zosa; Milder, Peter; Simpson, Edwin; Slonim, Noam; Dodge, Jesse; Strubell, Emma; Balasubramanian, Niranjan; Derczynski, Leon; Gurevych, Iryna; Schwartz, Roy","Efficient Methods for Natural Language Processing: A Survey","","","","10.48550/arXiv.2209.00099","http://arxiv.org/abs/2209.00099","Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research into efficient methods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.","2023-03-24","2022-11-23 06:51:22","2024-03-28 14:08:47","2024-03-24 20:48:13","","","","","","","Efficient Methods for Natural Language Processing","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2209.00099 [cs]","","/home/lexi/.zotero-data/storage/ZD2KSGGJ/Treviso et al. - 2022 - Efficient Methods for Natural Language Processing.pdf; /home/lexi/.zotero-data/storage/BYGQPNIM/Treviso et al. - 2023 - Efficient Methods for Natural Language Processing.pdf; /home/lexi/.zotero-data/storage/A62D45P5/2209.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2209.00099","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PC3X9JDS","preprint","2023","Kang, Junmo; Xu, Wei; Ritter, Alan","Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models","","","","10.48550/arXiv.2305.01645","http://arxiv.org/abs/2305.01645","Fine-tuning large models is highly effective, however, inference can be expensive and produces carbon emissions. Knowledge distillation has been shown to be a practical solution to reduce inference costs, but the distillation process itself requires significant computational resources. Rather than buying or renting GPUs to fine-tune, then distill a large model, an NLP practitioner might instead choose to allocate the available budget to hire annotators and manually label additional fine-tuning data. In this paper, we investigate how to most efficiently use a fixed budget to build a compact model. Through extensive experiments on six diverse tasks, we show that distilling from T5-XXL (11B) to T5-Small (60M) is almost always a cost-efficient strategy compared to annotating more data to directly train a compact model (T5-Small). We further investigate how the optimal budget allocated towards computation varies across scenarios. We will make our code, datasets, annotation cost estimates, and baseline models available as a benchmark to support further work on cost-efficient training of compact models.","2023-07-05","2024-03-24 20:58:35","2024-03-24 20:58:35","2024-03-24 20:58:35","","","","","","","Distill or Annotate?","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2305.01645 [cs]","","/home/lexi/.zotero-data/storage/II6XTUIY/Kang et al. - 2023 - Distill or Annotate Cost-Efficient Fine-Tuning of.pdf; /home/lexi/.zotero-data/storage/SMVSIPEQ/2305.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2305.01645","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DBZTUFTU","conferencePaper","2023","Zhou, Yongchao; Lyu, Kaifeng; Rawat, Ankit Singh; Menon, Aditya Krishna; Rostamizadeh, Afshin; Kumar, Sanjiv; Kagy, Jean-François; Agarwal, Rishabh","DistillSpec: Improving Speculative Decoding via Knowledge Distillation","","","","","https://openreview.net/forum?id=rsY6J3ZaTF","Speculative decoding~(SD) accelerates large language model inference by employing a faster {\em draft} model for generating multiple tokens, which are then verified in parallel by the larger {\em target} model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose {\em DistillSpec} that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improve the draft and target alignment: utilizing \emph{on-policy} data generation from the draft model, and \emph{tailoring the divergence function} to the task and decoding strategy. Notably, DistillSpec yields impressive $10 - 45\%$ speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by $6 - 10\times$ with minimal performance drop, compared to standard decoding without distillation.","2023-10-13","2024-03-27 20:45:07","2024-03-27 20:45:07","2024-03-27 20:45:07","","","","","","","DistillSpec","","","","","","","en","","","","","openreview.net","","","","/home/lexi/.zotero-data/storage/NLGZ5YMV/Zhou et al. - 2023 - DistillSpec Improving Speculative Decoding via Kn.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","The Twelfth International Conference on Learning Representations","","","","","","","","","","","","","","",""
"R2Z7VFXF","journalArticle","2022","Zhang, Minjia; Naresh, Niranjan Uma; He, Yuxiong","Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-trained Transformers","Proceedings of the AAAI Conference on Artificial Intelligence","","2374-3468","10.1609/aaai.v36i10.21423","https://ojs.aaai.org/index.php/AAAI/article/view/21423","Deep and large pre-trained language models (e.g., BERT, GPT-3) are state-of-the-art for various natural language processing tasks. However, the huge size of these models brings challenges to fine-tuning and online deployment due to latency and cost constraints. Existing knowledge distillation methods reduce the model size, but they may encounter difficulties transferring knowledge from the teacher model to the student model due to the limited data from the downstream tasks. In this work, we propose AD^2, a novel and effective data augmentation approach to improving the task-specific knowledge transfer when compressing large pre-trained transformer models. Different from prior methods, AD^2 performs distillation by using an enhanced training set that contains both original inputs and adversarially perturbed samples that mimic the output distribution from the teacher.     Experimental results show that this method allows better transfer of knowledge from the teacher to the student during distillation, producing student models that retain 99.6\% accuracy of the teacher model while outperforming existing task-specific knowledge distillation baselines by 1.2 points on average over a variety of natural language understanding tasks. Moreover, compared with alternative data augmentation methods, such as text-editing-based approaches, AD^2 is up to 28 times faster while achieving comparable or higher accuracy. In addition, when AD^2 is combined with more advanced task-agnostic distillation, we can advance the state-of-the-art performance even more. On top of the encouraging performance, this paper also provides thorough ablation studies and analysis. The discovered interplay between KD and adversarial data augmentation for compressing pre-trained Transformers may further inspire more advanced KD algorithms for compressing even larger scale models.","2022-06-28","2024-03-27 20:55:08","2024-03-27 21:03:23","2024-03-27 20:55:08","11685-11693","","10","36","","","","","","","","","","en","Copyright (c) 2022 Association for the Advancement of Artificial Intelligence","","","","ojs.aaai.org","","Number: 10","","/home/lexi/.zotero-data/storage/97JQJNMY/Zhang et al. - 2022 - Adversarial Data Augmentation for Task-Specific Kn.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QT2EQIPN","journalArticle","2022","Xu, Qing; Chen, Zhenghua; Ragab, Mohamed; Wang, Chao; Wu, Min; Li, Xiaoli","Contrastive adversarial knowledge distillation for deep model compression in time-series regression tasks","Neurocomputing","","0925-2312","10.1016/j.neucom.2021.04.139","https://www.sciencedirect.com/science/article/pii/S0925231221016374","Knowledge distillation (KD) attempts to compress a deep teacher model into a shallow student model by letting the student mimic the teacher’s outputs. However, conventional KD approaches can have the following shortcomings. First, existing KD approaches align the global distribution between teacher and student models and overlook the fine-grained features. Second, most of existing approaches focus on classification tasks and require the architecture of teacher and student models to be similar. To address these limitations, we propose a contrastive adversarial knowledge distillation called CAKD for time series regression tasks where the student and teacher are using different architectures. Specifically, we first propose adversarial adaptation to automatically align the feature distribution between student and teacher networks respectively. Yet, adversarial adaptation can only align the global feature distribution without considering the fine-grained features. To mitigate this issue, we employ a novel contrastive loss for instance-wise alignment between the student and teacher. Particularly, we maximize similarity between teacher and student features that originate from the same sample. Lastly, a KD loss is used to for the knowledge distillation where the teacher and student have two different architectures. We used a turbofan engine dataset that consists of four sub-datasets to evaluate the model performance. The results show that the proposed CAKD method consistently outperforms state-of-the-art methods in terms of two different metrics.","2022-05-07","2024-03-27 20:59:35","2024-03-27 20:59:35","2024-03-27 20:59:35","242-251","","","485","","Neurocomputing","","","","","","","","","","","","","ScienceDirect","","","","; /home/lexi/.zotero-data/storage/ELFQMJD2/S0925231221016374.html","https://personal.ntu.edu.sg/xlli/publication/Compression.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WAX55K82","preprint","2024","Kim, Hyungmin; Suh, Sungho; Baek, Sunghyun; Kim, Daehwan; Jeong, Daun; Cho, Hansang; Kim, Junmo","AI-KD: Adversarial learning and Implicit regularization for self-Knowledge Distillation","","","","10.48550/arXiv.2211.10938","http://arxiv.org/abs/2211.10938","We present a novel adversarial penalized self-knowledge distillation method, named adversarial learning and implicit regularization for self-knowledge distillation (AI-KD), which regularizes the training procedure by adversarial learning and implicit distillations. Our model not only distills the deterministic and progressive knowledge which are from the pre-trained and previous epoch predictive probabilities but also transfers the knowledge of the deterministic predictive distributions using adversarial learning. The motivation is that the self-knowledge distillation methods regularize the predictive probabilities with soft targets, but the exact distributions may be hard to predict. Our method deploys a discriminator to distinguish the distributions between the pre-trained and student models while the student model is trained to fool the discriminator in the trained procedure. Thus, the student model not only can learn the pre-trained model's predictive probabilities but also align the distributions between the pre-trained and student models. We demonstrate the effectiveness of the proposed method with network architectures on multiple datasets and show the proposed method achieves better performance than state-of-the-art methods.","2024-03-21","2024-03-27 21:03:49","2024-03-27 21:03:49","2024-03-27 21:03:49","","","","","","","AI-KD","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2211.10938 [cs]","","/home/lexi/.zotero-data/storage/QPTE42H9/Kim et al. - 2024 - AI-KD Adversarial learning and Implicit regulariz.pdf; /home/lexi/.zotero-data/storage/W47QSLMG/2211.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2211.10938","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C46A65RP","preprint","2023","Wang, Yuzheng; Chen, Zhaoyu; Zhang, Jie; Yang, Dingkang; Ge, Zuhao; Liu, Yang; Liu, Siao; Sun, Yunquan; Zhang, Wenqiang; Qi, Lizhe","Sampling to Distill: Knowledge Transfer from Open-World Data","","","","10.48550/arXiv.2307.16601","http://arxiv.org/abs/2307.16601","Data-Free Knowledge Distillation (DFKD) is a novel task that aims to train high-performance student models using only the teacher network without original training data. Despite encouraging results, existing DFKD methods rely heavily on generation modules with high computational costs. Meanwhile, they ignore the fact that the generated and original data exist domain shifts due to the lack of supervision information. Moreover, knowledge is transferred through each example, ignoring the implicit relationship among multiple examples. To this end, we propose a novel Open-world Data Sampling Distillation (ODSD) method without a redundant generation process. First, we try to sample open-world data close to the original data's distribution by an adaptive sampling module. Then, we introduce a low-noise representation to alleviate the domain shifts and build a structured relationship of multiple data examples to exploit data knowledge. Extensive experiments on CIFAR-10, CIFAR-100, NYUv2, and ImageNet show that our ODSD method achieves state-of-the-art performance. Especially, we improve 1.50\%-9.59\% accuracy on the ImageNet dataset compared with the existing results.","2023-07-31","2024-03-27 21:07:07","2024-03-27 21:07:08","2024-03-27 21:07:07","","","","","","","Sampling to Distill","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2307.16601 [cs]","","/home/lexi/.zotero-data/storage/U3HE4W32/Wang et al. - 2023 - Sampling to Distill Knowledge Transfer from Open-.pdf; /home/lexi/.zotero-data/storage/HCQPMD28/2307.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2307.16601","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LRWTSPRX","preprint","2021","Wang, Fusheng; Yan, Jianhao; Meng, Fandong; Zhou, Jie","Selective Knowledge Distillation for Neural Machine Translation","","","","10.48550/arXiv.2105.12967","http://arxiv.org/abs/2105.12967","Neural Machine Translation (NMT) models achieve state-of-the-art performance on many translation benchmarks. As an active research field in NMT, knowledge distillation is widely applied to enhance the model's performance by transferring teacher model's knowledge on each training sample. However, previous work rarely discusses the different impacts and connections among these samples, which serve as the medium for transferring teacher knowledge. In this paper, we design a novel protocol that can effectively analyze the different impacts of samples by comparing various samples' partitions. Based on above protocol, we conduct extensive experiments and find that the teacher's knowledge is not the more, the better. Knowledge over specific samples may even hurt the whole performance of knowledge distillation. Finally, to address these issues, we propose two simple yet effective strategies, i.e., batch-level and global-level selections, to pick suitable samples for distillation. We evaluate our approaches on two large-scale machine translation tasks, WMT'14 English->German and WMT'19 Chinese->English. Experimental results show that our approaches yield up to +1.28 and +0.89 BLEU points improvements over the Transformer baseline, respectively.","2021-05-27","2024-03-27 21:10:42","2024-03-27 21:10:48","2024-03-27 21:10:42","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2105.12967 [cs]","","/home/lexi/.zotero-data/storage/9KNU8Q5H/Wang et al. - 2021 - Selective Knowledge Distillation for Neural Machin.pdf; /home/lexi/.zotero-data/storage/3V82DMRJ/2105.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2105.12967","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TULQXKD5","preprint","2020","Wen, Tiancheng; Lai, Shenqi; Qian, Xueming","Preparing Lessons: Improve Knowledge Distillation with Better Supervision","","","","10.48550/arXiv.1911.07471","http://arxiv.org/abs/1911.07471","Knowledge distillation (KD) is widely used for training a compact model with the supervision of another large model, which could effectively improve the performance. Previous methods mainly focus on two aspects: 1) training the student to mimic representation space of the teacher; 2) training the model progressively or adding extra module like discriminator. Knowledge from teacher is useful, but it is still not exactly right compared with ground truth. Besides, overly uncertain supervision also influences the result. We introduce two novel approaches, Knowledge Adjustment (KA) and Dynamic Temperature Distillation (DTD), to penalize bad supervision and improve student model. Experiments on CIFAR-100, CINIC-10 and Tiny ImageNet show that our methods get encouraging performance compared with state-of-the-art methods. When combined with other KD-based methods, the performance will be further improved.","2020-07-24","2024-03-27 21:13:57","2024-03-27 21:13:58","2024-03-27 21:13:57","","","","","","","Preparing Lessons","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1911.07471 [cs]","","/home/lexi/.zotero-data/storage/4LU54XEL/Wen et al. - 2020 - Preparing Lessons Improve Knowledge Distillation .pdf; /home/lexi/.zotero-data/storage/89TC3NIJ/1911.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1911.07471","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8AZBXEUC","preprint","2022","Zhou, Wangchunshu; Xu, Canwen; McAuley, Julian","BERT Learns to Teach: Knowledge Distillation with Meta Learning","","","","10.48550/arXiv.2106.04570","http://arxiv.org/abs/2106.04570","We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models.","2022-04-01","2024-03-27 21:14:39","2024-03-27 21:14:39","2024-03-27 21:14:39","","","","","","","BERT Learns to Teach","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2106.04570 [cs]","","/home/lexi/.zotero-data/storage/5JUNYNPP/Zhou et al. - 2022 - BERT Learns to Teach Knowledge Distillation with .pdf; /home/lexi/.zotero-data/storage/HE665TIA/2106.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2106.04570","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SB6GTC5D","preprint","2022","Huang, Yukun; Chen, Yanda; Yu, Zhou; McKeown, Kathleen","In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models","","","","10.48550/arXiv.2212.10670","http://arxiv.org/abs/2212.10670","Given the success with in-context learning of large pre-trained language models, we introduce in-context learning distillation to transfer in-context few-shot learning ability from large models to smaller models. We propose to combine in-context learning objectives with language modeling objectives to distill both the ability to read in-context examples and task knowledge to the smaller models. We perform in-context learning distillation under two different few-shot learning paradigms: Meta In-context Tuning (Meta-ICT) and Multitask In-context Tuning (Multitask-ICT). Multitask-ICT performs better on multitask few-shot learning but also requires more computation than Meta-ICT. Our method shows consistent improvements for both Meta-ICT and Multitask-ICT on two benchmarks: LAMA and CrossFit. Our extensive experiments and analysis reveal that in-context learning objectives and language modeling objectives are complementary under the Multitask-ICT paradigm. In-context learning objectives achieve the best performance when combined with language modeling objectives.","2022-12-20","2024-03-27 21:15:28","2024-03-27 21:15:28","2024-03-27 21:15:28","","","","","","","In-context Learning Distillation","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2212.10670 [cs]","","/home/lexi/.zotero-data/storage/VFHY3LQX/Huang et al. - 2022 - In-context Learning Distillation Transferring Few.pdf; /home/lexi/.zotero-data/storage/ZL23VGCA/2212.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2212.10670","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6V9QKR7Q","preprint","2023","Chai, Yuji; Bailey, Luke; Jin, Yunho; Karle, Matthew; Ko, Glenn G.; Brooks, David; Wei, Gu-Yeon; Kung, H. T.","SpeedLimit: Neural Architecture Search for Quantized Transformer Models","","","","10.48550/arXiv.2209.12127","http://arxiv.org/abs/2209.12127","While research in the field of transformer models has primarily focused on enhancing performance metrics such as accuracy and perplexity, practical applications in industry often necessitate a rigorous consideration of inference latency constraints. Addressing this challenge, we introduce SpeedLimit, a novel Neural Architecture Search (NAS) technique that optimizes accuracy whilst adhering to an upper-bound latency constraint. Our method incorporates 8-bit integer quantization in the search process to outperform the current state-of-the-art technique. Our results underline the feasibility and efficacy of seeking an optimal balance between performance and latency, providing new avenues for deploying state-of-the-art transformer models in latency-sensitive environments.","2023-10-13","2024-03-27 21:17:02","2024-03-27 21:17:03","2024-03-27 21:17:02","","","","","","","SpeedLimit","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2209.12127 [cs]","","/home/lexi/.zotero-data/storage/V8E26THC/Chai et al. - 2023 - SpeedLimit Neural Architecture Search for Quantiz.pdf; /home/lexi/.zotero-data/storage/RI6P3P63/2209.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2209.12127","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LSWQZ7I2","journalArticle","2023","Jeon, Hyojin; Park, Seungcheol; Kim, Jin-Gee; Kang, U.","PET: Parameter-efficient Knowledge Distillation on Transformer","PLOS ONE","","1932-6203","10.1371/journal.pone.0288060","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10325108/","Given a large Transformer model, how can we obtain a small and computationally efficient model which maintains the performance of the original model? Transformer has shown significant performance improvements for many NLP tasks in recent years. However, their large size, expensive computational cost, and long inference time make it challenging to deploy them to resource-constrained devices. Existing Transformer compression methods mainly focus on reducing the size of the encoder ignoring the fact that the decoder takes the major portion of the long inference time. In this paper, we propose PET (Parameter-Efficient knowledge distillation on Transformer), an efficient Transformer compression method that reduces the size of both the encoder and decoder. In PET, we identify and exploit pairs of parameter groups for efficient weight sharing, and employ a warm-up process using a simplified task to increase the gain through Knowledge Distillation. Extensive experiments on five real-world datasets show that PET outperforms existing methods in machine translation tasks. Specifically, on the IWSLT’14 EN→DE task, PET reduces the memory usage by 81.20% and accelerates the inference speed by 45.15% compared to the uncompressed model, with a minor decrease in BLEU score of 0.27.","2023-07-06","2024-03-27 21:18:09","2024-03-27 21:18:09","2024-03-27 21:18:09","e0288060","","7","18","","PLoS One","PET","","","","","","","","","","","","PubMed Central","","PMID: 37410716 PMCID: PMC10325108","","/home/lexi/.zotero-data/storage/QF25PG62/Jeon et al. - 2023 - PET Parameter-efficient Knowledge Distillation on.pdf; ","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10325108/","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MQG24SBL","preprint","2020","Boo, Yoonho; Shin, Sungho; Choi, Jungwook; Sung, Wonyong","Stochastic Precision Ensemble: Self-Knowledge Distillation for Quantized Deep Neural Networks","","","","10.48550/arXiv.2009.14502","http://arxiv.org/abs/2009.14502","The quantization of deep neural networks (QDNNs) has been actively studied for deployment in edge devices. Recent studies employ the knowledge distillation (KD) method to improve the performance of quantized networks. In this study, we propose stochastic precision ensemble training for QDNNs (SPEQ). SPEQ is a knowledge distillation training scheme; however, the teacher is formed by sharing the model parameters of the student network. We obtain the soft labels of the teacher by changing the bit precision of the activation stochastically at each layer of the forward-pass computation. The student model is trained with these soft labels to reduce the activation quantization noise. The cosine similarity loss is employed, instead of the KL-divergence, for KD training. As the teacher model changes continuously by random bit-precision assignment, it exploits the effect of stochastic ensemble KD. SPEQ outperforms the existing quantization training methods in various tasks, such as image classification, question-answering, and transfer learning without the need for cumbersome teacher networks.","2020-09-30","2024-03-27 21:19:39","2024-03-27 21:19:39","2024-03-27 21:19:39","","","","","","","Stochastic Precision Ensemble","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2009.14502 [cs, stat]","","/home/lexi/.zotero-data/storage/TR78WNKF/Boo et al. - 2020 - Stochastic Precision Ensemble Self-Knowledge Dist.pdf; /home/lexi/.zotero-data/storage/HD7QRC98/2009.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2009.14502","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SNC3K5JC","conferencePaper","2022","Kim, Minsoo; Lee, Sihwa; Hong, Suk-Jin; Chang, Du-Seong; Choi, Jungwook","Understanding and Improving Knowledge Distillation for Quantization Aware Training of Large Transformer Encoders","Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing","","","10.18653/v1/2022.emnlp-main.450","https://aclanthology.org/2022.emnlp-main.450","Knowledge distillation (KD) has been a ubiquitous method for model compression to strengthen the capability of a lightweight model with the transferred knowledge from the teacher. In particular, KD has been employed in quantization-aware training (QAT) of Transformer encoders like BERT to improve the accuracy of the student model with the reduced-precision weight parameters. However, little is understood about which of the various KD approaches best fits the QAT of Transformers. In this work, we provide an in-depth analysis of the mechanism of KD on attention recovery of quantized large Transformers. In particular, we reveal that the previously adopted MSE loss on the attention score is insufficient for recovering the self-attention information. Therefore, we propose two KD methods; attention-map and attention-output losses. Furthermore, we explore the unification of both losses to address task-dependent preference between attention-map and output losses. The experimental results on various Transformer encoder models demonstrate that the proposed KD methods achieve state-of-the-art accuracy for QAT with sub-2-bit weight quantization.","2022-12","2024-03-27 21:21:00","2024-03-27 21:21:00","2024-03-27 21:21:00","6713–6725","","","","","","","","","","","Association for Computational Linguistics","Abu Dhabi, United Arab Emirates","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/ENVKTXM8/Kim et al. - 2022 - Understanding and Improving Knowledge Distillation.pdf","","","","Goldberg, Yoav; Kozareva, Zornitsa; Zhang, Yue","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","EMNLP 2022","","","","","","","","","","","","","","",""
"N6QW7HDD","preprint","2023","Zhang, Yifan; Dong, Zhen; Yang, Huanrui; Lu, Ming; Tseng, Cheng-Ching; Du, Yuan; Keutzer, Kurt; Du, Li; Zhang, Shanghang","QD-BEV : Quantization-aware View-guided Distillation for Multi-view 3D Object Detection","","","","10.48550/arXiv.2308.10515","http://arxiv.org/abs/2308.10515","Multi-view 3D detection based on BEV (bird-eye-view) has recently achieved significant improvements. However, the huge memory consumption of state-of-the-art models makes it hard to deploy them on vehicles, and the non-trivial latency will affect the real-time perception of streaming applications. Despite the wide application of quantization to lighten models, we show in our paper that directly applying quantization in BEV tasks will 1) make the training unstable, and 2) lead to intolerable performance degradation. To solve these issues, our method QD-BEV enables a novel view-guided distillation (VGD) objective, which can stabilize the quantization-aware training (QAT) while enhancing the model performance by leveraging both image features and BEV features. Our experiments show that QD-BEV achieves similar or even better accuracy than previous methods with significant efficiency gains. On the nuScenes datasets, the 4-bit weight and 6-bit activation quantized QD-BEV-Tiny model achieves 37.2% NDS with only 15.8 MB model size, outperforming BevFormer-Tiny by 1.8% with an 8x model compression. On the Small and Base variants, QD-BEV models also perform superbly and achieve 47.9% NDS (28.2 MB) and 50.9% NDS (32.9 MB), respectively.","2023-08-21","2024-03-27 21:23:21","2024-03-27 21:23:21","2024-03-27 21:23:21","","","","","","","QD-BEV","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2308.10515 [cs]","","/home/lexi/.zotero-data/storage/76LXAY2D/Zhang et al. - 2023 - QD-BEV  Quantization-aware View-guided Distillati.pdf; /home/lexi/.zotero-data/storage/9NGVDBQ3/2308.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2308.10515","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3I4JT5DZ","preprint","2021","Zi, Bojia; Zhao, Shihao; Ma, Xingjun; Jiang, Yu-Gang","Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better","","","","10.48550/arXiv.2108.07969","http://arxiv.org/abs/2108.07969","Adversarial training is one effective approach for training robust deep neural networks against adversarial attacks. While being able to bring reliable robustness, adversarial training (AT) methods in general favor high capacity models, i.e., the larger the model the better the robustness. This tends to limit their effectiveness on small models, which are more preferable in scenarios where storage or computing resources are very limited (e.g., mobile devices). In this paper, we leverage the concept of knowledge distillation to improve the robustness of small models by distilling from adversarially trained large models. We first revisit several state-of-the-art AT methods from a distillation perspective and identify one common technique that can lead to improved robustness: the use of robust soft labels -- predictions of a robust model. Following this observation, we propose a novel adversarial robustness distillation method called Robust Soft Label Adversarial Distillation (RSLAD) to train robust small student models. RSLAD fully exploits the robust soft labels produced by a robust (adversarially-trained) large teacher model to guide the student's learning on both natural and adversarial examples in all loss terms. We empirically demonstrate the effectiveness of our RSLAD approach over existing adversarial training and distillation methods in improving the robustness of small models against state-of-the-art attacks including the AutoAttack. We also provide a set of understandings on our RSLAD and the importance of robust soft labels for adversarial robustness distillation.","2021-08-18","2024-03-27 21:28:39","2024-03-27 21:28:40","2024-03-27 21:28:39","","","","","","","Revisiting Adversarial Robustness Distillation","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2108.07969 [cs]","","/home/lexi/.zotero-data/storage/2GMUNFNB/Zi et al. - 2021 - Revisiting Adversarial Robustness Distillation Ro.pdf; /home/lexi/.zotero-data/storage/22PC22RP/2108.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2108.07969","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U9GPWFN4","preprint","2022","Maroto, Javier; Ortiz-Jiménez, Guillermo; Frossard, Pascal","On the benefits of knowledge distillation for adversarial robustness","","","","10.48550/arXiv.2203.07159","http://arxiv.org/abs/2203.07159","Knowledge distillation is normally used to compress a big network, or teacher, onto a smaller one, the student, by training it to match its outputs. Recently, some works have shown that robustness against adversarial attacks can also be distilled effectively to achieve good rates of robustness on mobile-friendly models. In this work, however, we take a different point of view, and show that knowledge distillation can be used directly to boost the performance of state-of-the-art models in adversarial robustness. In this sense, we present a thorough analysis and provide general guidelines to distill knowledge from a robust teacher and boost the clean and adversarial performance of a student model even further. To that end, we present Adversarial Knowledge Distillation (AKD), a new framework to improve a model's robust performance, consisting on adversarially training a student on a mixture of the original labels and the teacher outputs. Through carefully controlled ablation studies, we show that using early-stopping, model ensembles and weak adversarial training are key techniques to maximize performance of the student, and show that these insights generalize across different robust distillation techniques. Finally, we provide insights on the effect of robust knowledge distillation on the dynamics of the student network, and show that AKD mostly improves the calibration of the network and modify its training dynamics on samples that the model finds difficult to learn, or even memorize.","2022-03-14","2024-03-27 21:29:45","2024-03-27 21:29:45","2024-03-27 21:29:45","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2203.07159 [cs]","","/home/lexi/.zotero-data/storage/XLRH9C6L/Maroto et al. - 2022 - On the benefits of knowledge distillation for adve.pdf; /home/lexi/.zotero-data/storage/ILZNYFIQ/2203.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2203.07159","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y6YLTKER","journalArticle","2019","Zhao, Sanqiang; Gupta, Raghav; Song, Yang; Zhou, Denny","Extreme Language Model Compression with Optimal Subwords and Shared Projections","","","","","https://openreview.net/forum?id=S1x6ueSKPr","Pre-trained deep neural network language models such as ELMo, GPT, BERT and XLNet have recently achieved state-of-the-art performance on a variety of language understanding tasks. However, their size makes them impractical for a number of scenarios, especially on mobile and edge devices. In particular, the input word embedding matrix accounts for a significant proportion of the model's memory footprint, due to the large input vocabulary and embedding dimensions. Knowledge distillation techniques have had success at compressing large neural network models, but they are ineffective at yielding student models with vocabularies different from the original teacher models. We introduce a novel knowledge distillation technique for training a student model with a significantly smaller vocabulary as well as lower embedding and hidden state dimensions. Specifically, we employ a dual-training mechanism that trains the teacher and student models simultaneously to obtain optimal word embeddings for the student vocabulary. We combine this approach with learning shared projection matrices that transfer layer-wise knowledge from the teacher model to the student model. Our method is able to compress the BERT-BASE model by more than 60x, with only a minor drop in downstream task metrics, resulting in a language model with a footprint of under 7MB. Experimental results also demonstrate higher compression efficiency and accuracy when compared with other state-of-the-art compression techniques.","2019-09-25","2024-03-27 21:30:58","2024-03-27 21:31:01","2024-03-27 21:30:58","","","","","","","","","","","","","","en","","","","","openreview.net","","","","/home/lexi/.zotero-data/storage/PCDR5H6R/Zhao et al. - 2019 - Extreme Language Model Compression with Optimal Su.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LHZJGD89","preprint","2022","López-Cifuentes, Alejandro; Escudero-Viñolo, Marcos; Bescós, Jesús; SanMiguel, Juan C.","Attention-based Knowledge Distillation in Multi-attention Tasks: The Impact of a DCT-driven Loss","","","","10.48550/arXiv.2205.01997","http://arxiv.org/abs/2205.01997","Knowledge Distillation (KD) is a strategy for the definition of a set of transferability gangways to improve the efficiency of Convolutional Neural Networks. Feature-based Knowledge Distillation is a subfield of KD that relies on intermediate network representations, either unaltered or depth-reduced via maximum activation maps, as the source knowledge. In this paper, we propose and analyse the use of a 2D frequency transform of the activation maps before transferring them. We pose that\textemdash by using global image cues rather than pixel estimates, this strategy enhances knowledge transferability in tasks such as scene recognition, defined by strong spatial and contextual relationships between multiple and varied concepts. To validate the proposed method, an extensive evaluation of the state-of-the-art in scene recognition is presented. Experimental results provide strong evidences that the proposed strategy enables the student network to better focus on the relevant image areas learnt by the teacher network, hence leading to better descriptive features and higher transferred performance than every other state-of-the-art alternative. We publicly release the training and evaluation framework used along this paper at http://www-vpu.eps.uam.es/publications/DCTBasedKDForSceneRecognition.","2022-06-06","2024-03-27 21:32:13","2024-03-27 21:32:13","2024-03-27 21:32:13","","","","","","","Attention-based Knowledge Distillation in Multi-attention Tasks","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2205.01997 [cs]","","/home/lexi/.zotero-data/storage/IHZR6S9G/López-Cifuentes et al. - 2022 - Attention-based Knowledge Distillation in Multi-at.pdf; /home/lexi/.zotero-data/storage/QL4B9VAG/2205.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2205.01997","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2U7XYFDF","preprint","2023","Zhou, Chunting; Liu, Pengfei; Xu, Puxin; Iyer, Srini; Sun, Jiao; Mao, Yuning; Ma, Xuezhe; Efrat, Avia; Yu, Ping; Yu, Lili; Zhang, Susan; Ghosh, Gargi; Lewis, Mike; Zettlemoyer, Luke; Levy, Omer","LIMA: Less Is More for Alignment","","","","10.48550/arXiv.2305.11206","http://arxiv.org/abs/2305.11206","Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.","2023-05-18","2024-03-27 21:35:37","2024-03-27 21:36:14","2024-03-27 21:35:37","","","","","","","LIMA","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2305.11206 [cs]","","/home/lexi/.zotero-data/storage/YDUT2LE4/Zhou et al. - 2023 - LIMA Less Is More for Alignment.pdf; /home/lexi/.zotero-data/storage/R79GT6AC/2305.html","","soon","","","","","","","","","","","","","","","","","","","","arXiv:2305.11206","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"998F43RE","preprint","2020","Tan, Mingxing; Le, Quoc V.","EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks","","","","10.48550/arXiv.1905.11946","http://arxiv.org/abs/1905.11946","Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.","2020-09-11","2024-03-28 14:09:02","2024-03-28 14:09:18","2024-03-28 14:09:02","","","","","","","EfficientNet","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1905.11946 [cs, stat]","","/home/lexi/.zotero-data/storage/PPSYLMRQ/Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf; /home/lexi/.zotero-data/storage/LK2ZR4JN/1905.html","","soon; base","","","","","","","","","","","","","","","","","","","","arXiv:1905.11946","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IL9BMGM3","preprint","2019","Elsken, Thomas; Metzen, Jan Hendrik; Hutter, Frank","Neural Architecture Search: A Survey","","","","10.48550/arXiv.1808.05377","http://arxiv.org/abs/1808.05377","Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.","2019-04-26","2024-03-28 14:10:57","2024-03-28 14:18:04","2024-03-28 14:10:57","","","","","","","Neural Architecture Search","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1808.05377 [cs, stat]","","/home/lexi/.zotero-data/storage/VYPSBDYZ/Elsken et al. - 2019 - Neural Architecture Search A Survey.pdf; /home/lexi/.zotero-data/storage/PI6PYJTD/1808.html","","soon","","","","","","","","","","","","","","","","","","","","arXiv:1808.05377","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XZ39MUTT","preprint","2023","White, Colin; Safari, Mahmoud; Sukthanker, Rhea; Ru, Binxin; Elsken, Thomas; Zela, Arber; Dey, Debadeepta; Hutter, Frank","Neural Architecture Search: Insights from 1000 Papers","","","","10.48550/arXiv.2301.08727","http://arxiv.org/abs/2301.08727","In the past decade, advances in deep learning have resulted in breakthroughs in a variety of areas, including computer vision, natural language understanding, speech recognition, and reinforcement learning. Specialized, high-performing neural architectures are crucial to the success of deep learning in these areas. Neural architecture search (NAS), the process of automating the design of neural architectures for a given task, is an inevitable next step in automating machine learning and has already outpaced the best human-designed architectures on many tasks. In the past few years, research in NAS has been progressing rapidly, with over 1000 papers released since 2020 (Deng and Lindauer, 2021). In this survey, we provide an organized and comprehensive guide to neural architecture search. We give a taxonomy of search spaces, algorithms, and speedup techniques, and we discuss resources such as benchmarks, best practices, other surveys, and open-source libraries.","2023-01-25","2024-03-28 14:11:07","2024-03-28 14:17:59","2024-03-28 14:11:07","","","","","","","Neural Architecture Search","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2301.08727 [cs, stat]","","/home/lexi/.zotero-data/storage/5KEWGFXG/White et al. - 2023 - Neural Architecture Search Insights from 1000 Pap.pdf; /home/lexi/.zotero-data/storage/PCCG3UCM/2301.html","","soon","","","","","","","","","","","","","","","","","","","","arXiv:2301.08727","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9MEN3GJX","preprint","2020","White, Colin; Neiswanger, Willie; Savani, Yash","BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search","","","","10.48550/arXiv.1910.11858","http://arxiv.org/abs/1910.11858","Over the past half-decade, many methods have been considered for neural architecture search (NAS). Bayesian optimization (BO), which has long had success in hyperparameter optimization, has recently emerged as a very promising strategy for NAS when it is coupled with a neural predictor. Recent work has proposed different instantiations of this framework, for example, using Bayesian neural networks or graph convolutional networks as the predictive model within BO. However, the analyses in these papers often focus on the full-fledged NAS algorithm, so it is difficult to tell which individual components of the framework lead to the best performance. In this work, we give a thorough analysis of the ""BO + neural predictor"" framework by identifying five main components: the architecture encoding, neural predictor, uncertainty calibration method, acquisition function, and acquisition optimization strategy. We test several different methods for each component and also develop a novel path-based encoding scheme for neural architectures, which we show theoretically and empirically scales better than other encodings. Using all of our analyses, we develop a final algorithm called BANANAS, which achieves state-of-the-art performance on NAS search spaces. We adhere to the NAS research checklist (Lindauer and Hutter 2019) to facilitate best practices, and our code is available at https://github.com/naszilla/naszilla.","2020-11-02","2024-03-28 14:35:32","2024-03-28 14:35:32","2024-03-28 14:35:32","","","","","","","BANANAS","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1910.11858 [cs, stat]","","/home/lexi/.zotero-data/storage/2MFBTM8B/White et al. - 2020 - BANANAS Bayesian Optimization with Neural Archite.pdf; /home/lexi/.zotero-data/storage/3K7CEHRZ/1910.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1910.11858","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5Y73QFR7","preprint","2019","Kandasamy, Kirthevasan; Neiswanger, Willie; Schneider, Jeff; Poczos, Barnabas; Xing, Eric","Neural Architecture Search with Bayesian Optimisation and Optimal Transport","","","","10.48550/arXiv.1802.07191","http://arxiv.org/abs/1802.07191","Bayesian Optimisation (BO) refers to a class of methods for global optimisation of a function $f$ which is only accessible via point evaluations. It is typically used in settings where $f$ is expensive to evaluate. A common use case for BO in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional BO methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network \emph{architectures}. In this work, we develop NASBOT, a Gaussian process based BO framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of BO. We demonstrate that NASBOT outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.","2019-03-15","2024-03-28 14:35:43","2024-03-28 14:35:43","2024-03-28 14:35:43","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1802.07191 [cs, stat]","","/home/lexi/.zotero-data/storage/BHD5ZKR9/Kandasamy et al. - 2019 - Neural Architecture Search with Bayesian Optimisat.pdf; /home/lexi/.zotero-data/storage/YA2BSIEF/1802.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1802.07191","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UDF7YY3A","preprint","2019","Zhou, Hongpeng; Yang, Minghao; Wang, Jun; Pan, Wei","BayesNAS: A Bayesian Approach for Neural Architecture Search","","","","10.48550/arXiv.1905.04919","http://arxiv.org/abs/1905.04919","One-Shot Neural Architecture Search (NAS) is a promising method to significantly reduce search time without any separate training. It can be treated as a Network Compression problem on the architecture parameters from an over-parameterized network. However, there are two issues associated with most one-shot NAS methods. First, dependencies between a node and its predecessors and successors are often disregarded which result in improper treatment over zero operations. Second, architecture parameters pruning based on their magnitude is questionable. In this paper, we employ the classic Bayesian learning approach to alleviate these two issues by modeling architecture parameters using hierarchical automatic relevance determination (HARD) priors. Unlike other NAS methods, we train the over-parameterized network for only one epoch then update the architecture. Impressively, this enabled us to find the architecture on CIFAR-10 within only 0.2 GPU days using a single GPU. Competitive performance can be also achieved by transferring to ImageNet. As a byproduct, our approach can be applied directly to compress convolutional neural networks by enforcing structural sparsity which achieves extremely sparse networks without accuracy deterioration.","2019-06-07","2024-03-28 14:35:51","2024-03-28 14:35:51","2024-03-28 14:35:51","","","","","","","BayesNAS","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1905.04919 [cs, stat]","","/home/lexi/.zotero-data/storage/VVPQ8S94/Zhou et al. - 2019 - BayesNAS A Bayesian Approach for Neural Architect.pdf; /home/lexi/.zotero-data/storage/AWXFKIMU/1905.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1905.04919","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X2783EZZ","preprint","2018","Hsu, Chi-Hung; Chang, Shu-Huan; Liang, Jhao-Hong; Chou, Hsin-Ping; Liu, Chun-Hao; Chang, Shih-Chieh; Pan, Jia-Yu; Chen, Yu-Ting; Wei, Wei; Juan, Da-Cheng","MONAS: Multi-Objective Neural Architecture Search using Reinforcement Learning","","","","10.48550/arXiv.1806.10332","http://arxiv.org/abs/1806.10332","Recent studies on neural architecture search have shown that automatically designed neural networks perform as good as expert-crafted architectures. While most existing works aim at finding architectures that optimize the prediction accuracy, these architectures may have complexity and is therefore not suitable being deployed on certain computing environment (e.g., with limited power budgets). We propose MONAS, a framework for Multi-Objective Neural Architectural Search that employs reward functions considering both prediction accuracy and other important objectives (e.g., power consumption) when searching for neural network architectures. Experimental results showed that, compared to the state-ofthe-arts, models found by MONAS achieve comparable or better classification accuracy on computer vision applications, while satisfying the additional objectives such as peak power.","2018-12-03","2024-03-28 14:36:16","2024-03-28 14:36:16","2024-03-28 14:36:16","","","","","","","MONAS","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1806.10332 [cs, stat]","","/home/lexi/.zotero-data/storage/SPN8K8GX/Hsu et al. - 2018 - MONAS Multi-Objective Neural Architecture Search .pdf; /home/lexi/.zotero-data/storage/8477GRYL/1806.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1806.10332","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J6MLGLKU","preprint","2019","Liu, Hanxiao; Simonyan, Karen; Yang, Yiming","DARTS: Differentiable Architecture Search","","","","10.48550/arXiv.1806.09055","http://arxiv.org/abs/1806.09055","This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.","2019-04-23","2024-03-28 14:40:13","2024-03-28 14:40:13","2024-03-28 14:40:13","","","","","","","DARTS","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1806.09055 [cs, stat]","","/home/lexi/.zotero-data/storage/BI29U9FI/Liu et al. - 2019 - DARTS Differentiable Architecture Search.pdf; /home/lexi/.zotero-data/storage/SQ9CQTDJ/1806.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1806.09055","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EKQ6MH5A","journalArticle","2023","Ali, Sarwat; Wani, M. Arif","Gradient-Based Neural Architecture Search: A Comprehensive Evaluation","Machine Learning and Knowledge Extraction","","2504-4990","10.3390/make5030060","https://www.mdpi.com/2504-4990/5/3/60","One of the challenges in deep learning involves discovering the optimal architecture for a specific task. This is effectively tackled through Neural Architecture Search (NAS). Neural Architecture Search encompasses three prominent approaches—reinforcement learning, evolutionary algorithms, and gradient descent—that have demonstrated noteworthy potential in identifying good candidate architectures. However, approaches based on reinforcement learning and evolutionary algorithms often necessitate extensive computational resources, requiring hundreds of GPU days or more. Therefore, we confine this work to a gradient-based approach due to its lower computational resource demands. Our objective encompasses identifying the optimal gradient-based NAS method and pinpointing opportunities for future enhancements. To achieve this, a comprehensive evaluation of the use of four major Gradient descent-based architecture search methods for discovering the best neural architecture for image classification tasks is provided. An overview of these gradient-based methods, i.e., DARTS, PDARTS, Fair DARTS and Att-DARTS, is presented. A theoretical comparison, based on search spaces, continuous relaxation strategy and bi-level optimization, for deriving the best neural architecture is then provided. The strong and weak features of these methods are also listed. Experimental results for comparing the error rate and computational cost of these gradient-based methods are analyzed. These experiments involved using bench marking datasets CIFAR-10, CIFAR-100 and ImageNet. The results show that PDARTS is better and faster among the examined methods, making it a potent candidate for automating Neural Architecture Search. By effectively conducting a comparative analysis, our research provides valuable insights and future research directions to address the criticism and gaps in the literature.","2023-09","2024-03-28 14:41:07","2024-03-28 14:41:07","2024-03-28 14:41:07","1176-1194","","3","5","","","Gradient-Based Neural Architecture Search","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","Number: 3 Publisher: Multidisciplinary Digital Publishing Institute","","/home/lexi/.zotero-data/storage/U4LVB6PS/Ali and Wani - 2023 - Gradient-Based Neural Architecture Search A Compr.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2FRRW53B","preprint","2019","Jin, Haifeng; Song, Qingquan; Hu, Xia","Auto-Keras: An Efficient Neural Architecture Search System","","","","10.48550/arXiv.1806.10282","http://arxiv.org/abs/1806.10282","Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Intensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.","2019-03-26","2024-03-28 14:42:09","2024-03-28 14:42:09","2024-03-28 14:42:09","","","","","","","Auto-Keras","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1806.10282 [cs, stat]","","/home/lexi/.zotero-data/storage/E7VMEHEN/Jin et al. - 2019 - Auto-Keras An Efficient Neural Architecture Searc.pdf; /home/lexi/.zotero-data/storage/4HFXL9L2/1806.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1806.10282","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M4A7HQTE","preprint","2020","Xie, Sirui; Zheng, Hehui; Liu, Chunxiao; Lin, Liang","SNAS: Stochastic Neural Architecture Search","","","","10.48550/arXiv.1812.09926","http://arxiv.org/abs/1812.09926","We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets. We have released our implementation at https://github.com/SNAS-Series/SNAS-Series.","2020-03-31","2024-03-28 16:02:14","2024-03-28 16:02:14","2024-03-28 16:02:14","","","","","","","SNAS","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1812.09926 [cs, stat]","","/home/lexi/.zotero-data/storage/JPIUZK7J/Xie et al. - 2020 - SNAS Stochastic Neural Architecture Search.pdf; /home/lexi/.zotero-data/storage/TCLLLMH4/1812.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1812.09926","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ERZDE4H4","preprint","2018","Pham, Hieu; Guan, Melody Y.; Zoph, Barret; Le, Quoc V.; Dean, Jeff","Efficient Neural Architecture Search via Parameter Sharing","","","","10.48550/arXiv.1802.03268","http://arxiv.org/abs/1802.03268","We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.","2018-02-11","2024-03-28 16:05:11","2024-03-28 16:05:11","2024-03-28 16:05:11","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1802.03268 [cs, stat]","","/home/lexi/.zotero-data/storage/AXF9HLBH/Pham et al. - 2018 - Efficient Neural Architecture Search via Parameter.pdf; /home/lexi/.zotero-data/storage/J6IWC2DA/1802.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1802.03268","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F6HMR9MR","preprint","2020","Cai, Han; Gan, Chuang; Wang, Tianzhe; Zhang, Zhekai; Han, Song","Once-for-All: Train One Network and Specialize it for Efficient Deployment","","","","10.48550/arXiv.1908.09791","http://arxiv.org/abs/1908.09791","We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all.","2020-04-29","2024-03-28 16:16:12","2024-03-28 16:16:12","2024-03-28 16:16:12","","","","","","","Once-for-All","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1908.09791 [cs, stat]","","/home/lexi/.zotero-data/storage/7ZX6YEBA/Cai et al. - 2020 - Once-for-All Train One Network and Specialize it .pdf; /home/lexi/.zotero-data/storage/JEVMS6VN/1908.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1908.09791","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EQDZGIG8","preprint","2018","Vanschoren, Joaquin","Meta-Learning: A Survey","","","","10.48550/arXiv.1810.03548","http://arxiv.org/abs/1810.03548","Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.","2018-10-08","2024-03-28 16:21:19","2024-03-28 16:21:19","2024-03-28 16:21:19","","","","","","","Meta-Learning","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1810.03548 [cs, stat]","","/home/lexi/.zotero-data/storage/XY6IZVFZ/Vanschoren - 2018 - Meta-Learning A Survey.pdf; /home/lexi/.zotero-data/storage/PJV2B9MW/1810.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1810.03548","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"24VVUKA2","preprint","2020","Hospedales, Timothy; Antoniou, Antreas; Micaelli, Paul; Storkey, Amos","Meta-Learning in Neural Networks: A Survey","","","","10.48550/arXiv.2004.05439","http://arxiv.org/abs/2004.05439","The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.","2020-11-07","2024-03-28 16:21:24","2024-03-28 16:21:24","2024-03-28 16:21:24","","","","","","","Meta-Learning in Neural Networks","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2004.05439 [cs, stat]","","/home/lexi/.zotero-data/storage/APC2TGFF/Hospedales et al. - 2020 - Meta-Learning in Neural Networks A Survey.pdf; /home/lexi/.zotero-data/storage/XYQRTRRU/2004.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2004.05439","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LMMZAGVB","conferencePaper","2022","Lee, Hung-yi; Li, Shang-Wen; Vu, Thang","Meta Learning for Natural Language Processing: A Survey","Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","","","10.18653/v1/2022.naacl-main.49","https://aclanthology.org/2022.naacl-main.49","Deep learning has been the mainstream technique in the natural language processing (NLP) area. However, deep learning requires many labeled data and is less generalizable across domains. Meta-learning is an arising field in machine learning. It studies approaches to learning better learning algorithms and aims to improve algorithms in various aspects, including data efficiency and generalizability. The efficacy of meta-learning has been shown in many NLP tasks, but there is no systematic survey of these approaches in NLP, which hinders more researchers from joining the field. Our goal with this survey paper is to offer researchers pointers to relevant meta-learning works in NLP and attract more attention from the NLP community to drive future innovation. This paper first introduces the general concepts of meta-learning and the common approaches. Then we summarize task construction settings, applications of meta-learning for various NLP problems and review the development of meta-learning in the NLP community.","2022-07","2024-03-28 16:21:29","2024-03-28 16:21:29","2024-03-28 16:21:29","666–684","","","","","","Meta Learning for Natural Language Processing","","","","","Association for Computational Linguistics","Seattle, United States","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/PALGQY5F/Lee et al. - 2022 - Meta Learning for Natural Language Processing A S.pdf","","","","Carpuat, Marine; de Marneffe, Marie-Catherine; Meza Ruiz, Ivan Vladimir","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NAACL-HLT 2022","","","","","","","","","","","","","","",""
"WQ365TQ7","book","2019","","Automated Machine Learning: Methods, Systems, Challenges","","978-3-030-05317-8 978-3-030-05318-5","","","http://link.springer.com/10.1007/978-3-030-05318-5","","2019","2024-03-28 16:23:08","2024-03-28 16:23:09","2024-03-28 16:23:08","","","","","","","Automated Machine Learning","The Springer Series on Challenges in Machine Learning","","","","Springer International Publishing","Cham","en","https://creativecommons.org/licenses/by/4.0","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-030-05318-5","","/home/lexi/.zotero-data/storage/NDPMEQER/Hutter et al. - 2019 - Automated Machine Learning Methods, Systems, Chal.pdf","","","","Hutter, Frank; Kotthoff, Lars; Vanschoren, Joaquin","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RVKYRZ49","preprint","2019","Ying, Chris; Klein, Aaron; Real, Esteban; Christiansen, Eric; Murphy, Kevin; Hutter, Frank","NAS-Bench-101: Towards Reproducible Neural Architecture Search","","","","10.48550/arXiv.1902.09635","http://arxiv.org/abs/1902.09635","Recent advances in neural architecture search (NAS) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers without access to large-scale computation. We aim to ameliorate these problems by introducing NAS-Bench-101, the first public architecture dataset for NAS research. To build NAS-Bench-101, we carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the pre-computed dataset. We demonstrate its utility by analyzing the dataset as a whole and by benchmarking a range of architecture optimization algorithms.","2019-05-14","2024-03-28 16:33:30","2024-03-28 16:33:30","2024-03-28 16:33:30","","","","","","","NAS-Bench-101","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1902.09635 [cs, stat]","","/home/lexi/.zotero-data/storage/NEL47N57/Ying et al. - 2019 - NAS-Bench-101 Towards Reproducible Neural Archite.pdf; /home/lexi/.zotero-data/storage/U8EJSXYD/1902.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1902.09635","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3GUCUJYV","preprint","2021","Elsken, Thomas; Staffler, Benedikt; Zela, Arber; Metzen, Jan Hendrik; Hutter, Frank","Bag of Tricks for Neural Architecture Search","","","","10.48550/arXiv.2107.03719","http://arxiv.org/abs/2107.03719","While neural architecture search methods have been successful in previous years and led to new state-of-the-art performance on various problems, they have also been criticized for being unstable, being highly sensitive with respect to their hyperparameters, and often not performing better than random search. To shed some light on this issue, we discuss some practical considerations that help improve the stability, efficiency and overall performance.","2021-07-08","2024-03-28 16:34:26","2024-03-28 16:34:26","2024-03-28 16:34:26","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2107.03719 [cs, stat]","","/home/lexi/.zotero-data/storage/VFNZQ833/Elsken et al. - 2021 - Bag of Tricks for Neural Architecture Search.pdf; /home/lexi/.zotero-data/storage/9Y8KVTNA/2107.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2107.03719","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P2UTQHKV","preprint","2023","Chitty-Venkata, Krishna Teja; Mittal, Sparsh; Emani, Murali; Vishwanath, Venkatram; Somani, Arun K.","A Survey of Techniques for Optimizing Transformer Inference","","","","10.48550/arXiv.2307.07982","http://arxiv.org/abs/2307.07982","Recent years have seen a phenomenal rise in performance and applications of transformer neural networks. The family of transformer networks, including Bidirectional Encoder Representations from Transformer (BERT), Generative Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their effectiveness across Natural Language Processing (NLP) and Computer Vision (CV) domains. Transformer-based networks such as ChatGPT have impacted the lives of common men. However, the quest for high predictive performance has led to an exponential increase in transformers' memory and compute footprint. Researchers have proposed techniques to optimize transformer inference at all levels of abstraction. This paper presents a comprehensive survey of techniques for optimizing the inference phase of transformer networks. We survey techniques such as knowledge distillation, pruning, quantization, neural architecture search and lightweight network design at the algorithmic level. We further review hardware-level optimization techniques and the design of novel hardware accelerators for transformers. We summarize the quantitative results on the number of parameters/FLOPs and accuracy of several models/techniques to showcase the tradeoff exercised by them. We also outline future directions in this rapidly evolving field of research. We believe that this survey will educate both novice and seasoned researchers and also spark a plethora of research efforts in this field.","2023-07-16","2024-03-28 20:30:14","2024-03-28 20:30:16","2024-03-28 20:30:14","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2307.07982 [cs]","","/home/lexi/.zotero-data/storage/UUVGQV9V/Chitty-Venkata et al. - 2023 - A Survey of Techniques for Optimizing Transformer .pdf; /home/lexi/.zotero-data/storage/URH48CID/2307.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2307.07982","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RBX4UN4E","preprint","2023","Kim, Sehoon; Hooper, Coleman; Wattanawong, Thanakul; Kang, Minwoo; Yan, Ruohan; Genc, Hasan; Dinh, Grace; Huang, Qijing; Keutzer, Kurt; Mahoney, Michael W.; Shao, Yakun Sophia; Gholami, Amir","Full Stack Optimization of Transformer Inference: a Survey","","","","10.48550/arXiv.2302.14017","http://arxiv.org/abs/2302.14017","Recent advances in state-of-the-art DNN architecture design have been moving toward Transformer models. These models achieve superior accuracy across a wide range of applications. This trend has been consistent over the past several years since Transformer models were originally introduced. However, the amount of compute and bandwidth required for inference of recent Transformer models is growing at a significant rate, and this has made their deployment in latency-sensitive applications challenging. As such, there has been an increased focus on making Transformer models more efficient, with methods that range from changing the architecture design, all the way to developing dedicated domain-specific accelerators. In this work, we survey different approaches for efficient Transformer inference, including: (i) analysis and profiling of the bottlenecks in existing Transformer architectures and their similarities and differences with previous convolutional models; (ii) implications of Transformer architecture on hardware, including the impact of non-linear operations such as Layer Normalization, Softmax, and GELU, as well as linear operations, on hardware design; (iii) approaches for optimizing a fixed Transformer architecture; (iv) challenges in finding the right mapping and scheduling of operations for Transformer models; and (v) approaches for optimizing Transformer models by adapting the architecture using neural architecture search. Finally, we perform a case study by applying the surveyed optimizations on Gemmini, the open-source, full-stack DNN accelerator generator, and we show how each of these approaches can yield improvements, compared to previous benchmark results on Gemmini. Among other things, we find that a full-stack co-design approach with the aforementioned methods can result in up to 88.7x speedup with a minimal performance degradation for Transformer inference.","2023-02-27","2024-03-28 20:31:10","2024-03-28 20:31:10","2024-03-28 20:31:10","","","","","","","Full Stack Optimization of Transformer Inference","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2302.14017 [cs]","","/home/lexi/.zotero-data/storage/R5RZVJ8R/Kim et al. - 2023 - Full Stack Optimization of Transformer Inference .pdf; /home/lexi/.zotero-data/storage/C7PJW8LF/2302.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2302.14017","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X87BJL7I","preprint","2024","Baan, Joris; Fernández, Raquel; Plank, Barbara; Aziz, Wilker","Interpreting Predictive Probabilities: Model Confidence or Human Label Variation?","","","","10.48550/arXiv.2402.16102","http://arxiv.org/abs/2402.16102","With the rise of increasingly powerful and user-facing NLP systems, there is growing interest in assessing whether they have a good representation of uncertainty by evaluating the quality of their predictive distribution over outcomes. We identify two main perspectives that drive starkly different evaluation protocols. The first treats predictive probability as an indication of model confidence; the second as an indication of human label variation. We discuss their merits and limitations, and take the position that both are crucial for trustworthy and fair NLP systems, but that exploiting a single predictive distribution is limiting. We recommend tools and highlight exciting directions towards models with disentangled representations of uncertainty about predictions and uncertainty about human labels.","2024-02-25","2024-03-28 21:43:09","2024-03-28 21:43:21","2024-03-28 21:43:09","","","","","","","Interpreting Predictive Probabilities","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2402.16102 [cs]","","/home/lexi/.zotero-data/storage/K9C4DRLE/Baan et al. - 2024 - Interpreting Predictive Probabilities Model Confi.pdf; /home/lexi/.zotero-data/storage/YWG4DI6Q/2402.html","","soon","","","","","","","","","","","","","","","","","","","","arXiv:2402.16102","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YKAUAKCN","webpage","2021","","On Getting Confidence Estimates from Neural Networks","Bharath's notes","","","","https://bharathpbhat.github.io/2021/04/04/getting-confidence-estimates-from-neural-networks.html","I've been reading on and off about this topic for a while now, and it comes up very often in applied ML. How confident is the neural network about a particular prediction it makes? Can we switch to a different end user experience when the network is not confident? In a typical scenario, a model is trained on a limited dataset and gets deployed in a product, where unless the user experience is strictly guarded, the model will get hit with inputs that were not anticipated. This is the problem of Out-Of-Distribution (OOD) detection - and if a model can either say that it does not ""know"" what to do in such cases, or equivalently, make a low confidence prediction - then that is useful for avoiding catastrophic failures.","2021-04-04","2024-03-28 21:44:38","2024-03-28 21:44:38","2024-03-28 21:44:38","","","","","","","","","","","","","","en","","","","","","","","","/home/lexi/.zotero-data/storage/3R2Y4SE3/getting-confidence-estimates-from-neural-networks.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JJUV6JB","preprint","2023","Vasilev, Ruslan; D'yakonov, Alexander","Calibration of Neural Networks","","","","10.48550/arXiv.2303.10761","http://arxiv.org/abs/2303.10761","Neural networks solving real-world problems are often required not only to make accurate predictions but also to provide a confidence level in the forecast. The calibration of a model indicates how close the estimated confidence is to the true probability. This paper presents a survey of confidence calibration problems in the context of neural networks and provides an empirical comparison of calibration methods. We analyze problem statement, calibration definitions, and different approaches to evaluation: visualizations and scalar measures that estimate whether the model is well-calibrated. We review modern calibration techniques: based on post-processing or requiring changes in training. Empirical experiments cover various datasets and models, comparing calibration methods according to different criteria.","2023-03-19","2024-03-28 21:45:27","2024-03-28 21:45:27","2024-03-28 21:45:27","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2303.10761 [cs, stat]","","/home/lexi/.zotero-data/storage/5AUZG77K/Vasilev and D'yakonov - 2023 - Calibration of Neural Networks.pdf; /home/lexi/.zotero-data/storage/CBE3KQAE/2303.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2303.10761","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VDZ7C6VN","preprint","2017","Guo, Chuan; Pleiss, Geoff; Sun, Yu; Weinberger, Kilian Q.","On Calibration of Modern Neural Networks","","","","10.48550/arXiv.1706.04599","http://arxiv.org/abs/1706.04599","Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.","2017-08-03","2024-03-28 21:45:33","2024-03-28 21:45:33","2024-03-28 21:45:33","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1706.04599 [cs]","","/home/lexi/.zotero-data/storage/BUGU2EVX/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf; /home/lexi/.zotero-data/storage/2YZJWSW2/1706.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1706.04599","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KEW35NL9","preprint","2024","Cui, Yiming; Yang, Ziqing; Yao, Xin","Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca","","","","10.48550/arXiv.2304.08177","http://arxiv.org/abs/2304.08177","Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically transformed natural language processing research and shown promising strides towards Artificial General Intelligence (AGI). Nonetheless, the high costs associated with training and deploying LLMs present substantial obstacles to transparent, accessible academic research. While several large language models, such as LLaMA, have been open-sourced by the community, these predominantly focus on English corpora, limiting their usefulness for other languages. In this paper, we propose a method to augment LLaMA with capabilities for understanding and generating Chinese text and its ability to follow instructions. We achieve this by extending LLaMA's existing vocabulary with an additional 20,000 Chinese tokens, thereby improving its encoding efficiency and semantic understanding of Chinese. We further incorporate secondary pre-training using Chinese data and fine-tune the model with Chinese instruction datasets, significantly enhancing the model's ability to comprehend and execute instructions. Our experimental results indicate that the newly proposed model markedly enhances the original LLaMA's proficiency in understanding and generating Chinese content. Additionally, the results on the C-Eval dataset yield competitive performance among the models with several times the size of ours. We have made our pre-trained models, training scripts, and other resources available through GitHub, fostering open research for our community. Chinese LLaMA series: \url{https://github.com/ymcui/Chinese-LLaMA-Alpaca} and Chinese Llama-2 series: \url{https://github.com/ymcui/Chinese-LLaMA-Alpaca-2}","2024-02-22","2024-05-20 15:37:34","2024-05-20 15:37:34","2024-05-20 15:37:34","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2304.08177 [cs]","","/home/lexi/.zotero-data/storage/JX7RRP9H/Cui et al. - 2024 - Efficient and Effective Text Encoding for Chinese .pdf; /home/lexi/.zotero-data/storage/EWLMIKD2/2304.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2304.08177","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JPPYWU6Q","preprint","2023","Muennighoff, Niklas; Tazi, Nouamane; Magne, Loïc; Reimers, Nils","MTEB: Massive Text Embedding Benchmark","","","","10.48550/arXiv.2210.07316","http://arxiv.org/abs/2210.07316","Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.","2023-03-19","2024-05-20 15:38:14","2024-05-20 15:38:14","2024-05-20 15:38:14","","","","","","","MTEB","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2210.07316 [cs]","","/home/lexi/.zotero-data/storage/Y2E664HP/Muennighoff et al. - 2023 - MTEB Massive Text Embedding Benchmark.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2210.07316","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5VF6YA9Z","journalArticle","2021","Soderholm, Justin","Investigating information structure and word order in Latin poetry: An analysis of epigrams","Theses and Dissertations","","","","https://commons.und.edu/theses/4193","This thesis examines the epigrammatic verses of the Latin poet Marcus Valerius Martialis (Martial). The emphasis is on the order of basic constituents and information structure, in particular, the concepts of sentence articulation and focus structure as modelled by Lambrecht (1994). I categorize clauses from roughly 200 lines of epigrammatic verse by the sentence articulations topic-comment, in which propositions have predicate focus, identificational, in which propositions have argument focus, and thetic, in which whole propositions are in focus. For each articulation, I also present various examples demonstrating the variety of word orders in each. Additionally, this study examines some patterns found in clause pairs with contrastive focus. The results of this study demonstrate that epigrams frequently exemplify noncanonical word orders (i.e., marked word orders) in all three types of sentence articulations. Indeed, in the data analyzed, non-canonical orders are more common than canonical word orders. Topic-comment propositions have the closest percentage comparison between canonical and non-canonical, with 33% canonical and 37% non-canonical. Identificational p ropositions a re c anonical 4 0% o f t he t ime a nd n on-canonical 5 2% o f the time. Thetic (presentational and event-reporting) propositions are canonical in 11% of cases and non-canonical in 81% of cases. For each sentence articulation, the remaining percentage of clauses contain only a verb, and thus are not categorized by canonical and non-canonical orders. Out of the roughly 200 lines of epigrammatic verse, the canonical SOV (subject-object-verb) order, where all three constituents are explicit, occurs two times in main clauses.","2021-12-01","2024-06-07 19:18:25","2024-06-07 19:43:20","","","","","","","","Investigating information structure and word order in Latin poetry","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/QQ2CDCIZ/4193.html; /home/lexi/.zotero-data/storage/NRSVMSIN/Soderholm - 2021 - Investigating information structure and word order.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZR82343A","book","2007","Krifka, Manfred","Basic notions of information structure","","978-3-939469-88-9","","","https://publishup.uni-potsdam.de/frontdoor/index/index/docId/1771","This article takes stock of the basic notions of Information Structure (IS). It first provides a general characterization of IS — following Chafe (1976) — within a communicative model of Common Ground(CG), which distinguishes between CG content and CG management. IS is concerned with those features of language that concern the local CG. Second, this paper defines and discusses the notions of Focus (as indicating alternatives) and its various uses, Givenness (as indicating that a denotation is already present in the CG), and Topic (as specifying what a statement is about). It also proposes a new notion, Delimitation, which comprises contrastive topics and frame setters, and indicates that the current conversational move does not entirely satisfy the local communicative needs. It also points out that rhetorical structuring partly belongs to IS.","2007","2024-06-07 20:07:33","2024-06-07 20:07:33","2024-06-07 20:07:33","","","","","","","","","","","","Universitätsverlag Potsdam","","eng","https://rightsstatements.org/vocab/InC/1.0/","","","","publishup.uni-potsdam.de","","ISSN: 1614-4708, 1866-4725 Pages: 13-55","","/home/lexi/.zotero-data/storage/7Y42BZ3F/Krifka - 2007 - Basic notions of information structure.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"24J4XCLB","book","2019","Devine, A. M.; Stephens, Laurence D.; Devine, A. M.; Stephens, Laurence D.","Pragmatics for Latin: From Syntax to Information Structure","","978-0-19-093947-2","","","","Latin is often described as a free word order language, but in general each word order encodes a particular information structure: in that sense, each word order has a different meaning. Pragmatics for Latin provides a descriptive analysis of Latin information structure based on detailed philological evidence and elaborates a syntax-pragmatics interface that formalizes the informational content of the various different word orders. Using a slightly adjusted version of the structured meanings theory, the book shows how the pragmatic meanings matching the different word orders arise naturally and spontaneously out of the compositional process as an integral part of a single semantic derivation covering denotational and informational meaning at one and the same time.","2019-03-21","2024-06-08 14:46:38","2024-06-08 14:50:57","","","248","","","","","Pragmatics for Latin","","","","","Oxford University Press","Oxford, New York","","","","","","Oxford University Press","","","","/home/lexi/.zotero-data/storage/K764M4G4/Devine et al. - 2019 - Pragmatics for Latin From Syntax to Information S.pdf; /home/lexi/.zotero-data/storage/DAPY8VXB/pragmatics-for-latin-9780190939472.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FUHFFADW","journalArticle","1968","Pearce, T. E. V.","A Pattern of Word Order in Latin Poetry","The Classical Quarterly","","1471-6844, 0009-8388","10.1017/S0009838800022175","https://www.cambridge.org/core/journals/classical-quarterly/article/abs/pattern-of-word-order-in-latin-poetry/57195553D90A6CD19F04F8635E210D3C","In each example an adjective is separated from its noun by a verb and an unqualified noun. The separation by the verb may be regarded as conditioned by the metre, but not the further separation by the unqualified noun, as the qualified and unqualified nouns are metrically interchangeable. Horace would appear to prefer the wider separation to the less wide.","1968-12","2024-06-09 12:24:53","2024-06-09 12:24:53","2024-06-09 12:24:53","334-354","","2","18","","","","","","","","","","en","","","","","Cambridge University Press","","","","/home/lexi/.zotero-data/storage/RYEIGTW9/Pearce - 1968 - A Pattern of Word Order in Latin Poetry.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QA6MSIL8","journalArticle","1979","Tsur, Reuven","Levels of Information Processing in Reading Poetry","Critical Inquiry","","0093-1896","10.1086/448020","https://www.journals.uchicago.edu/doi/abs/10.1086/448020","","1979-07","2024-06-09 12:26:48","2024-06-09 12:26:48","2024-06-09 12:26:48","751-759","","4","5","","","","","","","","","","","","","","","journals.uchicago.edu (Atypon)","","Publisher: The University of Chicago Press","","/home/lexi/.zotero-data/storage/78JPR6IM/Tsur - 1979 - Levels of Information Processing in Reading Poetry.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HIKQRD2Y","journalArticle","2010","Wagner, Michael; McCurdy, Katherine","Poetic rhyme reflects cross-linguistic differences in information structure","Cognition","","0010-0277","10.1016/j.cognition.2010.08.007","https://www.sciencedirect.com/science/article/pii/S0010027710001769","Identical rhymes (right/write, attire/retire) are considered satisfactory and even artistic in French poetry but are considered unsatisfactory in English. This has been a consistent generalization over the course of centuries, a surprising fact given that other aspects of poetic form in French were happily applied in English. This paper puts forward the hypothesis that this difference is not merely one of poetic tradition, but is grounded in the distinct ways in which information-structure affects prosody in the two languages. A study of rhyme usage in poetry and a perception experiment confirm that native speakers’ intuitions about rhyming in the two languages indeed differ, and a further perception experiment supports the hypothesis that this fact is due to a constraint on prosody that is active in English but not in French. The findings suggest that certain forms of artistic expression in poetry are influenced, and even constrained, by more general properties of a language.","2010-11-01","2024-06-09 12:31:18","2024-06-09 12:31:18","2024-06-09 12:31:18","166-175","","2","117","","Cognition","","","","","","","","","","","","","ScienceDirect","","","","/home/lexi/.zotero-data/storage/97IUBDAF/S0010027710001769.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HLSP94M3","journalArticle","2005","Floor, Sebastian J.","Poetic fronting in a wisdom poetry text : the information structures of Proverbs 7","Journal of Northwest Semitic Languages","","","10.10520/EJC101312","https://journals.co.za/doi/abs/10.10520/EJC101312","The purpose of this article is to address the high frequency of fronting in Biblical Hebrew poetry versus narrative. The approach is to illustrate a model of information structure analysis (with its notions of topic and focus) of a Biblical Hebrew poetic text, with particular attention given to the analysis of marked focus structures with a fronting word-order. The information structure theory proposed by Lambrecht (1994) and adapted for Biblical Hebrew by Floor (2004), is foundational to the approach followed in this article. The text chosen is from Biblical wisdom poetry, Proverbs 7:1-27. In this text, the fronted focus structures provide clues to the rhetorical and thematic flow of Proverbs 7. It is found that one of the functions of fronted argument focus structures is to activate points of departure or theme frames, which it does by identifying key characteristics of the thematic participants or of the circumstances. In addition, fronted argument focus structures are used as a poetic rhetorical device to focus on the main point of a parallelism.","2005-01","2024-06-09 12:35:59","2024-06-09 12:35:59","2024-06-09 12:35:59","23-58","","1","31","","","Poetic fronting in a wisdom poetry text","","","","","","","","","","","","journals.co.za (Atypon)","","Publisher: University of Stellenbosch","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FVHDQN6B","book","1981","None; Internet Archive","Essays in modern stylistics","","978-0-416-74420-0 978-0-416-74430-9","","","http://archive.org/details/essaysinmodernst0000unse","viii, 416 pages : 23 cm; Includes bibliographical references (pages 413-416)","1981","2024-06-09 12:37:32","2024-06-09 12:37:32","2024-06-09 12:37:32","","430","","","","","","","","","","London ; New York : Methuen","","eng","","","","","Internet Archive","","","","","","","","","","","Internet Archive","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SDUMCIJ9","book","2006","Lunn, Nicholas","Word-Order Variation in Biblical Hebrew Poetry: Differentiating Pragmatics and Poetics","","","","","","This study tackles the neglected subject of word order in biblical Hewbrew poetry. The fact that the order of clause constituents frequently differs from that found in prose has often been noted, but no systematic attempt has been offered by way of explanation. Here two separate factors are taken into consideration: that of purely poetic variation (defamiliarisation), and that of pragmatic markedness. This work offers a new approach to the poetry of the Old Testament that will aid towards more accurate translation, exegesis, and discourse analysis of poetic texts.","2006-01-01","2024-06-09 12:39:28","2024-06-09 12:39:28","","","","","","","","Word-Order Variation in Biblical Hebrew Poetry","","","","","","","","","","","","ResearchGate","","","","","https://www.researchgate.net/publication/305264475_Word-Order_Variation_in_Biblical_Hebrew_Poetry_Differentiating_Pragmatics_and_Poetics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C47WKFR6","journalArticle","","Wal, Jenneke van der","Diagnosing focus","","","","","https://www.academia.edu/15924454/Diagnosing_focus","Many tests have been used in eliciting focus constructions and determining what type of focus a certain linguistic strategy expresses. This paper provides an overview of the various diagnostics for focus, indicating how they show the size of the","","2024-06-09 12:48:27","2024-06-09 12:48:27","2024-06-09 12:48:27","","","","","","","","","","","","","","","","","","","www.academia.edu","","","","/home/lexi/.zotero-data/storage/CWGKIH5G/Wal - Diagnosing focus.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9L6ANA8B","journalArticle","2020","Khan, Geoffrey; Van Der Merwe, Christo H.J.","Towards A Comprehensive Model For Interpreting Word Order In Classical Biblical Hebrew","Journal of Semitic Studies","","0022-4480","10.1093/jss/fgaa025","https://doi.org/10.1093/jss/fgaa025","In this paper we examine the function of fronting constructions in verbal clauses in Classical Biblical Hebrew, i.e. constructions that involve the placement of a clause constituent before the verb. We present a model that accommodates the vast majority of cases of fronting in a sample corpus of 1 Samuel. A key feature of our model is the recognition that fronting constructions include both categorical sentences, which make a predication about a base of predication, and also thetic sentences, which present a unitary situation. We classify fronting constructions into three types: (i) narrow constituent focus constructions, (ii) selecting topic constructions, (iii) thetic constructions. An innovative idea that is developed in the paper is that constructions (i) and (iii), although exhibiting different configurations of information structure, nevertheless share various functional properties on the level of discourse organization. On account of these shared properties, we argue that thetic constructions (iii) can be regarded as functional extensions of narrow focus constructions (i). Recognition of these various types of fronting and their interrelationship contributes to a better understanding of the phenomenon.","2020-09-01","2024-06-09 12:50:32","2024-06-09 12:50:32","2024-06-09 12:50:32","347-390","","2","65","","Journal of Semitic Studies","","","","","","","","","","","","","Silverchair","","","","/home/lexi/.zotero-data/storage/CCZP6CW2/[Journal of Semitic Studies vol. 65 iss. 2] Khan, Geoffrey_ Van Der Merwe, Christo H.J. - Towards A Comprehensive Model For Interpreting Word Order In Classical Biblical Hebrew (2020) [10.1093_jss_fgaa025].pdf; /home/lexi/.zotero-data/storage/Q6DL5QUX/Khan and Van Der Merwe - 2020 - Towards A Comprehensive Model For Interpreting Wor.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2GCTKTPL","journalArticle","1894","Pierce, Charles Sanders","What Is a Sign?","","","","","https://www.marxists.org/reference/subject/philosophy/works/us/peirce1.htm","","1894","2024-06-09 13:06:41","2024-06-09 13:08:13","2024-06-09 13:06:41","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/AKFVDI7A/Pierce - 1894 - What Is a Sign.pdf; /home/lexi/.zotero-data/storage/VHYXIM46/peirce1.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5XRR3BP6","journalArticle","2012","Лукьянова, Наталия Александровна","Коммуникативные миры Ч. С. Пирса","Terra Linguistica","","2687-0525","","https://cyberleninka.ru/article/n/kommunikativnye-miry-ch-s-pirsa","The author of the article proposes to rethink the three interrelated semiotic categories such as ""sign"", ""semiosis"" and ""interpretanta"" established by Charles S. Peirce in a communicative vein. Argues that the dynamic process of interpretation of the sign ""semiosis"" the only possible way of its functioning in communication. The result of the operation is a series of interpretant, which are not only symbolic in nature. In this context, the vision of its own objectivity allows a person to construct ""unit features to the concept of"" using a sign as a communication tool.","2012","2024-06-09 13:44:15","2024-06-09 13:45:13","2024-06-09 13:44:15","235-243","","143","1","","","","","","","","","","","","","","","cyberleninka.ru","","Number: 143 Place: Россия, Санкт-Петербург Publisher: Федеральное государственное автономное образовательное учреждение высшего образования «Санкт-Петербургский политехнический университет Петра Великого»","","/home/lexi/.zotero-data/storage/SM43BNET/Александровна - 2012 - Коммуникативные миры Ч. С. Пирса.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SY758UXP","bookSection","2023","Atkin, Albert","Peirce’s Theory of Signs","The Stanford Encyclopedia of Philosophy","","","","https://plato.stanford.edu/archives/spr2023/entries/peirce-semiotics/","Peirce’s Sign Theory, or Semiotic, is an account of signification,representation, reference and meaning. Although sign theories have along history, Peirce’s accounts are distinctive and innovative fortheir breadth and complexity, and for capturing the importance ofinterpretation to signification. For Peirce, developing athoroughgoing theory of signs was a central philosophical andintellectual preoccupation. The importance of semiotic for Peirce iswide ranging. As he himself said, “[…] it has never beenin my power to study anything, – mathematics, ethics, metaphysics,gravitation, thermodynamics, optics, chemistry, comparative anatomy,astronomy, psychology, phonetics, economics, the history of science,whist, men and women, wine, metrology, except as a study ofsemiotic” (SS 1977, 85–6). Peirce also treated sign theoryas central to his work on logic, as the medium for inquiry and theprocess of scientific discovery, and even as one possible means for‘proving’ his pragmatism. Its importance in Peirce’s philosophy, then,cannot be overestimated.","2023","2024-06-09 13:53:43","2024-06-09 13:53:43","2024-06-09 13:53:43","","","","","","","","","","","","Metaphysics Research Lab, Stanford University","","","","","","","Stanford Encyclopedia of Philosophy","","","","/home/lexi/.zotero-data/storage/SIKEC3EY/peirce-semiotics.html","","","","Zalta, Edward N.; Nodelman, Uri","","","","","","","","","","","","","","","","","","","Spring 2023","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2MQKXMU9","journalArticle","1995","Яусс, Ханс Роберт","История литературы как провокация литературоведения","Новое литературное обозрение","","","","","","1995","2024-06-15 09:07:49","2024-06-15 09:12:07","2024-06-15 09:07:49","34-84","","12","1995","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/WCTCG2PB/Яусс_Ханс_Роберт_История_литературы_как_провокация.html; /home/lexi/.zotero-data/storage/HEFBFFAL/Яусс - 1995 - История литературы как провокация литературоведени.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VP4EZ8CB","book","","Онирида","Менталисты","","","","","","","","2024-06-23 19:17:42","2024-06-23 19:17:50","","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/documents/books/художка/Менталисты.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZRHLTNGN","webpage","2023","Millidge, Beren","Deep learning models are secretly (almost) linear","","","","","http://www.beren.io/2023-04-04-DL-models-are-secretly-linear/","I have been increasingly thinking about NN representations and slowly coming to the conclusion that they are (almost) completely secretly linear inside 1. This means that, theoretically, if we can understand their directions, we can very easily exert very powerful control on the internal representations, as well as compose and...","2023-04-04","2024-06-29 07:57:56","2024-06-29 08:01:48","2024-06-29 07:57:56","","","","","","","","","","","","","","en","","","","","","","","","; /home/lexi/.zotero-data/storage/6Z8W7WLY/2023-04-04-DL-models-are-secretly-linear.html","https://www.lesswrong.com/posts/JK9nxcBhQfzEgjjqe/deep-learning-models-might-be-secretly-almost-linear","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"38CC3PAD","preprint","2022","Liu, Chaoyue; Zhu, Libin; Belkin, Mikhail","Transition to Linearity of Wide Neural Networks is an Emerging Property of Assembling Weak Models","","","","10.48550/arXiv.2203.05104","http://arxiv.org/abs/2203.05104","Wide neural networks with linear output layer have been shown to be near-linear, and to have near-constant neural tangent kernel (NTK), in a region containing the optimization path of gradient descent. These findings seem counter-intuitive since in general neural networks are highly complex models. Why does a linear structure emerge when the networks become wide? In this work, we provide a new perspective on this ""transition to linearity"" by considering a neural network as an assembly model recursively built from a set of sub-models corresponding to individual neurons. In this view, we show that the linearity of wide neural networks is, in fact, an emerging property of assembling a large number of diverse ""weak"" sub-models, none of which dominate the assembly.","2022-03-09","2024-06-29 07:59:59","2024-06-29 07:59:59","2024-06-29 07:59:59","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2203.05104 [cs]","","/home/lexi/.zotero-data/storage/VULDKHNK/Liu et al. - 2022 - Transition to Linearity of Wide Neural Networks is.pdf; /home/lexi/.zotero-data/storage/XZSLMFCY/2203.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2203.05104","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4LYSVK3F","journalArticle","2020","Lee, Jaehoon; Xiao, Lechao; Schoenholz, Samuel S.; Bahri, Yasaman; Novak, Roman; Sohl-Dickstein, Jascha; Pennington, Jeffrey","Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent","Journal of Statistical Mechanics: Theory and Experiment","","1742-5468","10.1088/1742-5468/abc62b","http://arxiv.org/abs/1902.06720","A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.","2020-12-01","2024-06-29 08:00:10","2024-07-15 13:21:30","2024-06-29 08:00:10","124002","","12","2020","","J. Stat. Mech.","","","","","","","","","","","","","arXiv.org","","arXiv:1902.06720 [cs, stat]","","/home/lexi/.zotero-data/storage/ISUX9THA/Lee et al. - 2020 - Wide Neural Networks of Any Depth Evolve as Linear.pdf; /home/lexi/.zotero-data/storage/Q7VS3SBI/1902.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PUBZPUTL","preprint","2021","Bartlett, Peter L.; Montanari, Andrea; Rakhlin, Alexander","Deep learning: a statistical viewpoint","","","","10.48550/arXiv.2103.09177","http://arxiv.org/abs/2103.09177","The remarkable practical success of deep learning has revealed some major surprises from a theoretical perspective. In particular, simple gradient methods easily find near-optimal solutions to non-convex optimization problems, and despite giving a near-perfect fit to training data without any explicit effort to control model complexity, these methods exhibit excellent predictive accuracy. We conjecture that specific principles underlie these phenomena: that overparametrization allows gradient methods to find interpolating solutions, that these methods implicitly impose regularization, and that overparametrization leads to benign overfitting. We survey recent theoretical progress that provides examples illustrating these principles in simpler settings. We first review classical uniform convergence results and why they fall short of explaining aspects of the behavior of deep learning methods. We give examples of implicit regularization in simple settings, where gradient methods lead to minimal norm functions that perfectly fit the training data. Then we review prediction methods that exhibit benign overfitting, focusing on regression problems with quadratic loss. For these methods, we can decompose the prediction rule into a simple component that is useful for prediction and a spiky component that is useful for overfitting but, in a favorable setting, does not harm prediction accuracy. We focus specifically on the linear regime for neural networks, where the network can be approximated by a linear model. In this regime, we demonstrate the success of gradient flow, and we consider benign overfitting with two-layer networks, giving an exact asymptotic analysis that precisely demonstrates the impact of overparametrization. We conclude by highlighting the key challenges that arise in extending these insights to realistic deep learning settings.","2021-03-16","2024-06-29 08:00:16","2024-06-29 08:00:16","2024-06-29 08:00:16","","","","","","","Deep learning","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2103.09177 [cs, math, stat]","","/home/lexi/.zotero-data/storage/QXYIFG36/Bartlett et al. - 2021 - Deep learning a statistical viewpoint.pdf; /home/lexi/.zotero-data/storage/PQJQ9WJQ/2103.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2103.09177","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IXDQUBME","preprint","2024","Burns, Collin; Ye, Haotian; Klein, Dan; Steinhardt, Jacob","Discovering Latent Knowledge in Language Models Without Supervision","","","","10.48550/arXiv.2212.03827","http://arxiv.org/abs/2212.03827","Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.","2024-03-02","2024-06-29 08:21:13","2024-07-15 13:21:41","2024-06-29 08:21:13","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2212.03827 [cs]","","/home/lexi/.zotero-data/storage/WF3V4IWP/Burns et al. - 2024 - Discovering Latent Knowledge in Language Models Wi.pdf; /home/lexi/.zotero-data/storage/KDTGCX5Z/2212.html; /home/lexi/.zotero-data/storage/UKLBLQLH/2212.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2212.03827","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"556UBY7E","book","2022","Roberts, Daniel A.; Yaida, Sho; Hanin, Boris","The Principles of Deep Learning Theory","","","","","http://arxiv.org/abs/2106.10165","This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow (RG flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.","2022-05-26","2024-06-29 08:59:51","2024-06-29 08:59:52","2024-06-29 08:59:51","","","","","","","","","","","","","","","","","","","arXiv.org","","DOI: 10.1017/9781009023405 arXiv:2106.10165 [hep-th, stat]","","/home/lexi/.zotero-data/storage/IILK343D/Roberts et al. - 2022 - The Principles of Deep Learning Theory.pdf; /home/lexi/.zotero-data/storage/3VIQB7P2/2106.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VV87QVQB","preprint","2024","Ahn, Kwangjun; Cheng, Xiang; Song, Minhak; Yun, Chulhee; Jadbabaie, Ali; Sra, Suvrit","Linear attention is (maybe) all you need (to understand transformer optimization)","","","","10.48550/arXiv.2310.01082","http://arxiv.org/abs/2310.01082","Transformer training is notoriously difficult, requiring a careful design of optimizers and use of various heuristics. We make progress towards understanding the subtleties of training Transformers by carefully studying a simple yet canonical linearized shallow Transformer model. Specifically, we train linear Transformers to solve regression tasks, inspired by J.~von Oswald et al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we observe that our proposed linearized models can reproduce several prominent aspects of Transformer training dynamics. Consequently, the results obtained in this paper suggest that a simple linearized Transformer model could actually be a valuable, realistic abstraction for understanding Transformer optimization.","2024-03-13","2024-06-29 09:28:38","2024-06-29 09:28:48","2024-06-29 09:28:38","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2310.01082 [cs, math]","","/home/lexi/.zotero-data/storage/F4ARHY3B/Ahn et al. - 2024 - Linear attention is (maybe) all you need (to under.pdf; /home/lexi/.zotero-data/storage/PY54FJHH/2310.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2310.01082","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TMJ5HL97","conferencePaper","2020","Van, Toan Pham; Nguyen, Tam Minh; Tran, Ngoc N.; Nguyen, Hoai Viet; Doan, Linh Bao; Dao, Huy Quang; Minh, Thanh Ta","Interpreting the Latent Space of Generative Adversarial Networks using Supervised Learning","2020 International Conference on Advanced Computing and Applications (ACOMP)","","","10.1109/ACOMP50827.2020.00015","http://arxiv.org/abs/2102.12139","With great progress in the development of Generative Adversarial Networks (GANs), in recent years, the quest for insights in understanding and manipulating the latent space of GAN has gained more and more attention due to its wide range of applications. While most of the researches on this task have focused on unsupervised learning method, which induces difficulties in training and limitation in results, our work approaches another direction, encoding human's prior knowledge to discover more about the hidden space of GAN. With this supervised manner, we produce promising results, demonstrated by accurate manipulation of generated images. Even though our model is more suitable for task-specific problems, we hope that its ease in implementation, preciseness, robustness, and the allowance of richer set of properties (compared to other approaches) for image manipulation can enhance the result of many current applications.","2020-11","2024-06-29 10:03:59","2024-06-29 11:57:23","2024-06-29 10:03:59","49-54","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv:2102.12139 [cs]","","/home/lexi/.zotero-data/storage/CXRSIU6Z/Van et al. - 2020 - Interpreting the Latent Space of Generative Advers.pdf; /home/lexi/.zotero-data/storage/87FTREHY/2102.html","","skimmed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ASHZWZCJ","preprint","2018","Berthelot, David; Raffel, Colin; Roy, Aurko; Goodfellow, Ian","Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer","","","","10.48550/arXiv.1807.07543","http://arxiv.org/abs/1807.07543","Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can ""interpolate"": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.","2018-07-23","2024-06-29 11:48:44","2024-06-29 11:48:44","2024-06-29 11:48:44","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1807.07543 [cs, stat]","","/home/lexi/.zotero-data/storage/QFS3FH5D/Berthelot et al. - 2018 - Understanding and Improving Interpolation in Autoe.pdf; /home/lexi/.zotero-data/storage/TDCPT69W/1807.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1807.07543","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A2IK97IG","preprint","2024","Razzhigaev, Anton; Mikhalchuk, Matvey; Goncharova, Elizaveta; Gerasimenko, Nikolai; Oseledets, Ivan; Dimitrov, Denis; Kuznetsov, Andrey","Your Transformer is Secretly Linear","","","","10.48550/arXiv.2405.12250","http://arxiv.org/abs/2405.12250","This paper reveals a novel linear characteristic exclusive to transformer decoders, including models such as GPT, LLaMA, OPT, BLOOM and others. We analyze embedding transformations between sequential layers, uncovering a near-perfect linear relationship (Procrustes similarity score of 0.99). However, linearity decreases when the residual component is removed due to a consistently low output norm of the transformer layer. Our experiments show that removing or linearly approximating some of the most linear blocks of transformers does not affect significantly the loss or model performance. Moreover, in our pretraining experiments on smaller models we introduce a cosine-similarity-based regularization, aimed at reducing layer linearity. This regularization improves performance metrics on benchmarks like Tiny Stories and SuperGLUE and as well successfully decreases the linearity of the models. This study challenges the existing understanding of transformer architectures, suggesting that their operation may be more linear than previously assumed.","2024-05-19","2024-06-29 15:23:08","2024-06-29 15:23:08","2024-06-29 15:23:08","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2405.12250 [cs] version: 1","","/home/lexi/.zotero-data/storage/QIIT9JK6/Razzhigaev et al. - 2024 - Your Transformer is Secretly Linear.pdf; /home/lexi/.zotero-data/storage/EIKXPTWM/2405.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2405.12250","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R63UXICX","preprint","2023","Meng, Kevin; Bau, David; Andonian, Alex; Belinkov, Yonatan","Locating and Editing Factual Associations in GPT","","","","10.48550/arXiv.2202.05262","http://arxiv.org/abs/2202.05262","We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/","2023-01-13","2024-06-29 15:31:56","2024-06-29 15:31:56","2024-06-29 15:31:56","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2202.05262 [cs]","","/home/lexi/.zotero-data/storage/W3IHYA7U/Meng et al. - 2023 - Locating and Editing Factual Associations in GPT.pdf; /home/lexi/.zotero-data/storage/PFZ24UDZ/2202.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2202.05262","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MQZ6HJC7","preprint","2023","Liu, Zhiheng; Feng, Ruili; Zhu, Kai; Zhang, Yifei; Zheng, Kecheng; Liu, Yu; Zhao, Deli; Zhou, Jingren; Cao, Yang","Cones: Concept Neurons in Diffusion Models for Customized Generation","","","","10.48550/arXiv.2303.05125","http://arxiv.org/abs/2303.05125","Human brains respond to semantic features of presented stimuli with different neurons. It is then curious whether modern deep neural networks admit a similar behavior pattern. Specifically, this paper finds a small cluster of neurons in a diffusion model corresponding to a particular subject. We call those neurons the concept neurons. They can be identified by statistics of network gradients to a stimulation connected with the given subject. The concept neurons demonstrate magnetic properties in interpreting and manipulating generation results. Shutting them can directly yield the related subject contextualized in different scenes. Concatenating multiple clusters of concept neurons can vividly generate all related concepts in a single image. A few steps of further fine-tuning can enhance the multi-concept capability, which may be the first to manage to generate up to four different subjects in a single image. For large-scale applications, the concept neurons are environmentally friendly as we only need to store a sparse cluster of int index instead of dense float32 values of the parameters, which reduces storage consumption by 90\% compared with previous subject-driven generation methods. Extensive qualitative and quantitative studies on diverse scenarios show the superiority of our method in interpreting and manipulating diffusion models.","2023-03-09","2024-06-29 15:59:11","2024-06-29 15:59:11","2024-06-29 15:59:11","","","","","","","Cones","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2303.05125 [cs]","","/home/lexi/.zotero-data/storage/2B7SY66D/Liu et al. - 2023 - Cones Concept Neurons in Diffusion Models for Cus.pdf; /home/lexi/.zotero-data/storage/366SHXGK/2303.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2303.05125","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XNVKSY76","conferencePaper","2020","Vig, Jesse; Gehrmann, Sebastian; Belinkov, Yonatan; Qian, Sharon; Nevo, Daniel; Singer, Yaron; Shieber, Stuart","Investigating Gender Bias in Language Models Using Causal Mediation Analysis","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html","Many interpretation methods for neural models in natural language processing investigate how information is encoded inside hidden representations. However, these methods can only measure whether the information exists, not whether it is actually used by the model.  We propose a methodology grounded in the theory of causal mediation analysis for interpreting which parts of a model are causally implicated in its behavior. The approach enables us to analyze the mechanisms that facilitate the flow of information from input to output through various model components, known as mediators. As a case study, we apply this methodology to analyzing gender bias in pre-trained Transformer language models. We study the role of individual neurons and attention heads in mediating gender bias across three datasets designed to gauge a model's sensitivity to gender bias. Our mediation analysis reveals that gender bias effects are concentrated in specific components of the model that may exhibit highly specialized behavior.","2020","2024-06-29 18:31:53","2024-06-29 18:31:53","2024-06-29 18:31:53","12388–12401","","","33","","","","","","","","Curran Associates, Inc.","","","","","","","Neural Information Processing Systems","","","","/home/lexi/.zotero-data/storage/GAZ9GSHE/Vig et al. - 2020 - Investigating Gender Bias in Language Models Using.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CND3H8FT","preprint","2019","Vig, Jesse","A Multiscale Visualization of Attention in the Transformer Model","","","","10.48550/arXiv.1906.05714","http://arxiv.org/abs/1906.05714","The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.","2019-06-12","2024-06-30 13:37:50","2024-06-30 13:37:50","2024-06-30 13:37:50","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1906.05714 [cs]","","/home/lexi/.zotero-data/storage/TQPNPDZJ/Vig - 2019 - A Multiscale Visualization of Attention in the Tra.pdf; /home/lexi/.zotero-data/storage/QGVCNSB7/1906.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1906.05714","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PSNFP5XV","preprint","2020","Vig, Jesse; Gehrmann, Sebastian; Belinkov, Yonatan; Qian, Sharon; Nevo, Daniel; Sakenis, Simas; Huang, Jason; Singer, Yaron; Shieber, Stuart","Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias","","","","10.48550/arXiv.2004.12265","http://arxiv.org/abs/2004.12265","Common methods for interpreting neural models in natural language processing typically examine either their structure or their behavior, but not both. We propose a methodology grounded in the theory of causal mediation analysis for interpreting which parts of a model are causally implicated in its behavior. It enables us to analyze the mechanisms by which information flows from input to output through various model components, known as mediators. We apply this methodology to analyze gender bias in pre-trained Transformer language models. We study the role of individual neurons and attention heads in mediating gender bias across three datasets designed to gauge a model's sensitivity to gender bias. Our mediation analysis reveals that gender bias effects are (i) sparse, concentrated in a small part of the network; (ii) synergistic, amplified or repressed by different components; and (iii) decomposable into effects flowing directly from the input and indirectly through the mediators.","2020-11-22","2024-06-30 13:51:08","2024-06-30 13:51:08","2024-06-30 13:51:08","","","","","","","Causal Mediation Analysis for Interpreting Neural NLP","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2004.12265 [cs]","","/home/lexi/.zotero-data/storage/YVTPEXW7/Vig et al. - 2020 - Causal Mediation Analysis for Interpreting Neural .pdf; /home/lexi/.zotero-data/storage/MDLG5PQ2/2004.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2004.12265","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZWZ4MB8I","preprint","2018","Alain, Guillaume; Bengio, Yoshua","Understanding intermediate layers using linear classifier probes","","","","10.48550/arXiv.1610.01644","http://arxiv.org/abs/1610.01644","Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as ""probes"", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.","2018-11-22","2024-07-11 11:26:04","2024-07-11 11:26:04","2024-07-11 11:26:04","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1610.01644 [cs, stat]","","/home/lexi/.zotero-data/storage/Z6UDFCBN/Alain and Bengio - 2018 - Understanding intermediate layers using linear cla.pdf; /home/lexi/.zotero-data/storage/HF6S5EC5/1610.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1610.01644","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CIW2CVYJ","preprint","2024","Kossen, Jannik; Han, Jiatong; Razzak, Muhammed; Schut, Lisa; Malik, Shreshth; Gal, Yarin","Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs","","","","10.48550/arXiv.2406.15927","http://arxiv.org/abs/2406.15927","We propose semantic entropy probes (SEPs), a cheap and reliable method for uncertainty quantification in Large Language Models (LLMs). Hallucinations, which are plausible-sounding but factually incorrect and arbitrary model generations, present a major challenge to the practical adoption of LLMs. Recent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can detect hallucinations by estimating uncertainty in the space semantic meaning for a set of model generations. However, the 5-to-10-fold increase in computation cost associated with SE computation hinders practical adoption. To address this, we propose SEPs, which directly approximate SE from the hidden states of a single generation. SEPs are simple to train and do not require sampling multiple model generations at test time, reducing the overhead of semantic uncertainty quantification to almost zero. We show that SEPs retain high performance for hallucination detection and generalize better to out-of-distribution data than previous probing methods that directly predict model accuracy. Our results across models and tasks suggest that model hidden states capture SE, and our ablation studies give further insights into the token positions and model layers for which this is the case.","2024-06-22","2024-07-11 11:26:17","2024-07-11 11:26:17","2024-07-11 11:26:17","","","","","","","Semantic Entropy Probes","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2406.15927 [cs]","","/home/lexi/.zotero-data/storage/ERT3A9QW/Kossen et al. - 2024 - Semantic Entropy Probes Robust and Cheap Hallucin.pdf; /home/lexi/.zotero-data/storage/EL37N2A2/2406.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2406.15927","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZH5QHIPD","journalArticle","2022","Mielke, Sabrina J.; Szlam, Arthur; Dinan, Emily; Boureau, Y-Lan","Reducing Conversational Agents’ Overconfidence Through Linguistic Calibration","Transactions of the Association for Computational Linguistics","","2307-387X","10.1162/tacl_a_00494","https://doi.org/10.1162/tacl_a_00494","While improving neural dialogue agents’ factual accuracy is the object of much research, another important aspect of communication, less studied in the setting of neural dialogue, is transparency about ignorance. In this work, we analyze to what extent state-of-the-art chit-chat models are linguistically calibrated in the sense that their verbalized expression of doubt (or confidence) matches the likelihood that the model’s responses are factually incorrect (or correct). We find that these models are poorly calibrated, yet we show that likelihood of correctness can accurately be predicted. By incorporating such metacognitive features into the training of a controllable generation model, we obtain a dialogue agent with greatly improved linguistic calibration.","2022-08-12","2024-07-11 11:26:37","2024-07-11 11:26:37","2024-07-11 11:26:37","857-872","","","10","","Transactions of the Association for Computational Linguistics","","","","","","","","","","","","","Silverchair","","","","/home/lexi/.zotero-data/storage/JI93XPCU/Mielke et al. - 2022 - Reducing Conversational Agents’ Overconfidence Thr.pdf; /home/lexi/.zotero-data/storage/DKNKL8DV/Reducing-Conversational-Agents-Overconfidence.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XH3JLXMM","conferencePaper","2023","Azaria, Amos; Mitchell, Tom","The Internal State of an LLM Knows When It's Lying","Findings of the Association for Computational Linguistics: EMNLP 2023","","","10.18653/v1/2023.findings-emnlp.68","https://aclanthology.org/2023.findings-emnlp.68","While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71% to 83% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.","2023-12","2024-07-11 11:27:14","2024-07-11 11:27:14","2024-07-11 11:27:14","967–976","","","","","","","","","","","Association for Computational Linguistics","Singapore","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/NXXL8HEI/Azaria and Mitchell - 2023 - The Internal State of an LLM Knows When It's Lying.pdf","","","","Bouamor, Houda; Pino, Juan; Bali, Kalika","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Findings 2023","","","","","","","","","","","","","","",""
"2ZGXRMKT","preprint","2024","Gurnee, Wes; Tegmark, Max","Language Models Represent Space and Time","","","","10.48550/arXiv.2310.02207","http://arxiv.org/abs/2310.02207","The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual ""space neurons"" and ""time neurons"" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.","2024-03-04","2024-07-11 11:29:04","2024-07-11 11:29:04","2024-07-11 11:29:04","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2310.02207 [cs]","","/home/lexi/.zotero-data/storage/7SM4WCQ7/Gurnee and Tegmark - 2024 - Language Models Represent Space and Time.pdf; /home/lexi/.zotero-data/storage/DPZRCKXM/2310.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2310.02207","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N6NWHABM","preprint","2024","CH-Wang, Sky; Van Durme, Benjamin; Eisner, Jason; Kedzie, Chris","Do Androids Know They're Only Dreaming of Electric Sheep?","","","","10.48550/arXiv.2312.17249","http://arxiv.org/abs/2312.17249","We design probes trained on the internal representations of a transformer language model to predict its hallucinatory behavior on three grounded generation tasks. To train the probes, we annotate for span-level hallucination on both sampled (organic) and manually edited (synthetic) reference outputs. Our probes are narrowly trained and we find that they are sensitive to their training domain: they generalize poorly from one task to another or from synthetic to organic hallucinations. However, on in-domain data, they can reliably detect hallucinations at many transformer layers, achieving 95% of their peak performance as early as layer 4. Here, probing proves accurate for evaluating hallucination, outperforming several contemporary baselines and even surpassing an expert human annotator in response-level detection F1. Similarly, on span-level labeling, probes are on par or better than the expert annotator on two out of three generation tasks. Overall, we find that probing is a feasible and efficient alternative to language model hallucination evaluation when model states are available.","2024-06-08","2024-07-11 11:29:25","2024-07-11 11:29:25","2024-07-11 11:29:25","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2312.17249 [cs]","","/home/lexi/.zotero-data/storage/QLMBJTNU/CH-Wang et al. - 2024 - Do Androids Know They're Only Dreaming of Electric.pdf; /home/lexi/.zotero-data/storage/DDBJCMQL/2312.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2312.17249","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9M76KW2X","preprint","2024","Hoscilowicz, Jakub; Wiacek, Adam; Chojnacki, Jan; Cieslak, Adam; Michon, Leszek; Urbanevych, Vitalii; Janicki, Artur","Non-Linear Inference Time Intervention: Improving LLM Truthfulness","","","","10.48550/arXiv.2403.18680","http://arxiv.org/abs/2403.18680","In this work, we explore LLM's internal representation space to identify attention heads that contain the most truthful and accurate information. We further developed the Inference Time Intervention (ITI) framework, which lets bias LLM without the need for fine-tuning. The improvement manifests in introducing a non-linear multi-token probing and multi-token intervention: Non-Linear ITI (NL-ITI), which significantly enhances performance on evaluation benchmarks. NL-ITI is tested on diverse multiple-choice datasets, including TruthfulQA, on which we report over 16% relative MC1 (accuracy of model pointing to the correct answer) improvement with respect to the baseline ITI results. Moreover, we achieved a 10% relative improvement over the recently released Truth Forest (TrFf) method that also focused on ITI improvement.","2024-06-06","2024-07-11 11:31:39","2024-07-11 11:31:40","2024-07-11 11:31:39","","","","","","","Non-Linear Inference Time Intervention","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2403.18680 [cs]","","/home/lexi/.zotero-data/storage/TAAI2TBS/Hoscilowicz et al. - 2024 - Non-Linear Inference Time Intervention Improving .pdf; /home/lexi/.zotero-data/storage/2HYTCFY7/2403.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2403.18680","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZTCRXJAU","journalArticle","2024","Levinstein, B. A.; Herrmann, Daniel A.","Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks","Philosophical Studies","","0031-8116, 1573-0883","10.1007/s11098-023-02094-3","http://arxiv.org/abs/2307.00175","We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.","2024-02-17","2024-07-11 11:32:08","2024-07-11 11:32:08","2024-07-11 11:32:08","","","","","","Philos Stud","Still No Lie Detector for Language Models","","","","","","","","","","","","arXiv.org","","arXiv:2307.00175 [cs]","","/home/lexi/.zotero-data/storage/QD57YZTS/Levinstein and Herrmann - 2024 - Still No Lie Detector for Language Models Probing.pdf; /home/lexi/.zotero-data/storage/2RAUASDH/2307.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G5JUA322","preprint","2024","Papillon, Mathilde; Sanborn, Sophia; Hajij, Mustafa; Miolane, Nina","Architectures of Topological Deep Learning: A Survey of Message-Passing Topological Neural Networks","","","","10.48550/arXiv.2304.10031","http://arxiv.org/abs/2304.10031","The natural world is full of complex systems characterized by intricate relations between their components: from social interactions between individuals in a social network to electrostatic interactions between atoms in a protein. Topological Deep Learning (TDL) provides a comprehensive framework to process and extract knowledge from data associated with these systems, such as predicting the social community to which an individual belongs or predicting whether a protein can be a reasonable target for drug development. TDL has demonstrated theoretical and practical advantages that hold the promise of breaking ground in the applied sciences and beyond. However, the rapid growth of the TDL literature for relational systems has also led to a lack of unification in notation and language across message-passing Topological Neural Network (TNN) architectures. This presents a real obstacle for building upon existing works and for deploying message-passing TNNs to new real-world problems. To address this issue, we provide an accessible introduction to TDL for relational systems, and compare the recently published message-passing TNNs using a unified mathematical and graphical notation. Through an intuitive and critical review of the emerging field of TDL, we extract valuable insights into current challenges and exciting opportunities for future development.","2024-02-21","2024-07-11 11:32:34","2024-07-11 11:32:34","2024-07-11 11:32:34","","","","","","","Architectures of Topological Deep Learning","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2304.10031 [cs]","","/home/lexi/.zotero-data/storage/26GAP9I4/Papillon et al. - 2024 - Architectures of Topological Deep Learning A Surv.pdf; /home/lexi/.zotero-data/storage/723Q3CLY/2304.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2304.10031","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FN7BMFLT","preprint","2023","Kostenok, Elizaveta; Cherniavskii, Daniil; Zaytsev, Alexey","Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices","","","","10.48550/arXiv.2308.11295","http://arxiv.org/abs/2308.11295","Determining the degree of confidence of deep learning model in its prediction is an open problem in the field of natural language processing. Most of the classical methods for uncertainty estimation are quite weak for text classification models. We set the task of obtaining an uncertainty estimate for neural networks based on the Transformer architecture. A key feature of such mo-dels is the attention mechanism, which supports the information flow between the hidden representations of tokens in the neural network. We explore the formed relationships between internal representations using Topological Data Analysis methods and utilize them to predict model's confidence. In this paper, we propose a method for uncertainty estimation based on the topological properties of the attention mechanism and compare it with classical methods. As a result, the proposed algorithm surpasses the existing methods in quality and opens up a new area of application of the attention mechanism, but requires the selection of topological features.","2023-08-22","2024-07-11 11:32:39","2024-07-11 11:32:39","2024-07-11 11:32:39","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2308.11295 [cs]","","/home/lexi/.zotero-data/storage/TUNMALMY/Kostenok et al. - 2023 - Uncertainty Estimation of Transformers' Prediction.pdf; /home/lexi/.zotero-data/storage/JILIHEVS/2308.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2308.11295","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AIUH2HYU","journalArticle","2021","Hensel, Felix; Moor, Michael; Rieck, Bastian","A Survey of Topological Machine Learning Methods","Frontiers in Artificial Intelligence","","2624-8212","10.3389/frai.2021.681108","https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2021.681108/full","<p>The last decade saw an enormous boost in the field of computational topology: methods and concepts from algebraic and differential topology, formerly confined to the realm of pure mathematics, have demonstrated their utility in numerous areas such as computational biology personalised medicine, and time-dependent data analysis, to name a few. The newly-emerging domain comprising topology-based techniques is often referred to as topological data analysis (TDA). Next to their applications in the aforementioned areas, TDA methods have also proven to be effective in supporting, enhancing, and augmenting both classical machine learning and deep learning models. In this paper, we review the state of the art of a nascent field we refer to as “topological machine learning,” i.e., the successful symbiosis of topology-based methods and machine learning algorithms, such as deep neural networks. We identify common threads, current applications, and future challenges.</p>","2021-05-26","2024-07-11 11:32:49","2024-07-11 11:32:49","2024-07-11 11:32:49","","","","4","","Front. Artif. Intell.","","","","","","","","English","","","","","Frontiers","","Publisher: Frontiers","","/home/lexi/.zotero-data/storage/ETEZT8KP/Hensel et al. - 2021 - A Survey of Topological Machine Learning Methods.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D9YY4YW7","journalArticle","2021","Chazal, Frédéric; Michel, Bertrand","An Introduction to Topological Data Analysis: Fundamental and Practical Aspects for Data Scientists","Frontiers in Artificial Intelligence","","2624-8212","10.3389/frai.2021.667963","https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2021.667963/full","<p>With the recent explosion in the amount, the variety, and the dimensionality of available data, identifying, extracting, and exploiting their underlying structure has become a problem of fundamental importance for data analysis and statistical learning. Topological data analysis (<sc>tda</sc>) is a recent and fast-growing field providing a set of new topological and geometric tools to infer relevant features for possibly complex data. It proposes new well-founded mathematical theories and computational tools that can be used independently or in combination with other data analysis and statistical learning techniques. This article is a brief introduction, through a few selected topics, to basic fundamental and practical aspects of <sc>tda</sc> for nonexperts.</p>","2021-09-29","2024-07-11 11:33:05","2024-07-11 11:33:05","2024-07-11 11:33:05","","","","4","","Front. Artif. Intell.","An Introduction to Topological Data Analysis","","","","","","","English","","","","","Frontiers","","Publisher: Frontiers","","/home/lexi/.zotero-data/storage/M9U4LET7/Chazal and Michel - 2021 - An Introduction to Topological Data Analysis Fund.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X4YB6B5Z","conferencePaper","2023","Proskurina, Irina; Artemova, Ekaterina; Piontkovskaya, Irina","Can BERT eat RuCoLA? Topological Data Analysis to Explain","Proceedings of the 9th Workshop on Slavic Natural Language Processing 2023 (SlavicNLP 2023)","","","10.18653/v1/2023.bsnlp-1.15","https://aclanthology.org/2023.bsnlp-1.15","This paper investigates how Transformer language models (LMs) fine-tuned for acceptability classification capture linguistic features. Our approach is based on best practices of topological data analysis (TDA) in NLP: we construct directed attention graphs from attention matrices, derive topological features from them and feed them to linear classifiers. We introduce two novel features, chordality and the matching number, and show that TDA-based classifiers outperform fine-tuning baselines. We experiment with two datasets, CoLA and RuCoLA, in English and Russian, which are typologically different languages. On top of that, we propose several black-box introspection techniques aimed at detecting changes in the attention mode of the LM's during fine-tuning, defining the LM's prediction confidences, and associating individual heads with fine-grained grammar phenomena. Our results contribute to understanding the behaviour of monolingual LMs in the acceptability classification task, provide insights into the functional roles of attention heads, and highlight the advantages of TDA-based approaches for analyzing LMs.We release the code and the experimental results for further uptake.","2023-05","2024-07-11 11:33:13","2024-07-11 11:33:13","2024-07-11 11:33:13","123–137","","","","","","Can BERT eat RuCoLA?","","","","","Association for Computational Linguistics","Dubrovnik, Croatia","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/IMB3NBND/Proskurina et al. - 2023 - Can BERT eat RuCoLA Topological Data Analysis to .pdf","","","","Piskorski, Jakub; Marcińczuk, Michał; Nakov, Preslav; Ogrodniczuk, Maciej; Pollak, Senja; Přibáň, Pavel; Rybak, Piotr; Steinberger, Josef; Yangarber, Roman","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","BSNLP 2023","","","","","","","","","","","","","","",""
"3LFLVAXQ","preprint","2024","Xu, Rongwu; Qi, Zehan; Guo, Zhijiang; Wang, Cunxiang; Wang, Hongru; Zhang, Yue; Xu, Wei","Knowledge Conflicts for LLMs: A Survey","","","","10.48550/arXiv.2403.08319","http://arxiv.org/abs/2403.08319","This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.","2024-06-22","2024-07-11 13:51:20","2024-07-11 13:51:20","2024-07-11 13:51:20","","","","","","","Knowledge Conflicts for LLMs","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2403.08319 [cs]","","/home/lexi/.zotero-data/storage/E2BB8VC3/Xu et al. - 2024 - Knowledge Conflicts for LLMs A Survey.pdf; /home/lexi/.zotero-data/storage/P8I3NDUW/2403.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2403.08319","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4RFT3IIR","journalArticle","2018","Lee, Katherine; Firat, Orhan; Agarwal, Ashish; Fannjiang, Clara; Sussillo, David","Hallucinations in Neural Machine Translation","","","","","https://openreview.net/forum?id=SkxJ-309FQ","Neural machine translation (NMT) systems have reached state of the art performance in translating text and are in wide deployment. Yet little is understood about how these systems function or break. Here we show that NMT systems are susceptible to producing highly pathological translations that are completely untethered from the source material, which we term hallucinations. Such pathological translations are problematic because they are are deeply disturbing of user trust and easy to find with a simple search. We describe a method to generate hallucinations and show that many common variations of the NMT architecture are susceptible to them. We study a variety of approaches to reduce the frequency of hallucinations, including data augmentation, dynamical systems and regularization techniques, showing that data augmentation significantly reduces hallucination frequency. Finally, we analyze networks that produce hallucinations and show that there are signatures in the attention matrix as well as in the hidden states of the decoder.","2018-09-27","2024-07-11 13:51:22","2024-07-11 13:51:22","2024-07-11 13:51:22","","","","","","","","","","","","","","en","","","","","openreview.net","","","","/home/lexi/.zotero-data/storage/56TPEED8/Lee et al. - 2018 - Hallucinations in Neural Machine Translation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZDS85LQZ","preprint","2024","Yuksekgonul, Mert; Chandrasekaran, Varun; Jones, Erik; Gunasekar, Suriya; Naik, Ranjita; Palangi, Hamid; Kamar, Ece; Nushi, Besmira","Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models","","","","10.48550/arXiv.2309.15098","http://arxiv.org/abs/2309.15098","We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as constraint satisfaction problems and use this framework to investigate how the LLM interacts internally with factual constraints. We find a strong positive relationship between the LLM's attention to constraint tokens and the factual accuracy of generations. We curate a suite of 10 datasets containing over 40,000 prompts to study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing attention patterns, that can predict factual errors and fine-grained constraint satisfaction, and allow early error identification. The approach and findings take another step towards using the mechanistic understanding of LLMs to enhance their reliability.","2024-04-17","2024-07-11 13:51:27","2024-07-11 13:51:27","2024-07-11 13:51:27","","","","","","","Attention Satisfies","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2309.15098 [cs]","","/home/lexi/.zotero-data/storage/P76Y7FTT/Yuksekgonul et al. - 2024 - Attention Satisfies A Constraint-Satisfaction Len.pdf; /home/lexi/.zotero-data/storage/DZIWYQYP/2309.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2309.15098","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7M9CIG27","preprint","2023","Varshney, Neeraj; Yao, Wenlin; Zhang, Hongming; Chen, Jianshu; Yu, Dong","A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation","","","","10.48550/arXiv.2307.03987","http://arxiv.org/abs/2307.03987","Recently developed large language models have achieved remarkable success in generating fluent and coherent text. However, these models often tend to 'hallucinate' which critically hampers their reliability. In this work, we address this crucial problem and propose an approach that actively detects and mitigates hallucinations during the generation process. Specifically, we first identify the candidates of potential hallucination leveraging the model's logit output values, check their correctness through a validation procedure, mitigate the detected hallucinations, and then continue with the generation process. Through extensive experiments with GPT-3.5 (text-davinci-003) on the 'article generation task', we first demonstrate the individual efficacy of our detection and mitigation techniques. Specifically, the detection technique achieves a recall of ~88% and the mitigation technique successfully mitigates 57.6% of the correctly detected hallucinations. Importantly, our mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. Then, we show that the proposed active detection and mitigation approach successfully reduces the hallucinations of the GPT-3.5 model from 47.5% to 14.5% on average. We further demonstrate the effectiveness and wide applicability of our approach through additional studies including performance on different types of questions (multi-hop and false premise questions) and with another LLM from a different model family (Vicuna). In summary, our work contributes to improving the reliability and trustworthiness of large language models, a crucial step en route to enabling their widespread adoption in real-world applications.","2023-08-12","2024-07-11 14:06:03","2024-07-11 14:06:03","2024-07-11 14:06:03","","","","","","","A Stitch in Time Saves Nine","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2307.03987 [cs]","","/home/lexi/.zotero-data/storage/DAWEE534/Varshney et al. - 2023 - A Stitch in Time Saves Nine Detecting and Mitigat.pdf; /home/lexi/.zotero-data/storage/JILSRWUX/2307.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2307.03987","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BVN5GLQF","journalArticle","2024","Farquhar, Sebastian; Kossen, Jannik; Kuhn, Lorenz; Gal, Yarin","Detecting hallucinations in large language models using semantic entropy","Nature","","1476-4687","10.1038/s41586-024-07421-0","https://www.nature.com/articles/s41586-024-07421-0","Large language model (LLM) systems, such as ChatGPT1 or Gemini2, can show impressive reasoning and question-answering capabilities but often ‘hallucinate’ false outputs and unsubstantiated answers3,4. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5 or untrue facts in news articles6 and even posing a risk to human life in medical domains such as radiology7. Encouraging truthfulness through supervision or reinforcement has been only partially successful8. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.","2024-06","2024-07-11 14:31:48","2024-07-11 14:31:48","2024-07-11 14:31:48","625-630","","8017","630","","","","","","","","","","en","2024 The Author(s)","","","","www.nature.com","","Publisher: Nature Publishing Group","","/home/lexi/.zotero-data/storage/95Z79J64/Farquhar et al. - 2024 - Detecting hallucinations in large language models .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B5KI6ZFF","preprint","2023","Gurnee, Wes; Nanda, Neel; Pauly, Matthew; Harvey, Katherine; Troitskii, Dmitrii; Bertsimas, Dimitris","Finding Neurons in a Haystack: Case Studies with Sparse Probing","","","","10.48550/arXiv.2305.01610","http://arxiv.org/abs/2305.01610","Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to increase on average, but there are multiple types of scaling dynamics. In all, we probe for over 100 unique features comprising 10 different categories in 7 different models spanning 70 million to 6.9 billion parameters.","2023-06-02","2024-07-11 19:20:14","2024-07-11 19:20:15","2024-07-11 19:20:14","","","","","","","Finding Neurons in a Haystack","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2305.01610 [cs]","","/home/lexi/.zotero-data/storage/H7JTVG8U/Gurnee et al. - 2023 - Finding Neurons in a Haystack Case Studies with S.pdf; /home/lexi/.zotero-data/storage/NEYQPU2S/2305.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2305.01610","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BHX9ASFQ","preprint","2023","Marks, Samuel; Tegmark, Max","The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets","","","","10.48550/arXiv.2310.06824","http://arxiv.org/abs/2310.06824","Large Language Models (LLMs) have impressive capabilities, but are also prone to outputting falsehoods. Recent work has developed techniques for inferring whether a LLM is telling the truth by training probes on the LLM's internal activations. However, this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues. In this work, we curate high-quality datasets of true/false statements and use them to study in detail the structure of LLM representations of truth, drawing on three lines of evidence: 1. Visualizations of LLM true/false statement representations, which reveal clear linear structure. 2. Transfer experiments in which probes trained on one dataset generalize to different datasets. 3. Causal evidence obtained by surgically intervening in a LLM's forward pass, causing it to treat false statements as true and vice versa. Overall, we present evidence that language models linearly represent the truth or falsehood of factual statements. We also introduce a novel technique, mass-mean probing, which generalizes better and is more causally implicated in model outputs than other probing techniques.","2023-12-08","2024-07-12 12:54:22","2024-07-12 12:54:22","2024-07-12 12:54:21","","","","","","","The Geometry of Truth","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2310.06824 [cs]","","/home/lexi/.zotero-data/storage/LBS8WQYU/Marks and Tegmark - 2023 - The Geometry of Truth Emergent Linear Structure i.pdf; /home/lexi/.zotero-data/storage/7H6Y9M95/2310.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2310.06824","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FTJUABAD","conferencePaper","2022","Vazhentsev, Artem; Kuzmin, Gleb; Shelmanov, Artem; Tsvigun, Akim; Tsymbalov, Evgenii; Fedyanin, Kirill; Panov, Maxim; Panchenko, Alexander; Gusev, Gleb; Burtsev, Mikhail; Avetisian, Manvel; Zhukov, Leonid","Uncertainty Estimation of Transformer Predictions for Misclassification Detection","Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/2022.acl-long.566","https://aclanthology.org/2022.acl-long.566","Uncertainty estimation (UE) of model predictions is a crucial step for a variety of tasks such as active learning, misclassification detection, adversarial attack detection, out-of-distribution detection, etc. Most of the works on modeling the uncertainty of deep neural networks evaluate these methods on image classification tasks. Little attention has been paid to UE in natural language processing. To fill this gap, we perform a vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and propose two computationally efficient modifications, one of which approaches or even outperforms computationally intensive methods.","2022-05","2024-07-12 15:01:17","2024-07-12 15:01:17","2024-07-12 15:01:17","8237–8252","","","","","","","","","","","Association for Computational Linguistics","Dublin, Ireland","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/4SD9LF83/Vazhentsev et al. - 2022 - Uncertainty Estimation of Transformer Predictions .pdf","","","","Muresan, Smaranda; Nakov, Preslav; Villavicencio, Aline","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL 2022","","","","","","","","","","","","","","",""
"EPMTGX82","conferencePaper","2022","Cherniavskii, Daniil; Tulchinskii, Eduard; Mikhailov, Vladislav; Proskurina, Irina; Kushnareva, Laida; Artemova, Ekaterina; Barannikov, Serguei; Piontkovskaya, Irina; Piontkovski, Dmitri; Burnaev, Evgeny","Acceptability Judgements via Examining the Topology of Attention Maps","Findings of the Association for Computational Linguistics: EMNLP 2022","","","10.18653/v1/2022.findings-emnlp.7","https://aclanthology.org/2022.findings-emnlp.7","The role of the attention mechanism in encoding linguistic knowledge has received special interest in NLP. However, the ability of the attention heads to judge the grammatical acceptability of a sentence has been underexplored. This paper approaches the paradigm of acceptability judgments with topological data analysis (TDA), showing that the geometric properties of the attention graph can be efficiently exploited for two standard practices in linguistics: binary judgments and linguistic minimal pairs. Topological features enhance the BERT-based acceptability classifier scores by 8%-24% on CoLA in three languages (English, Italian, and Swedish). By revealing the topological discrepancy between attention maps of minimal pairs, we achieve the human-level performance on the BLiMP benchmark, outperforming nine statistical and Transformer LM baselines. At the same time, TDA provides the foundation for analyzing the linguistic functions of attention heads and interpreting the correspondence between the graph features and grammatical phenomena. We publicly release the code and other materials used in the experiments.","2022-12","2024-07-12 16:37:46","2024-07-12 16:37:46","2024-07-12 16:37:46","88–107","","","","","","","","","","","Association for Computational Linguistics","Abu Dhabi, United Arab Emirates","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/Y92MF8I7/Cherniavskii et al. - 2022 - Acceptability Judgements via Examining the Topolog.pdf","","","","Goldberg, Yoav; Kozareva, Zornitsa; Zhang, Yue","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Findings 2022","","","","","","","","","","","","","","",""
"KVSFJ3AA","conferencePaper","2021","Kushnareva, Laida; Cherniavskii, Daniil; Mikhailov, Vladislav; Artemova, Ekaterina; Barannikov, Serguei; Bernstein, Alexander; Piontkovskaya, Irina; Piontkovski, Dmitri; Burnaev, Evgeny","Artificial Text Detection via Examining the Topology of Attention Maps","Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing","","","10.18653/v1/2021.emnlp-main.50","https://aclanthology.org/2021.emnlp-main.50","The impressive capabilities of recent generative models to create texts that are challenging to distinguish from the human-written ones can be misused for generating fake news, product reviews, and even abusive content. Despite the prominent performance of existing methods for artificial text detection, they still lack interpretability and robustness towards unseen models. To this end, we propose three novel types of interpretable topological features for this task based on Topological Data Analysis (TDA) which is currently understudied in the field of NLP. We empirically show that the features derived from the BERT model outperform count- and neural-based baselines up to 10% on three common datasets, and tend to be the most robust towards unseen GPT-style generation models as opposed to existing methods. The probing analysis of the features reveals their sensitivity to the surface and syntactic properties. The results demonstrate that TDA is a promising line with respect to NLP tasks, specifically the ones that incorporate surface and structural information.","2021-11","2024-07-12 17:05:46","2024-07-12 17:05:46","2024-07-12 17:05:46","635–649","","","","","","","","","","","Association for Computational Linguistics","Online and Punta Cana, Dominican Republic","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/SAUIUR2N/Kushnareva et al. - 2021 - Artificial Text Detection via Examining the Topolo.pdf","","","","Moens, Marie-Francine; Huang, Xuanjing; Specia, Lucia; Yih, Scott Wen-tau","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","EMNLP 2021","","","","","","","","","","","","","","",""
"KMPNBGEJ","preprint","2022","Lin, Stephanie; Hilton, Jacob; Evans, Owain","TruthfulQA: Measuring How Models Mimic Human Falsehoods","","","","10.48550/arXiv.2109.07958","http://arxiv.org/abs/2109.07958","We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.","2022-05-07","2024-07-14 10:07:38","2024-07-14 10:07:41","2024-07-14 10:07:38","","","","","","","TruthfulQA","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2109.07958 [cs]","","/home/lexi/.zotero-data/storage/CWPSLCB2/Lin et al. - 2022 - TruthfulQA Measuring How Models Mimic Human False.pdf; /home/lexi/.zotero-data/storage/62JQ24R8/2109.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2109.07958","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S3N5U4US","preprint","2024","Zhang, Shaolei; Yu, Tian; Feng, Yang","TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space","","","","10.48550/arXiv.2402.17811","http://arxiv.org/abs/2402.17811","Large Language Models (LLMs) sometimes suffer from producing hallucinations, especially LLMs may generate untruthful responses despite knowing the correct knowledge. Activating the truthfulness within LLM is the key to fully unlocking LLM's knowledge potential. In this paper, we propose TruthX, an inference-time intervention method to activate the truthfulness of LLM by identifying and editing the features within LLM's internal representations that govern the truthfulness. TruthX employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLM. Experiments show that TruthX improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that TruthX can control LLM to produce truthful or hallucinatory responses via editing only one vector in LLM's internal representations.","2024-06-05","2024-07-14 10:11:40","2024-07-14 10:11:40","2024-07-14 10:11:40","","","","","","","TruthX","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2402.17811 [cs]","","/home/lexi/.zotero-data/storage/GHS9FSAL/Zhang et al. - 2024 - TruthX Alleviating Hallucinations by Editing Larg.pdf; /home/lexi/.zotero-data/storage/IKV3DFLD/2402.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2402.17811","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PY9B9BNF","preprint","2023","Zou, Andy; Phan, Long; Chen, Sarah; Campbell, James; Guo, Phillip; Ren, Richard; Pan, Alexander; Yin, Xuwang; Mazeika, Mantas; Dombrowski, Ann-Kathrin; Goel, Shashwat; Li, Nathaniel; Byun, Michael J.; Wang, Zifan; Mallen, Alex; Basart, Steven; Koyejo, Sanmi; Song, Dawn; Fredrikson, Matt; Kolter, J. Zico; Hendrycks, Dan","Representation Engineering: A Top-Down Approach to AI Transparency","","","","10.48550/arXiv.2310.01405","http://arxiv.org/abs/2310.01405","In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.","2023-10-10","2024-07-14 10:12:08","2024-07-14 10:12:08","2024-07-14 10:12:08","","","","","","","Representation Engineering","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2310.01405 [cs]","","/home/lexi/.zotero-data/storage/D8NAW3J3/Zou et al. - 2023 - Representation Engineering A Top-Down Approach to.pdf; /home/lexi/.zotero-data/storage/5BLH7AEU/2310.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2310.01405","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q4GWAU2F","preprint","2021","Zhou, Sharon; Zelikman, Eric; Lu, Fred; Ng, Andrew Y.; Carlsson, Gunnar; Ermon, Stefano","Evaluating the Disentanglement of Deep Generative Models through Manifold Topology","","","","10.48550/arXiv.2006.03680","http://arxiv.org/abs/2006.03680","Learning disentangled representations is regarded as a fundamental task for improving the generalization, robustness, and interpretability of generative models. However, measuring disentanglement has been challenging and inconsistent, often dependent on an ad-hoc external model or specific to a certain dataset. To address this, we present a method for quantifying disentanglement that only uses the generative model, by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. To illustrate the effectiveness and applicability of our method, we empirically evaluate several state-of-the-art models across multiple datasets. We find that our method ranks models similarly to existing methods. We make ourcode publicly available at https://github.com/stanfordmlgroup/disentanglement.","2021-03-17","2024-07-14 15:33:43","2024-07-14 15:33:44","2024-07-14 15:33:43","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2006.03680 [cs, stat]","","/home/lexi/.zotero-data/storage/PSPQXJ5U/Zhou et al. - 2021 - Evaluating the Disentanglement of Deep Generative .pdf; /home/lexi/.zotero-data/storage/7J9DK2EI/2006.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2006.03680","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KFQJTQRR","preprint","2018","Khrulkov, Valentin; Oseledets, Ivan","Geometry Score: A Method For Comparing Generative Adversarial Networks","","","","10.48550/arXiv.1802.02664","http://arxiv.org/abs/1802.02664","One of the biggest challenges in the research of generative adversarial networks (GANs) is assessing the quality of generated samples and detecting various levels of mode collapse. In this work, we construct a novel measure of performance of a GAN by comparing geometrical properties of the underlying data manifold and the generated one, which provides both qualitative and quantitative means for evaluation. Our algorithm can be applied to datasets of an arbitrary nature and is not limited to visual data. We test the obtained metric on various real-life models and datasets and demonstrate that our method provides new insights into properties of GANs.","2018-06-09","2024-07-14 15:34:13","2024-07-14 15:34:13","2024-07-14 15:34:13","","","","","","","Geometry Score","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1802.02664 [cs, stat]","","/home/lexi/.zotero-data/storage/9L6XRUCE/Khrulkov and Oseledets - 2018 - Geometry Score A Method For Comparing Generative .pdf; /home/lexi/.zotero-data/storage/9WRYCX5Q/1802.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1802.02664","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"676IH7CV","preprint","2024","Li, Kenneth; Patel, Oam; Viégas, Fernanda; Pfister, Hanspeter; Wattenberg, Martin","Inference-Time Intervention: Eliciting Truthful Answers from a Language Model","","","","10.48550/arXiv.2306.03341","http://arxiv.org/abs/2306.03341","We introduce Inference-Time Intervention (ITI), a technique designed to enhance the ""truthfulness"" of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.","2024-06-26","2024-07-14 16:20:59","2024-07-14 16:20:59","2024-07-14 16:20:59","","","","","","","Inference-Time Intervention","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.03341 [cs]","","/home/lexi/.zotero-data/storage/WJ672HKL/Li et al. - 2024 - Inference-Time Intervention Eliciting Truthful An.pdf; /home/lexi/.zotero-data/storage/R6M67XAQ/2306.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2306.03341","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KHHWIEYG","preprint","2021","White, Jennifer C.; Pimentel, Tiago; Saphra, Naomi; Cotterell, Ryan","A Non-Linear Structural Probe","","","","10.48550/arXiv.2105.10185","http://arxiv.org/abs/2105.10185","Probes are models devised to investigate the encoding of knowledge -- e.g. syntactic structure -- in contextual representations. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information; one such restriction is linearity. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations through learning only linear transformations. By observing that the structural probe learns a metric, we are able to kernelize it and develop a novel non-linear variant with an identical number of parameters. We test on 6 languages and find that the radial-basis function (RBF) kernel, in conjunction with regularization, achieves a statistically significant improvement over the baseline in all languages -- implying that at least part of the syntactic knowledge is encoded non-linearly. We conclude by discussing how the RBF kernel resembles BERT's self-attention layers and speculate that this resemblance leads to the RBF-based probe's stronger performance.","2021-05-21","2024-07-14 19:43:03","2024-07-15 04:46:18","2024-07-14 19:43:03","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2105.10185 [cs]","","/home/lexi/.zotero-data/storage/IYEPLWQ9/White et al. - 2021 - A Non-Linear Structural Probe.pdf; /home/lexi/.zotero-data/storage/ZA6ESJ3L/2105.html","","soon","","","","","","","","","","","","","","","","","","","","arXiv:2105.10185","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JRZEZILL","preprint","2024","Chen, Zhongzhi; Sun, Xingwu; Jiao, Xianfeng; Lian, Fengzong; Kang, Zhanhui; Wang, Di; Xu, Cheng-Zhong","Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning","","","","10.48550/arXiv.2312.17484","http://arxiv.org/abs/2312.17484","Despite the great success of large language models (LLMs) in various tasks, they suffer from generating hallucinations. We introduce Truth Forest, a method that enhances truthfulness in LLMs by uncovering hidden truth representations using multi-dimensional orthogonal probes. Specifically, it creates multiple orthogonal bases for modeling truth by incorporating orthogonal constraints into the probes. Moreover, we introduce Random Peek, a systematic technique considering an extended range of positions within the sequence, reducing the gap between discerning and generating truth features in LLMs. By employing this approach, we improved the truthfulness of Llama-2-7B from 40.8\% to 74.5\% on TruthfulQA. Likewise, significant improvements are observed in fine-tuned models. We conducted a thorough analysis of truth features using probes. Our visualization results show that orthogonal probes capture complementary truth-related features, forming well-defined clusters that reveal the inherent structure of the dataset.","2024-01-14","2024-07-14 19:46:23","2024-07-14 19:46:23","2024-07-14 19:46:23","","","","","","","Truth Forest","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2312.17484 [cs]","","/home/lexi/.zotero-data/storage/YHALTT3X/Chen et al. - 2024 - Truth Forest Toward Multi-Scale Truthfulness in L.pdf; /home/lexi/.zotero-data/storage/D9ZX99GZ/2312.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2312.17484","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TH4YWIPS","journalArticle","2023","Harding, Jacqueline","Operationalising Representation in Natural Language Processing","The British Journal for the Philosophy of Science","","0007-0882, 1464-3537","10.1086/728685","http://arxiv.org/abs/2306.08193","Despite its centrality in the philosophy of cognitive science, there has been little prior philosophical work engaging with the notion of representation in contemporary NLP practice. This paper attempts to fill that lacuna: drawing on ideas from cognitive science, I introduce a framework for evaluating the representational claims made about components of neural NLP models, proposing three criteria with which to evaluate whether a component of a model represents a property and operationalising these criteria using probing classifiers, a popular analysis technique in NLP (and deep learning more broadly). The project of operationalising a philosophically-informed notion of representation should be of interest to both philosophers of science and NLP practitioners. It affords philosophers a novel testing-ground for claims about the nature of representation, and helps NLPers organise the large literature on probing experiments, suggesting novel avenues for empirical research.","2023-11-10","2024-07-14 20:37:48","2024-07-14 20:38:00","2024-07-14 20:37:48","728685","","","","","The British Journal for the Philosophy of Science","","","","","","","","","","","","","arXiv.org","","arXiv:2306.08193 [cs]","","/home/lexi/.zotero-data/storage/LIJLRGRR/Harding - 2023 - Operationalising Representation in Natural Languag.pdf; /home/lexi/.zotero-data/storage/NGYEHBPL/2306.html","","soon","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QAEPGDAB","preprint","2024","Glenn, Susan; Cisewski-Kehe, Jessi; Zhu, Jun; Bement, William M.","Confidence regions for a persistence diagram of a single image with one or more loops","","","","10.48550/arXiv.2405.01651","http://arxiv.org/abs/2405.01651","Topological data analysis (TDA) uses persistent homology to quantify loops and higher-dimensional holes in data, making it particularly relevant for examining the characteristics of images of cells in the field of cell biology. In the context of a cell injury, as time progresses, a wound in the form of a ring emerges in the cell image and then gradually vanishes. Performing statistical inference on this ring-like pattern in a single image is challenging due to the absence of repeated samples. In this paper, we develop a novel framework leveraging TDA to estimate underlying structures within individual images and quantify associated uncertainties through confidence regions. Our proposed method partitions the image into the background and the damaged cell regions. Then pixels within the affected cell region are used to establish confidence regions in the space of persistence diagrams (topological summary statistics). The method establishes estimates on the persistence diagrams which correct the bias of traditional TDA approaches. A simulation study is conducted to evaluate the coverage probabilities of the proposed confidence regions in comparison to an alternative approach is proposed in this paper. We also illustrate our methodology by a real-world example provided by cell repair.","2024-05-02","2024-07-15 13:10:00","2024-07-15 13:10:01","2024-07-15 13:10:00","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2405.01651 [stat]","","/home/lexi/.zotero-data/storage/GJEJHRJ7/Glenn et al. - 2024 - Confidence regions for a persistence diagram of a .pdf; /home/lexi/.zotero-data/storage/FTUZHHLV/2405.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2405.01651","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CWNNTA4K","preprint","2019","Vig, Jesse; Belinkov, Yonatan","Analyzing the Structure of Attention in a Transformer Language Model","","","","10.48550/arXiv.1906.04284","http://arxiv.org/abs/1906.04284","The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.","2019-06-18","2024-07-15 13:11:12","2024-07-15 13:11:12","2024-07-15 13:11:12","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1906.04284 [cs, stat]","","/home/lexi/.zotero-data/storage/JT2V24V5/Vig and Belinkov - 2019 - Analyzing the Structure of Attention in a Transfor.pdf; /home/lexi/.zotero-data/storage/F3W7RKHL/1906.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1906.04284","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WXZVQSPD","webpage","","Alammar, Jay","The Illustrated Transformer","","","","","https://jalammar.github.io/illustrated-transformer/","Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese Watch: MIT’s Deep Learning State of the Art lecture referencing this post Featured in courses at Stanford, Harvard, MIT, Princeton, CMU and others In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions. The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter. 2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic: A High-Level Look Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.","","2024-07-15 13:12:25","2024-07-15 13:12:25","2024-07-15 13:12:25","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/IDQGMXMA/illustrated-transformer.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AD8GX2NF","blogPost","2024","yaadav, vignesh","Exploring and building the LLaMA 3 Architecture : A Deep Dive into Components, Coding, and…","Medium","","","","https://medium.com/@vi.ai_/exploring-and-building-the-llama-3-architecture-a-deep-dive-into-components-coding-and-43d4097cfbbb","Meta is stepping up its game in the artificial intelligence (AI) race with the introduction of its new open-source AI model, Llama 3…","2024-04-25","2024-07-15 13:13:38","2024-07-15 13:13:38","2024-07-15 13:13:38","","","","","","","Exploring and building the LLaMA 3 Architecture","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZSL7TRSE","webpage","","Alammar, Jay","How GPT3 Works - Visualizations and Animations","","","","","https://jalammar.github.io/how-gpt3-works-visualizations-animations/","Discussions: Hacker News (397 points, 97 comments), Reddit r/MachineLearning (247 points, 27 comments) Translations: German, Korean, Chinese (Simplified), Russian, Turkish The tech world is abuzz with GPT3 hype. Massive language models (like GPT3) are starting to surprise us with their abilities. While not yet completely reliable for most businesses to put in front of their customers, these models are showing sparks of cleverness that are sure to accelerate the march of automation and the possibilities of intelligent computer systems. Let’s remove the aura of mystery around GPT3 and learn how it’s trained and how it works. A trained language model generates text. We can optionally pass it some text as input, which influences its output. The output is generated from what the model “learned” during its training period where it scanned vast amounts of text.","","2024-07-15 13:13:44","2024-07-15 13:13:44","2024-07-15 13:13:44","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6L9RXMBJ","webpage","","Alammar, Jay","The Illustrated GPT-2 (Visualizing Transformer Language Models)","","","","","https://jalammar.github.io/illustrated-gpt2/","Discussions: Hacker News (64 points, 3 comments), Reddit r/MachineLearning (219 points, 18 comments) Translations: Simplified Chinese, French, Korean, Russian, Turkish       This year, we saw a dazzling application of machine learning. The OpenAI GPT-2 exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling. My goal here is to also supplement my earlier post, The Illustrated Transformer, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.","","2024-07-15 13:13:50","2024-07-15 13:13:50","2024-07-15 13:13:50","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DPKX3HA3","webpage","","Alammar, Jay","The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)","","","","","https://jalammar.github.io/illustrated-bert/","Discussions: Hacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments) Translations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish 2021 Update: I created this brief and highly accessible video intro to BERT The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).","","2024-07-15 13:14:00","2024-07-15 13:14:00","2024-07-15 13:14:00","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JLL5HBSE","webpage","2024","Weng, Lilian","Extrinsic Hallucinations in LLMs","","","","","https://lilianweng.github.io/posts/2024-07-07-hallucination/","Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge. There are two types of hallucination: In-context hallucination: The model output should be consistent with the source content in context.","2024-07-07","2024-07-15 13:14:41","2024-07-15 13:14:41","2024-07-15 13:14:41","","","","","","","","","","","","","","en","","","","","","","Section: posts","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NCS6L5WK","webpage","","","Intro — mlcourse.ai","","","","","https://mlcourse.ai/book/index.html","","","2024-07-15 13:17:40","2024-07-15 13:17:48","2024-07-15 13:17:40","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/XJTAIYPN/index.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XA2PR9AE","webpage","","","Practice","","","","","https://neetcode.io/practice?subpage=practice&tab=coreSkills&topic=Machine%20Learning","","","2024-07-15 13:18:15","2024-07-15 13:18:15","2024-07-15 13:18:15","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VPXI9DXA","computerProgram","2024","Дьяконов, Alexander D'yakonov (Александр","Dyakonov/MLDM_BOOK","","","","","https://github.com/Dyakonov/MLDM_BOOK","Книга ""Машинное обучение и анализ данных""","2024-07-12","2024-07-15 13:18:33","2024-07-15 13:19:05","2024-07-15 13:18:33","","","","","","","","","","","","","","","","","","","GitHub","","original-date: 2023-09-12T14:31:33Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SC7QNECM","webpage","2024","Котенков, Игорь","Мини-курс «Полная история семейства GPT»","YouTube","","","","http://www.youtube.com/playlist?list=PLy6K3_Hx-udj6n1S88Vslyw2QVxSXLP2c","Share your videos with friends, family, and the world","2024","2024-07-15 13:19:32","2024-07-15 13:20:00","2024-07-15 13:19:32","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LCAMFEFW","preprint","2024","Rajendran, Goutham; Buchholz, Simon; Aragam, Bryon; Schölkopf, Bernhard; Ravikumar, Pradeep","Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models","","","","10.48550/arXiv.2402.09236","http://arxiv.org/abs/2402.09236","To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.","2024-02-14","2024-07-15 13:22:52","2024-07-15 13:23:00","2024-07-15 13:22:51","","","","","","","Learning Interpretable Concepts","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2402.09236 [cs, math, stat]","","/home/lexi/.zotero-data/storage/2HZD387V/Rajendran et al. - 2024 - Learning Interpretable Concepts Unifying Causal R.pdf; /home/lexi/.zotero-data/storage/QTKZ9RKG/2402.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2402.09236","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A3FFV22E","webpage","","","Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch","","","","","https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html","","","2024-07-15 13:25:17","2024-07-15 13:25:24","2024-07-15 13:25:17","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/TYEPA737/self-attention-from-scratch.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QXFXTDKP","computerProgram","2024","Karpathy, Andrej","karpathy/minbpe","","","","","https://github.com/karpathy/minbpe","Minimal, clean code for the Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization.","2024-07-15","2024-07-15 13:33:45","2024-07-15 13:36:31","2024-07-15 13:33:45","","","","","","","","","","","","","","","MIT","","","","GitHub","","original-date: 2024-02-16T16:18:15Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Python","","","","","","","","",""
"U9T9CE3Y","webpage","","","Introduction to Machine Learning Interviews Book · MLIB","","","","","https://huyenchip.com/ml-interviews-book/","","","2024-07-15 13:37:16","2024-07-15 13:37:16","2024-07-15 13:37:16","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/DMGIUNDL/ml-interviews-book.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4RJAAA9Y","bookSection","2022","Berner, Julius; Grohs, Philipp; Kutyniok, Gitta; Petersen, Philipp","The Modern Mathematics of Deep Learning","","","","","http://arxiv.org/abs/2105.04026","We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.","2022-12-22","2024-07-15 20:20:04","2024-07-15 20:20:05","2024-07-15 20:20:04","1-111","","","","","","","","","","","","","","","","","","arXiv.org","","DOI: 10.1017/9781009025096.002 arXiv:2105.04026 [cs, stat]","","/home/lexi/.zotero-data/storage/7Y7P2E45/Berner et al. - 2022 - The Modern Mathematics of Deep Learning.pdf; /home/lexi/.zotero-data/storage/MZN6EBYG/2105.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6QUYHRRC","preprint","2023","Jentzen, Arnulf; Kuckuck, Benno; von Wurstemberger, Philippe","Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory","","","","10.48550/arXiv.2310.20360","http://arxiv.org/abs/2310.20360","This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet have any background in deep learning at all and would like to gain a solid foundation as well as for practitioners who would like to obtain a firmer mathematical understanding of the objects and methods considered in deep learning.","2023-10-31","2024-07-15 20:21:06","2024-07-15 20:21:06","2024-07-15 20:21:06","","","","","","","Mathematical Introduction to Deep Learning","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2310.20360 [cs, math, stat]","","/home/lexi/.zotero-data/storage/TXMPIYXD/Jentzen et al. - 2023 - Mathematical Introduction to Deep Learning Method.pdf; /home/lexi/.zotero-data/storage/FNTDF7QX/2310.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2310.20360","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GRQ8BV85","preprint","2023","Khan, Alif Elham; Hasan, Mohammad Junayed; Anjum, Humayra; Mohammed, Nabeel","Shadow: A Novel Loss Function for Efficient Training in Siamese Networks","","","","10.48550/arXiv.2311.14012","http://arxiv.org/abs/2311.14012","Despite significant recent advances in similarity detection tasks, existing approaches pose substantial challenges under memory constraints. One of the primary reasons for this is the use of computationally expensive metric learning loss functions such as Triplet Loss in Siamese networks. In this paper, we present a novel loss function called Shadow Loss that compresses the dimensions of an embedding space during loss calculation without loss of performance. The distance between the projections of the embeddings is learned from inputs on a compact projection space where distances directly correspond to a measure of class similarity. Projecting on a lower-dimension projection space, our loss function converges faster, and the resulting classified image clusters have higher inter-class and smaller intra-class distances. Shadow Loss not only reduces embedding dimensions favoring memory constraint devices but also consistently performs better than the state-of-the-art Triplet Margin Loss by an accuracy of 5\%-10\% across diverse datasets. The proposed loss function is also model agnostic, upholding its performance across several tested models. Its effectiveness and robustness across balanced, imbalanced, medical, and non-medical image datasets suggests that it is not specific to a particular model or dataset but demonstrates superior performance consistently while using less memory and computation.","2023-11-23","2024-07-16 06:37:51","2024-07-16 06:37:54","2024-07-16 06:37:51","","","","","","","Shadow","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2311.14012 [cs]","","/home/lexi/.zotero-data/storage/JU2444PP/Khan et al. - 2023 - Shadow A Novel Loss Function for Efficient Traini.pdf; /home/lexi/.zotero-data/storage/RKZCDL4N/2311.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2311.14012","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P9JHYGN8","preprint","2023","Rafailov, Rafael; Sharma, Archit; Mitchell, Eric; Ermon, Stefano; Manning, Christopher D.; Finn, Chelsea","Direct Preference Optimization: Your Language Model is Secretly a Reward Model","","","","10.48550/arXiv.2305.18290","http://arxiv.org/abs/2305.18290","While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.","2023-12-13","2024-07-16 06:39:09","2024-07-16 06:39:13","2024-07-16 06:39:09","","","","","","","Direct Preference Optimization","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2305.18290 [cs]","","/home/lexi/.zotero-data/storage/U46W9EQQ/Rafailov et al. - 2023 - Direct Preference Optimization Your Language Mode.pdf; /home/lexi/.zotero-data/storage/2WXCWG49/2305.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2305.18290","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RNGEFD8P","preprint","2023","Azar, Mohammad Gheshlaghi; Rowland, Mark; Piot, Bilal; Guo, Daniel; Calandriello, Daniele; Valko, Michal; Munos, Rémi","A General Theoretical Paradigm to Understand Learning from Human Preferences","","","","10.48550/arXiv.2310.12036","http://arxiv.org/abs/2310.12036","The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation. In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an in-depth analysis of the behavior of RLHF and DPO (as special cases of $\Psi$PO) and to identify their potential pitfalls. We then consider another special case for $\Psi$PO by setting $\Psi$ simply to Identity, for which we can derive an efficient optimisation procedure, prove performance guarantees and demonstrate its empirical superiority to DPO on some illustrative examples.","2023-11-21","2024-07-16 06:39:40","2024-07-16 06:39:40","2024-07-16 06:39:40","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2310.12036 [cs, stat]","","/home/lexi/.zotero-data/storage/Z3RWEVQM/Azar et al. - 2023 - A General Theoretical Paradigm to Understand Learn.pdf; /home/lexi/.zotero-data/storage/QRFULG5N/2310.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2310.12036","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z6WMDBDF","computerProgram","2024","","huggingface/alignment-handbook","","","","","https://github.com/huggingface/alignment-handbook","Robust recipes to align language models with human and AI preferences","2024-07-16","2024-07-16 06:40:26","2024-07-16 06:40:26","2024-07-16 06:40:26","","","","","","","","","","","","Hugging Face","","","Apache-2.0","","","","GitHub","","original-date: 2023-08-25T11:35:34Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Python","","","","","","","","",""
"E4JDSBBC","webpage","","","Preference Tuning LLMs with Direct Preference Optimization Methods","","","","","https://huggingface.co/blog/pref-tuning","We’re on a journey to advance and democratize artificial intelligence through open source and open science.","","2024-07-16 06:40:41","2024-07-16 06:40:41","2024-07-16 06:40:41","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"92E9JRLV","preprint","2024","Wang, Yihan; Si, Si; Li, Daliang; Lukasik, Michal; Yu, Felix; Hsieh, Cho-Jui; Dhillon, Inderjit S.; Kumar, Sanjiv","Two-stage LLM Fine-tuning with Less Specialization and More Generalization","","","","10.48550/arXiv.2211.00635","http://arxiv.org/abs/2211.00635","Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. In this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task.We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format specialization and improves generalization.ProMoT offloads task-specific format learning into additional and removable parameters by first doing prompt tuning and then fine-tuning the model itself with this soft prompt attached. With experiments on several fine-tuning tasks and 8 in-context evaluation tasks, we show that ProMoT achieves comparable performance on fine-tuned tasks to standard fine-tuning, but with much less loss of in-context learning performances across a board range of out-of-domain evaluation tasks. More importantly, ProMoT can even enhance generalization on in-context learning tasks that are semantically related to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly improves performance on other language pairs, and ProMoT on NLI improves performance on summarization. Experiments also show that ProMoT can improve the generalization performance of multi-task training.","2024-03-12","2024-07-16 16:19:22","2024-07-16 16:19:22","2024-07-16 16:19:21","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2211.00635 [cs] version: 3","","/home/lexi/.zotero-data/storage/DFHL8A7H/Wang et al. - 2024 - Two-stage LLM Fine-tuning with Less Specialization.pdf; /home/lexi/.zotero-data/storage/K4REBFRH/2211.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2211.00635","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U6NSTYEH","preprint","2022","ValizadehAslani, Taha; Shi, Yiwen; Wang, Jing; Ren, Ping; Zhang, Yi; Hu, Meng; Zhao, Liang; Liang, Hualou","Two-Stage Fine-Tuning: A Novel Strategy for Learning Class-Imbalanced Data","","","","10.48550/arXiv.2207.10858","http://arxiv.org/abs/2207.10858","Classification on long-tailed distributed data is a challenging problem, which suffers from serious class-imbalance and hence poor performance on tail classes with only a few samples. Owing to this paucity of samples, learning on the tail classes is especially challenging for the fine-tuning when transferring a pretrained model to a downstream task. In this work, we present a simple modification of standard fine-tuning to cope with these challenges. Specifically, we propose a two-stage fine-tuning: we first fine-tune the final layer of the pretrained model with class-balanced reweighting loss, and then we perform the standard fine-tuning. Our modification has several benefits: (1) it leverages pretrained representations by only fine-tuning a small portion of the model parameters while keeping the rest untouched; (2) it allows the model to learn an initial representation of the specific task; and importantly (3) it protects the learning of tail classes from being at a disadvantage during the model updating. We conduct extensive experiments on synthetic datasets of both two-class and multi-class tasks of text classification as well as a real-world application to ADME (i.e., absorption, distribution, metabolism, and excretion) semantic labeling. The experimental results show that the proposed two-stage fine-tuning outperforms both fine-tuning with conventional loss and fine-tuning with a reweighting loss on the above datasets.","2022-07-21","2024-07-16 16:19:54","2024-07-16 16:19:54","2024-07-16 16:19:54","","","","","","","Two-Stage Fine-Tuning","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2207.10858 [cs]","","/home/lexi/.zotero-data/storage/QKGEADYA/ValizadehAslani et al. - 2022 - Two-Stage Fine-Tuning A Novel Strategy for Learni.pdf; /home/lexi/.zotero-data/storage/KAJNNT75/2207.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2207.10858","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XXWZD6X5","book","2020","Deisenroth, Marc Peter; Faisal, A. Aldo; Ong, Cheng Soon","Mathematics for Machine Learning","","978-1-108-67993-0 978-1-108-47004-9 978-1-108-45514-5","","","https://www.cambridge.org/highereducation/books/mathematics-for-machine-learning/5EE57FD1CFB23E6EB11E130309C7EF98#contents","","2020-02-29","2024-07-16 16:26:03","2024-07-16 16:26:03","2024-07-16 16:26:03","","","","","","","","","","","","Cambridge University Press","","","https://www.cambridge.org/core/terms","","","","DOI.org (Crossref)","","DOI: 10.1017/9781108679930","","/home/lexi/.zotero-data/storage/QKV97L22/Deisenroth et al. - 2020 - Mathematics for Machine Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","1","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IUE4J9VC","book","2022","Murphy, Kevin P.","Probabilistic machine learning: an introduction","","978-0-262-04682-4","","","","""This book provides a detailed and up-to-date coverage of machine learning. It is unique in that it unifies approaches based on deep learning with approaches based on probabilistic modeling and inference. It provides mathematical background (e.g. linear algebra, optimization), basic topics (e.g., linear and logistic regression, deep neural networks), as well as more advanced topics (e.g., Gaussian processes). It provides a perfect introduction for people who want to understand cutting edge work in top machine learning conferences such as NeurIPS, ICML and ICLR"".--","2022","2024-07-16 16:27:16","2024-07-16 16:27:16","","","826","","","","","Probabilistic machine learning","Adaptive computation and machine learning","","","","The MIT Press","Cambridge, Massachusetts London, England","eng","","","","","K10plus ISBN","","","","/home/lexi/.zotero-data/storage/CJEB8JJC/3 - Probabilistic Machine Learning [томище].pdf; /home/lexi/.zotero-data/storage/N6N3ZG3M/Murphy - 2022 - Probabilistic machine learning an introduction.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PZI5M8FE","webpage","2022","tanksale, nachiket","Finding Good Learning Rate and The One Cycle Policy.","Medium","","","","https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6","Introduction","2022-10-19","2024-07-17 15:59:22","2024-07-17 15:59:22","2024-07-17 15:59:22","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NSR9TSNE","journalArticle","2008","Maaten, Laurens van der; Hinton, Geoffrey","Visualizing Data using t-SNE","Journal of Machine Learning Research","","1533-7928","","http://jmlr.org/papers/v9/vandermaaten08a.html","We present a new technique called ""t-SNE"" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.","2008","2024-07-20 20:08:23","2024-07-20 20:08:44","2024-07-20 20:08:23","2579-2605","","86","9","","","","","","","","","","","","","","","jmlr.org","","","","/home/lexi/.zotero-data/storage/VZVAECEV/Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BSE5HNP3","preprint","2022","Cai, T. Tony; Ma, Rong","Theoretical Foundations of t-SNE for Visualizing High-Dimensional Clustered Data","","","","10.48550/arXiv.2105.07536","http://arxiv.org/abs/2105.07536","This paper investigates the theoretical foundations of the t-distributed stochastic neighbor embedding (t-SNE) algorithm, a popular nonlinear dimension reduction and data visualization method. A novel theoretical framework for the analysis of t-SNE based on the gradient descent approach is presented. For the early exaggeration stage of t-SNE, we show its asymptotic equivalence to power iterations based on the underlying graph Laplacian, characterize its limiting behavior, and uncover its deep connection to Laplacian spectral clustering, and fundamental principles including early stopping as implicit regularization. The results explain the intrinsic mechanism and the empirical benefits of such a computational strategy. For the embedding stage of t-SNE, we characterize the kinematics of the low-dimensional map throughout the iterations, and identify an amplification phase, featuring the intercluster repulsion and the expansive behavior of the low-dimensional map, and a stabilization phase. The general theory explains the fast convergence rate and the exceptional empirical performance of t-SNE for visualizing clustered data, brings forth interpretations of the t-SNE visualizations, and provides theoretical guidance for applying t-SNE and selecting its tuning parameters in various applications.","2022-10-31","2024-07-20 20:10:17","2024-07-20 20:10:17","2024-07-20 20:10:17","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2105.07536 [cs, math, stat]","","/home/lexi/.zotero-data/storage/54PK445I/Cai and Ma - 2022 - Theoretical Foundations of t-SNE for Visualizing H.pdf; /home/lexi/.zotero-data/storage/6XJJCC9R/2105.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2105.07536","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TS3DMZUI","webpage","","","Visualizing MNIST: An Exploration of Dimensionality Reduction - colah's blog","","","","","https://colah.github.io/posts/2014-10-Visualizing-MNIST/","","","2024-07-20 20:20:00","2024-07-20 20:20:08","2024-07-20 20:20:00","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/68JR54W3/2014-10-Visualizing-MNIST.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MJVPGV7H","webpage","2024","","Как развивалась технология экстремального сжатия LLM: от QuIP до AQLM с PV-tuning","Хабр","","","","https://habr.com/ru/companies/yandex/articles/830410/","Мы живём в эпоху LLM — компании применяют на практике всё более крупные модели с миллиардами параметров. Это здорово, потому что большие модели открывают пользователям сервисов новые возможности, но...","2024-07-23","2024-07-23 13:10:38","2024-07-23 13:10:38","2024-07-23 13:10:38","","","","","","","Как развивалась технология экстремального сжатия LLM","","","","","","","ru","","","","","","","","","/home/lexi/.zotero-data/storage/5E6HTVZT/830410.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IE5NG5X5","preprint","2024","Liu, Chen Cecilia; Pfeiffer, Jonas; Vulić, Ivan; Gurevych, Iryna","FUN with Fisher: Improving Generalization of Adapter-Based Cross-lingual Transfer with Scheduled Unfreezing","","","","10.48550/arXiv.2301.05487","http://arxiv.org/abs/2301.05487","Standard fine-tuning of language models typically performs well on in-distribution data, but suffers with generalization to distribution shifts. In this work, we aim to improve the generalization of adapter-based cross-lingual task transfer where such cross-language distribution shifts are imminent. We investigate scheduled unfreezing algorithms -- originally proposed to mitigate catastrophic forgetting in transfer learning -- for fine-tuning task adapters. Our experiments show that scheduled unfreezing methods close the gap to full fine-tuning and achieve stronger cross-lingual transfer performance, suggesting that these methods can go beyond just mitigating catastrophic forgetting. Next, aiming to understand these empirical findings, we investigate the learning dynamics of scheduled unfreezing using Fisher Information. Our experiments reveal that scheduled unfreezing induces different learning dynamics compared to standard fine-tuning, and provide evidence that the dynamics of Fisher Information during training correlate with cross-lingual generalization performance. We additionally propose a general scheduled unfreezing algorithm that achieves an average of 2 points improvement over four datasets compared to standard fine-tuning and provides empirical evidence for a theory-based justification of the heuristic unfreezing schedule for adapter training.","2024-04-04","2024-07-26 06:38:50","2024-07-26 06:38:50","2024-07-26 06:38:50","","","","","","","FUN with Fisher","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2301.05487 [cs]","","/home/lexi/.zotero-data/storage/EQ2E2STP/Liu et al. - 2024 - FUN with Fisher Improving Generalization of Adapt.pdf; /home/lexi/.zotero-data/storage/GYRTT8NA/2301.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2301.05487","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7UQLKXX3","journalArticle","2016","Wattenberg, Martin; Viégas, Fernanda; Johnson, Ian","How to Use t-SNE Effectively","Distill","","2476-0757","10.23915/distill.00002","http://distill.pub/2016/misread-tsne","Although extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading.","2016-10-13","2024-07-20 20:18:31","2025-06-13 14:17:31","2024-07-26 19:58:37","e2","","10","1","","Distill","","","","","","","","en","","","","","distill.pub","","","","/home/lexi/.zotero-data/storage/NRYGBRGL/Wattenberg et al. - 2016 - How to Use t-SNE Effectively.pdf; /home/lexi/.zotero-data/storage/DBF27QPM/misread-tsne.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BSWMDHJ8","webpage","","","2.2. Manifold learning","scikit-learn","","","","https://scikit-learn/stable/modules/manifold.html","Look for the bare necessities, The simple bare necessities, Forget about your worries and your strife, I mean the bare necessities, Old Mother Nature’s recipes, That bring the bare necessities of l...","","2024-07-26 19:58:40","2024-07-26 19:58:40","2024-07-26 19:58:40","","","","","","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QCMQDVNV","computerProgram","2024","Ulyanov, Dmitry","DmitryUlyanov/Multicore-TSNE","","","","","https://github.com/DmitryUlyanov/Multicore-TSNE","Parallel t-SNE implementation with Python and Torch wrappers.","2024-07-19","2024-07-26 19:59:01","2024-07-26 19:59:01","2024-07-26 19:59:01","","","","","","","","","","","","","","","","","","","GitHub","","original-date: 2016-10-19T05:46:52Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","C++","","","","","","","","",""
"N9TZMMXG","webpage","1917","Шкловский, Виктор","Искусство как прием","","","","","https://www.opojaz.ru/manifests/kakpriem.html#Anchor-%1CKA;8%20%3E1%20M-46888","","1917","2024-09-04 07:25:56","2024-09-04 07:27:13","2024-09-04 07:25:56","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/6CP6QRLW/kakpriem.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IWMBYSXV","webpage","1918","Эйхенбаум, Борис","Как сделана „Шинель“ Гоголя","","","","","https://www.opojaz.ru/manifests/kaksdelana.html","","1918","2024-09-15 10:42:18","2024-09-15 10:42:51","2024-09-15 10:42:18","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/H3UTEC3L/kaksdelana.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SSKA22NT","journalArticle","1975","Якобсон, Роман","Лингвистика и поэтика","Структурализм: ""за"" и ""против""","","","","https://philologos.narod.ru/classics/jakobson-lp.htm","","1975","2024-09-29 20:08:57","2024-09-29 20:10:24","2024-09-29 20:08:57","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/MHSRS9G3/jakobson-lp.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T5VXHWDI","book","1914","Шкловский, Виктор","Воскрешение Слова","","","","","https://rusneb.ru/catalog/000199_000009_004197043/","Воскрешение Слова — Шкловский В.Б., 16 с. (1914). Место хранения оригинала - Российская государственная библиотека (РГБ). Код оцифрованного документа в НЭБ: 000199_000009_004197043","1914","2024-10-01 20:41:15","2024-10-01 20:41:48","2024-10-01 20:41:15","","","","","","","","","","","","","","ru","","","","","rusneb.ru","","","","/home/lexi/.zotero-data/storage/UF2T8KMV/000199_000009_004197043.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RSNW3Q45","journalArticle","1927","Эйхенбаум, Борис","Литература и литературный быт","На литературном посту","","","","http://vivovoco.astronet.ru/VV/PAPERS/LITRA/LITBYT.HTM","","1927","2024-10-02 11:13:49","2024-10-02 11:14:31","2024-10-02 11:13:49","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/XEZ6MWEV/LITBYT.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DSB4J5J8","journalArticle","1924","Тынянов, Юрий","Литературный факт","Леф","","","","https://philologos.narod.ru/tynyanov/pilk/poet4.htm","","1924","2024-10-02 11:52:36","2024-10-02 11:53:10","2024-10-02 11:52:36","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/3QSQUMA2/poet4.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8TWU2S92","journalArticle","1928","Гуковский, Григорий","К вопросу о русском классицизме","Поэтика. Временник отдела словесных искусств ГИИИ","","","","","","1928","2024-10-05 09:59:11","2024-10-05 10:00:11","","","","","4","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/T9ZV6KTP/Гуковский - 1928 - К вопросу о русском классицизме.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6YGN8R2M","journalArticle","1929","Гуковский, Григорий","О русском классицизме","Поэтика. Временник отдела словесных искусств ГИИИ","","","","","","1929","2024-10-05 10:00:42","2024-10-05 10:01:08","","","","","5","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/CJPJZGRB/Гуковский - 1929 - О русском классицизме.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M9G6MYUN","journalArticle","1924","Эйхенбаум, Борис","В поисках жанра","Русский современник","","","","","","1924","2024-10-05 18:08:40","2024-10-05 18:09:15","","","","","3","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/J6A5P7G9/Эйхенбаум - 1924 - В поисках жанра.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EZAXRPVW","journalArticle","1977","Лотман, Юрий","Текст и структура аудитории","","","","","https://historicus.ru/tekst_i_struktura_auditorii","","1977","2024-10-09 12:40:35","2024-10-09 12:41:13","2024-10-09 12:40:35","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/MSHKSNRM/tekst_i_struktura_auditorii.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"954EDCFG","webpage","","","Р.Якобсон. Грамматический параллелизм и его русские аспекты","","","","","https://philologos.narod.ru/classics/jakparal/jakparal.htm","","","2024-10-09 14:19:13","2024-10-09 14:19:13","2024-10-09 14:19:13","","","","","","","","","","","","","","","","","","","","","","","/home/lexi/.zotero-data/storage/XCCHBALW/jakparal.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TQX8ALI9","webpage","2024","Алмаев, Максим; Разволодин, Дмитрий","Как мы внедрили генеративную модель в объявления на Авто.ру. Доклад Яндекса","Хабр","","","","https://habr.com/ru/companies/yandex/articles/850902/","Фронтенд‑разработчики из&nbsp;Авто.ру Максим Алмаев и Дмитрий Размолодин рассказали на&nbsp;внутреннем митапе, как&nbsp;их команда запустила генерацию описаний машин в&nbsp;помощь тем, кто публикует...","2024-10-18","2024-10-22 08:00:36","2024-10-22 08:01:29","2024-10-22 08:00:36","","","","","","","","","","","","","","ru","","","","","","","","","/home/lexi/.zotero-data/storage/RF59295Y/850902.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7TZBXG4","videoRecording","","","Мастерство промпт-инжиниринга","","","","","https://www.youtube.com/watch?v=ttZSnuO-xXI","На вебинаре рассказали, как правильно составить запрос к языковой модели, чтобы бизнес получил максимум пользы. Зрители узнали о методах промпт-инжиниринга, ...","","2024-10-22 08:09:33","2024-10-22 08:09:33","2024-10-22 08:09:33","","","","","","","","","","","","","","en","","","","","www.youtube.com","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HVPE8KEL","journalArticle","1968","Барт, Ролан","Смерть автора","","","","","","","1968","2024-10-29 07:30:55","2024-10-29 07:31:22","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N8MLTUM3","journalArticle","1938","Бахтин, Михаил","Формы времени и хронотопа в романе","","","","","","","1938","2024-10-29 15:53:14","2024-10-29 15:54:10","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"56N6DQZH","preprint","2024","Zhou, Zhanchao; Wu, Tianyi; Jiang, Zhiyun; Lan, Zhenzhong","Value Residual Learning For Alleviating Attention Concentration In Transformers","","","","10.48550/arXiv.2410.17897","http://arxiv.org/abs/2410.17897","Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding a residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer. Comprehensive empirical evidence demonstrates ResFormer achieves equivalent validation loss with 10.4% fewer model parameters and 13.6% less training data compared to Transformer, while maintaining similar memory usage and computational cost. Besides, SVFormer reduces KV cache size by nearly half with only a small performance penalty and can be integrated with other KV-efficient methods, yielding further reductions in KV cache, with performance influenced by sequence length and cumulative learning rate. Further visualization results suggest that Resformer and SVFormer alleviate attention concentration in deeper layers through avoiding value-state drains and enhance representation across most layers.","2024-12-03","2024-12-30 13:13:55","2024-12-30 13:14:06","2024-12-30 13:13:55","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2410.17897 [cs]","","/home/lexi/.zotero-data/storage/BRGES8JU/Zhou et al. - 2024 - Value Residual Learning For Alleviating Attention .pdf; /home/lexi/.zotero-data/storage/79KV6GXB/2410.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2410.17897","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WHBQ3KLD","preprint","2024","Team, Gemma; Riviere, Morgane; Pathak, Shreya; Sessa, Pier Giuseppe; Hardin, Cassidy; Bhupatiraju, Surya; Hussenot, Léonard; Mesnard, Thomas; Shahriari, Bobak; Ramé, Alexandre; Ferret, Johan; Liu, Peter; Tafti, Pouya; Friesen, Abe; Casbon, Michelle; Ramos, Sabela; Kumar, Ravin; Lan, Charline Le; Jerome, Sammy; Tsitsulin, Anton; Vieillard, Nino; Stanczyk, Piotr; Girgin, Sertan; Momchev, Nikola; Hoffman, Matt; Thakoor, Shantanu; Grill, Jean-Bastien; Neyshabur, Behnam; Bachem, Olivier; Walton, Alanna; Severyn, Aliaksei; Parrish, Alicia; Ahmad, Aliya; Hutchison, Allen; Abdagic, Alvin; Carl, Amanda; Shen, Amy; Brock, Andy; Coenen, Andy; Laforge, Anthony; Paterson, Antonia; Bastian, Ben; Piot, Bilal; Wu, Bo; Royal, Brandon; Chen, Charlie; Kumar, Chintu; Perry, Chris; Welty, Chris; Choquette-Choo, Christopher A.; Sinopalnikov, Danila; Weinberger, David; Vijaykumar, Dimple; Rogozińska, Dominika; Herbison, Dustin; Bandy, Elisa; Wang, Emma; Noland, Eric; Moreira, Erica; Senter, Evan; Eltyshev, Evgenii; Visin, Francesco; Rasskin, Gabriel; Wei, Gary; Cameron, Glenn; Martins, Gus; Hashemi, Hadi; Klimczak-Plucińska, Hanna; Batra, Harleen; Dhand, Harsh; Nardini, Ivan; Mein, Jacinda; Zhou, Jack; Svensson, James; Stanway, Jeff; Chan, Jetha; Zhou, Jin Peng; Carrasqueira, Joana; Iljazi, Joana; Becker, Jocelyn; Fernandez, Joe; Amersfoort, Joost van; Gordon, Josh; Lipschultz, Josh; Newlan, Josh; Ji, Ju-yeong; Mohamed, Kareem; Badola, Kartikeya; Black, Kat; Millican, Katie; McDonell, Keelin; Nguyen, Kelvin; Sodhia, Kiranbir; Greene, Kish; Sjoesund, Lars Lowe; Usui, Lauren; Sifre, Laurent; Heuermann, Lena; Lago, Leticia; McNealus, Lilly; Soares, Livio Baldini; Kilpatrick, Logan; Dixon, Lucas; Martins, Luciano; Reid, Machel; Singh, Manvinder; Iverson, Mark; Görner, Martin; Velloso, Mat; Wirth, Mateo; Davidow, Matt; Miller, Matt; Rahtz, Matthew; Watson, Matthew; Risdal, Meg; Kazemi, Mehran; Moynihan, Michael; Zhang, Ming; Kahng, Minsuk; Park, Minwoo; Rahman, Mofi; Khatwani, Mohit; Dao, Natalie; Bardoliwalla, Nenshad; Devanathan, Nesh; Dumai, Neta; Chauhan, Nilay; Wahltinez, Oscar; Botarda, Pankil; Barnes, Parker; Barham, Paul; Michel, Paul; Jin, Pengchong; Georgiev, Petko; Culliton, Phil; Kuppala, Pradeep; Comanescu, Ramona; Merhej, Ramona; Jana, Reena; Rokni, Reza Ardeshir; Agarwal, Rishabh; Mullins, Ryan; Saadat, Samaneh; Carthy, Sara Mc; Cogan, Sarah; Perrin, Sarah; Arnold, Sébastien M. R.; Krause, Sebastian; Dai, Shengyang; Garg, Shruti; Sheth, Shruti; Ronstrom, Sue; Chan, Susan; Jordan, Timothy; Yu, Ting; Eccles, Tom; Hennigan, Tom; Kocisky, Tomas; Doshi, Tulsee; Jain, Vihan; Yadav, Vikas; Meshram, Vilobh; Dharmadhikari, Vishal; Barkley, Warren; Wei, Wei; Ye, Wenming; Han, Woohyun; Kwon, Woosuk; Xu, Xiang; Shen, Zhe; Gong, Zhitao; Wei, Zichuan; Cotruta, Victor; Kirk, Phoebe; Rao, Anand; Giang, Minh; Peran, Ludovic; Warkentin, Tris; Collins, Eli; Barral, Joelle; Ghahramani, Zoubin; Hadsell, Raia; Sculley, D.; Banks, Jeanine; Dragan, Anca; Petrov, Slav; Vinyals, Oriol; Dean, Jeff; Hassabis, Demis; Kavukcuoglu, Koray; Farabet, Clement; Buchatskaya, Elena; Borgeaud, Sebastian; Fiedel, Noah; Joulin, Armand; Kenealy, Kathleen; Dadashi, Robert; Andreev, Alek","Gemma 2: Improving Open Language Models at a Practical Size","","","","10.48550/arXiv.2408.00118","http://arxiv.org/abs/2408.00118","In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3 times bigger. We release all our models to the community.","2024-10-02","2024-12-30 13:14:36","2024-12-30 13:14:36","2024-12-30 13:14:36","","","","","","","Gemma 2","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2408.00118 [cs]","","/home/lexi/.zotero-data/storage/7ADHRZGM/Team et al. - 2024 - Gemma 2 Improving Open Language Models at a Pract.pdf; /home/lexi/.zotero-data/storage/HJRLT9WR/2408.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2408.00118","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QE6DQGQN","webpage","2024","","AlphaFold 3 predicts the structure and interactions of all of life’s molecules","Google","","","","https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/","Our new AI model AlphaFold 3 can predict the structure and interactions of all life’s molecules with unprecedented accuracy.","2024-05-08","2024-12-30 13:34:36","2024-12-30 13:34:36","2024-12-30 13:34:36","","","","","","","","","","","","","","en-us","","","","","","","","","/home/lexi/.zotero-data/storage/QVQ4HI69/google-deepmind-isomorphic-alphafold-3-ai-model.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KBKTBSK8","journalArticle","2024","Abramson, Josh; Adler, Jonas; Dunger, Jack; Evans, Richard; Green, Tim; Pritzel, Alexander; Ronneberger, Olaf; Willmore, Lindsay; Ballard, Andrew J.; Bambrick, Joshua; Bodenstein, Sebastian W.; Evans, David A.; Hung, Chia-Chun; O’Neill, Michael; Reiman, David; Tunyasuvunakool, Kathryn; Wu, Zachary; Žemgulytė, Akvilė; Arvaniti, Eirini; Beattie, Charles; Bertolli, Ottavia; Bridgland, Alex; Cherepanov, Alexey; Congreve, Miles; Cowen-Rivers, Alexander I.; Cowie, Andrew; Figurnov, Michael; Fuchs, Fabian B.; Gladman, Hannah; Jain, Rishub; Khan, Yousuf A.; Low, Caroline M. R.; Perlin, Kuba; Potapenko, Anna; Savy, Pascal; Singh, Sukhdeep; Stecula, Adrian; Thillaisundaram, Ashok; Tong, Catherine; Yakneen, Sergei; Zhong, Ellen D.; Zielinski, Michal; Žídek, Augustin; Bapst, Victor; Kohli, Pushmeet; Jaderberg, Max; Hassabis, Demis; Jumper, John M.","Accurate structure prediction of biomolecular interactions with AlphaFold 3","Nature","","1476-4687","10.1038/s41586-024-07487-w","https://www.nature.com/articles/s41586-024-07487-w","The introduction of AlphaFold 21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2–6. Here we describe our AlphaFold 3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein–ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein–nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody–antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.","2024-06","2024-12-30 13:34:55","2024-12-30 13:34:55","2024-12-30 13:34:55","493-500","","8016","630","","","","","","","","","","en","2024 The Author(s)","","","","www.nature.com","","Publisher: Nature Publishing Group","","/home/lexi/.zotero-data/storage/ME4ARFKY/41586_2024_7487_MOESM1_ESM.pdf; /home/lexi/.zotero-data/storage/BLNVI4ZE/Abramson et al. - 2024 - Accurate structure prediction of biomolecular inte.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LEZYW6FN","journalArticle","2021","Jumper, John; Evans, Richard; Pritzel, Alexander; Green, Tim; Figurnov, Michael; Ronneberger, Olaf; Tunyasuvunakool, Kathryn; Bates, Russ; Žídek, Augustin; Potapenko, Anna; Bridgland, Alex; Meyer, Clemens; Kohl, Simon A. A.; Ballard, Andrew J.; Cowie, Andrew; Romera-Paredes, Bernardino; Nikolov, Stanislav; Jain, Rishub; Adler, Jonas; Back, Trevor; Petersen, Stig; Reiman, David; Clancy, Ellen; Zielinski, Michal; Steinegger, Martin; Pacholska, Michalina; Berghammer, Tamas; Bodenstein, Sebastian; Silver, David; Vinyals, Oriol; Senior, Andrew W.; Kavukcuoglu, Koray; Kohli, Pushmeet; Hassabis, Demis","Highly accurate protein structure prediction with AlphaFold","Nature","","1476-4687","10.1038/s41586-021-03819-2","https://www.nature.com/articles/s41586-021-03819-2","Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.","2021-08","2024-12-30 13:36:26","2024-12-30 13:36:26","2024-12-30 13:36:26","583-589","","7873","596","","","","","","","","","","en","2021 The Author(s)","","","","www.nature.com","","Publisher: Nature Publishing Group","","/home/lexi/.zotero-data/storage/4B5KDWRL/41586_2021_3819_MOESM1_ESM.pdf; /home/lexi/.zotero-data/storage/KSHGFFJG/Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AIWTTHF6","preprint","2023","Mao, Anqi; Mohri, Mehryar; Zhong, Yutao","Cross-Entropy Loss Functions: Theoretical Analysis and Applications","","","","10.48550/arXiv.2304.07288","http://arxiv.org/abs/2304.07288","Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of loss functions, comp-sum losses, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other cross-entropy-like loss functions. We give the first $H$-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the specific hypothesis set $H$ used. We further show that our bounds are tight. These bounds depend on quantities called minimizability gaps. To make them more explicit, we give a specific analysis of these gaps for comp-sum losses. We also introduce a new family of loss functions, smooth adversarial comp-sum losses, that are derived from their comp-sum counterparts by adding in a related smooth term. We show that these loss functions are beneficial in the adversarial setting by proving that they admit $H$-consistency bounds. This leads to new adversarial robustness algorithms that consist of minimizing a regularized smooth adversarial comp-sum loss. While our main purpose is a theoretical analysis, we also present an extensive empirical analysis comparing comp-sum losses. We further report the results of a series of experiments demonstrating that our adversarial robustness algorithms outperform the current state-of-the-art, while also achieving a superior non-adversarial accuracy.","2023-06-20","2025-03-06 17:06:51","2025-03-06 17:06:54","2025-03-06 17:06:51","","","","","","","Cross-Entropy Loss Functions","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2304.07288 [cs]","","/home/lexi/.zotero-data/storage/5GW5QVLE/Mao et al. - 2023 - Cross-Entropy Loss Functions Theoretical Analysis.pdf; /home/lexi/.zotero-data/storage/IEYBCF5I/2304.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2304.07288","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8RG7JCIQ","preprint","2016","Veit, Andreas; Wilber, Michael; Belongie, Serge","Residual Networks Behave Like Ensembles of Relatively Shallow Networks","","","","10.48550/arXiv.1605.06431","http://arxiv.org/abs/1605.06431","In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.","2016-10-27","2025-03-21 21:22:27","2025-03-21 21:22:27","2025-03-21 21:22:26","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1605.06431 [cs]","","/home/lexi/.zotero-data/storage/T4VBWRXT/Veit et al. - 2016 - Residual Networks Behave Like Ensembles of Relativ.pdf; /home/lexi/.zotero-data/storage/H3QERAKC/1605.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1605.06431","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CX5W9YBC","preprint","2025","Zhu, Jiachen; Chen, Xinlei; He, Kaiming; LeCun, Yann; Liu, Zhuang","Transformers without Normalization","","","","10.48550/arXiv.2503.10622","http://arxiv.org/abs/2503.10622","Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation $DyT($x$) = \tanh(\alpha $x$)$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, $S$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.","2025-03-13","2025-03-22 14:33:23","2025-03-22 14:33:23","2025-03-22 14:33:23","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2503.10622 [cs]","","/home/lexi/.zotero-data/storage/4RRQPZ9F/Zhu et al. - 2025 - Transformers without Normalization.pdf; /home/lexi/.zotero-data/storage/36WYA7PH/2503.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2503.10622","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6Q6NR9WV","preprint","2023","Conmy, Arthur; Mavor-Parker, Augustine N.; Lynch, Aengus; Heimersheim, Stefan; Garriga-Alonso, Adrià","Towards Automated Circuit Discovery for Mechanistic Interpretability","","","","10.48550/arXiv.2304.14997","http://arxiv.org/abs/2304.14997","Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit that implements the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work. Our code is available at https://github.com/ArthurConmy/Automatic-Circuit-Discovery.","2023-10-28","2025-03-22 20:09:44","2025-03-22 20:09:51","2025-03-22 20:09:44","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2304.14997 [cs]","","/home/lexi/.zotero-data/storage/DMHIRB2E/Conmy et al. - 2023 - Towards Automated Circuit Discovery for Mechanisti.pdf; /home/lexi/.zotero-data/storage/YZ8P4DET/2304.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2304.14997","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L778UHWG","blogPost","2025","Sapunov, Grigory","Not All Layers Are Equal","Gonzo ML","","","","https://gonzoml.substack.com/p/not-all-layers-are-equal","An experiment in mech interp","2025-01-31","2025-03-22 20:10:26","2025-03-22 20:10:26","2025-03-22 20:10:26","","","","","","","","","","","","","","","","Substack newsletter","","","","","","","/home/lexi/.zotero-data/storage/FFK2CRW4/not-all-layers-are-equal.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WHBJ89UC","preprint","2025","Guo, Han; Brandon, William; Cholakov, Radostin; Ragan-Kelley, Jonathan; Xing, Eric P.; Kim, Yoon","Fast Matrix Multiplications for Lookup Table-Quantized LLMs","","","","10.48550/arXiv.2407.10960","http://arxiv.org/abs/2407.10960","The deployment of large language models (LLMs) is often constrained by memory bandwidth, where the primary bottleneck is the cost of transferring model parameters from the GPU's global memory to its registers. When coupled with custom kernels that fuse the dequantization and matmul operations, weight-only quantization can thus enable faster inference by reducing the amount of memory movement. However, developing high-performance kernels for weight-quantized LLMs presents substantial challenges, especially when the weights are compressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform, lookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup table engine for LUT-quantized LLMs, which uses offline restructuring of the quantized weight matrix to minimize bit manipulations associated with unpacking, and vectorization and duplication of the lookup table to mitigate shared memory bandwidth constraints. At batch sizes < 32 and quantization group size of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster than existing GEMM kernels. As an application of FLUTE, we explore a simple extension to lookup table-based NormalFloat quantization and apply it to quantize LLaMA3 to various configurations, obtaining competitive quantization performance against strong baselines while obtaining an end-to-end throughput increase of 1.5 to 2 times.","2025-01-17","2025-05-06 16:23:40","2025-05-06 16:23:50","2025-05-06 16:23:40","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2407.10960 [cs]","","/home/lexi/.zotero-data/storage/UJ86RGRQ/Guo et al. - 2025 - Fast Matrix Multiplications for Lookup Table-Quant.pdf; /home/lexi/.zotero-data/storage/F7GYLSIM/2407.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2407.10960","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MWMCSJQA","conferencePaper","2016","Sennrich, Rico; Haddow, Barry; Birch, Alexandra","Neural Machine Translation of Rare Words with Subword Units","Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","","","10.18653/v1/P16-1162","https://aclanthology.org/P16-1162/","","2016-08","2025-06-13 14:15:51","2025-06-13 14:16:36","2025-06-13 14:15:51","1715–1725","","","","","","","","","","","Association for Computational Linguistics","Berlin, Germany","","","","","","ACLWeb","","","","/home/lexi/.zotero-data/storage/IYPT7NSH/Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf","","milestone","","Erk, Katrin; Smith, Noah A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL 2016","","","","","","","","","","","","","","",""
"R63R67FN","conferencePaper","2019","Radford, Alec; Wu, Jeff; Child, R.; Luan, D.; Amodei, Dario; Sutskever, I.","Language Models are Unsupervised Multitask Learners","","","","","https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe","Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.","2019","2025-06-13 14:30:13","2025-06-13 14:30:13","2025-06-13 14:30:13","","","","","","","","","","","","","","","","","","","Semantic Scholar","","","","","https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VZ8TXKC3","blogPost","2021","Barak, ~ Boaz","ML Theory with bad drawings","Windows On Theory","","","","https://windowsontheory.org/2021/01/15/ml-theory-with-bad-drawings/","(Next post: Lecture 1 – a blitz through classical statistical learning theory . See also all seminar posts) This semester I am teaching a seminar on the theory of machine learning. For the fi…","2021-01-15","2025-07-07 19:55:11","2025-07-07 19:55:11","2025-07-07 19:55:11","","","","","","","","","","","","","","en","","","","","","","","","/home/lexi/.zotero-data/storage/JPC8PDZX/ml-theory-with-bad-drawings.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QYJN29J5","webpage","","","Introducing Ai2 Paper Finder | Ai2","","","","","https://allenai.org/blog/paper-finder","Ai2 Paper Finder is an LLM-powered literature search system that mimics the iterative paper-finding process.","","2025-07-09 13:50:03","2025-07-09 13:50:03","2025-07-09 13:50:03","","","","","","","","","","","","","","en","","","","","","","","","/home/lexi/.zotero-data/storage/IDL8ZIM2/paper-finder.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""